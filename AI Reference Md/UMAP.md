UMAP: Uniform Manifold
Approximation and Projection for
Dimension Reduction
Leland McInnes
Tu/t_te Institute for Mathematics and Computing
leland.mcinnes@gmail.com
John Healy
Tu/t_te Institute for Mathematics and Computing
jchealy@gmail.com
James Melville
jlmelville@gmail.com
September 21, 2020
Abstract
UMAP (Uniform Manifold Approximation and Projection) is a novel
manifold learning technique for dimension reduction. UMAP is constructed
from a theoretical framework based in Riemannian geometry and algebraic
topology. /T_he result is a practical scalable algorithm that is applicable to
real world data. /T_he UMAP algorithm is competitive with t-SNE for visu-
alization quality, and arguably preserves more of the global structure with
superior run time performance. Furthermore, UMAP has no computational
restrictions on embedding dimension, making it viable as a general purpose
dimension reduction technique for machine learning.
1 Introduction
Dimension reduction plays an important role in data science, being a funda-
mental technique in both visualisation and as pre-processing for machine
1
arXiv:1802.03426v3  [stat.ML]  18 Sep 2020

learning. Dimension reduction techniques are being applied in a broaden-
ing range of /f_ields and on ever increasing sizes of datasets. It is thus desir-
able to have an algorithm that is both scalable to massive data and able to
cope with the diversity of data available. Dimension reduction algorithms
tend to fall into two categories; those that seek to preserve the pairwise
distance structure amongst all the data samples and those that favor the
preservation of local distances over global distance. Algorithms such as
PCA [27], MDS [30], and Sammon mapping [50] fall into the former cate-
gory while t-SNE [59, 58], Isomap [56], LargeVis [54], Laplacian eigenmaps
[6, 7] and diﬀusion maps [16] all fall into the la/t_ter category.
In this paper we introduce a novel manifold learning technique for di-
mension reduction. We provide a sound mathematical theory grounding
the technique and a practical scalable algorithm that applies to real world
data. UMAP (Uniform Manifold Approximation and Projection) builds upon
mathematical foundations related to the work of Belkin and Niyogi on
Laplacian eigenmaps. We seek to address the issue of uniform data distri-
butions on manifolds through a combination of Riemannian geometry and
the work of David Spivak [52] in category theoretic approaches to geomet-
ric realization of fuzzy simplicial sets. t-SNE is the current state-of-the-art
for dimension reduction for visualization. Our algorithm is competitive
with t-SNE for visualization quality and arguably preserves more of the
global structure with superior run time performance. Furthermore the al-
gorithm is able to scale to signi/f_icantly larger data set sizes than are feasible
for t-SNE. Finally, UMAP has no computational restrictions on embedding
dimension, making it viable as a general purpose dimension reduction tech-
nique for machine learning.
Based upon preliminary releases of a so/f_tware implementation, UMAP
has already found widespread use in the /f_ields of bioinformatics [5, 12, 17,
46, 2, 45, 15], materials science [34, 23], and machine learning [14, 20, 21,
24, 19, 47] among others.
/T_his paper is laid out as follows. In Section 2 we describe the theory un-
derlying the algorithm. Section 2 is necessary to understand both the the-
ory underlying why UMAP works and the motivation for the choices that
where made in developing the algorithm. A reader without a background
(or interest) in topological data analysis, category theory or the theoretical
underpinnings of UMAP should skip over this section and proceed directly
to Section 3.
/T_hat being said, we feel that strong theory and mathematically justi/f_ied
algorithmic decisions are of particular importance in the /f_ield of unsuper-
vised learning. /T_his is, at least partially, due to plethora of proposed objec-
2

tive functions within the area. We a/t_tempt to highlight in this paper that
UMAPs design decisions were all grounded in a solid theoretic foundation
and not derived through experimentation with any particular task focused
objective function. /T_hough all neighbourhood based manifold learning al-
gorithms must share certain fundamental components we believe it to be
advantageous for these components to be selected through well grounded
theoretical decisions. One of the primary contributions of this paper is to
reframe the problem of manifold learning and dimension reduction in a dif-
ferent mathematical language allowing pracitioners to apply a new /f_ield of
mathemtaics to the problems.
In Section 3 we provide a more computational description of UMAP.
Section 3 should provide readers less familiar with topological data analysis
with a be/t_ter foundation for understanding the theory described in Section
2. Appendix C contrasts UMAP against the more familiar algorithms t-SNE
and LargeVis, describing all these algorithms in similar language. /T_his sec-
tion should assist readers already familiar with those techniques to quickly
gain an understanding of the UMAP algorithm though they will grant li/t_tle
insite into its theoretical underpinnings.
In Section 4 we discuss implementation details of the UMAP algorithm.
/T_his includes a more detailed algorithmic description, and discussion of the
hyper-parameters involved and their practical eﬀects.
In Section 5 we provide practical results on real world datasets as well
as scaling experiments to demonstrate the algorithm’s performance in real
world scenarios as compared with other dimension reduction algorithms.
In Section 6 we discuss relative weakenesses of the algorithm, and ap-
plications for which UMAP may not be the best choice.
Finally, in Section 7 we detail a number of potential extensions of UMAP
that are made possible by its construction upon solid mathematical foun-
dations. /T_hese avenues for further development include semi-supervised
learning, metric learning and heterogeneous data embedding.
2 /T_heoretical Foundations for UMAP
/T_he theoretical foundations for UMAP are largely based in manifold theory
and topological data analysis. Much of the theory is most easily explained
in the language of topology and category theory. Readers may consult
[39], [49] and [40] for background. Readers more interested in practical
computational aspects of the algorithm, and not necessarily the theoretical
motivation for the computations involved, may wish to skip this section.
3

Readers more familiar with traditional machine learning may /f_ind the re-
lationships between UMAP, t-SNE and Largeviz located in Appendix C en-
lightening. Unfortunately, this purely computational view fails to shed any
light upon the reasoning that underlies the algorithmic decisions made in
UMAP. Without strong theoretical foundations the only arguments which
can be made about algorithms amount to empirical measures, for which
there are no clear universal choices for unsupervised problems.
At a high level, UMAP uses local manifold approximations and patches
together their local fuzzy simplicial set representations to construct a topo-
logical representation of the high dimensional data. Given some low dimen-
sional representation of the data, a similar process can be used to construct
an equivalent topological representation. UMAP then optimizes the layout
of the data representation in the low dimensional space, to minimize the
cross-entropy between the two topological representations.
/T_he construction of fuzzy topological representations can be broken
down into two problems: approximating a manifold on which the data is
assumed to lie; and constructing a fuzzy simplicial set representation of
the approximated manifold. In explaining the algorithm we will /f_irst dis-
cuss the method of approximating the manifold for the source data. Next
we will discuss how to construct a fuzzy simplicial set structure from the
manifold approximation. Finally, we will discuss the construction of the
fuzzy simplicial set associated to a low dimensional representation (where
the manifold is simply Rd), and how to optimize the representation with
respect to our objective function.
2.1 Uniform distribution of data on a manifold and
geodesic approximation
/T_he /f_irst step of our algorithm is to approximate the manifold we assume
the data (approximately) lies on. /T_he manifold may be known apriori (as
simply Rn) or may need to be inferred from the data. Suppose the manifold
is not known in advance and we wish to approximate geodesic distance on
it. Let the input data be X = {X1,...,X N}. As in the work of Belkin and
Niyogi on Laplacian eigenmaps [6, 7], for theoretical reasons it is bene/f_icial
to assume the data is uniformly distributed on the manifold, and even if that
assumption is not made (e.g [26]) results are only valid in the limit of in/f_inite
data. In practice, /f_inite real world data is rarely so nicely behaved. However,
if we assume that the manifold has a Riemannian metric not inherited from
the ambient space, we can /f_ind a metric such that the data is approximately
uniformly distributed with regard to that metric.
4

Formally, let Mbe the manifold we assume the data to lie on, and letg
be the Riemannian metric on M. /T_hus, for each pointp∈M we have gp,
an inner product on the tangent space TpM.
Lemma 1.Let (M,g) be a Riemannian manifold in an ambientRn, and let
p∈M be a point. Ifgis locally constant aboutpin an open neighbourhood
U such thatgis a constant diagonal matrix in ambient coordinates, then in a
ball B ⊆Ucentered atpwith volume πn/2
Γ(n/2+1) with respect tog, the geodesic
distance frompto any pointq∈Bis 1
rdRn(p,q), whereris the radius of the
ball in the ambient space anddRn is the existing metric on the ambient space.
See Appendix A of the supplementary materials for a proof of Lemma
1.
If we assume the data to be uniformly distributed onM(with respect to
g) then, away from any boundaries, any ball of /f_ixed volume should contain
approximately the same number of points of Xregardless of where on the
manifold it is centered. Given /f_inite data and small enough local neighbor-
hoods this crude approximation should be accurate enough even for data
samples near manifold boundaries. Now, conversely, a ball centered at Xi
that contains exactly the k-nearest-neighbors of Xi should have approxi-
mately /f_ixed volume regardless of the choice ofXi ∈X. Under Lemma 1
it follows that we can approximate geodesic distance from Xi to its neigh-
bors by normalising distances with respect to the distance to thekth nearest
neighbor of Xi.
In essence, by creating a custom distance for each Xi, we can ensure
the validity of the assumption of uniform distribution on the manifold. /T_he
cost is that we now have an independent notion of distance for each and
every Xi, and these notions of distance may not be compatible. We have
a family of discrete metric spaces (one for each Xi) that we wish to merge
into a consistent global structure. /T_his can be done in a natural way by
converting the metric spaces into fuzzy simplicial sets.
2.2 Fuzzy topological representation
We will use functors between the relevant categories to convert from metric
spaces to fuzzy topological representations. /T_his will provide a means to
merge the incompatible local views of the data. /T_he topological structure
of choice is that of simplicial sets. For more details on simplicial sets we
refer the reader to [25], [40], [48], or [22]. Our approach draws heavily
upon the work of Michael Barr [3] and David Spivak in [52], and many
of the de/f_initions and theorems below are drawn or adapted from those
5

sources. We assume familiarity with the basics of category theory. For an
introduction to category theory readers may consult [39] or [49].
To start we will review the de/f_initions for simplicial sets. Simplicial sets
provide a combinatorial approach to the study of topological spaces. /T_hey
are related to the simpler notion of simplicial complexes – which construct
topological spaces by gluing together simple building blocks called sim-
plices – but are more general. Simplicial sets are most easily de/f_ined purely
abstractly in the language of category theory.
De/f_inition 1./T_he category∆ has as objects the /f_inite order sets[n] =
{1,...,n }, with morphims given by (non-strictly) order-preserving maps.
Following standard category theoretic notation, ∆op denotes the cate-
gory with the same objects as ∆ and morphisms given by the morphisms
of ∆ with the direction (domain and codomain) reversed.
De/f_inition 2.A simplicial set is a functor from∆op to Sets, the category of
sets; that is, a contravariant functor from∆ to Sets.
Given a simplicial set X : ∆op →Sets,it is common to denote the set
X([n]) as Xn and refer to the elements of the set as the n-simplices of X.
/T_he simplest possible examples of simplicial sets are thestandard simplices
∆n, de/f_ined as the representable functorshom∆(·,[n]). It follows from the
Yoneda lemma that there is a natural correspondence betweenn-simplices
of X and morphisms ∆n →X in the category of simplicial sets, and it
is o/f_ten helpful to think in these terms. /T_hus for eachx ∈Xn we have
a corresponding morphism x : ∆ n → X. By the density theorem and
employing a minor abuse of notation we then have
colim
x∈Xn
∆n ∼= X
/T_here is a standard covariant functor|·| : ∆ →Top mapping from
the category ∆ to the category of topological spaces that sends [n] to the
standard n-simplex |∆n|⊂ Rn+1 de/f_ined as
|∆n|≜
{
(t0,...,t n) ∈Rn+1 |
n∑
i=0
ti = 1,ti ≥0
}
with the standard subspace topology. If X : ∆op →Sets is a simplicial
set then we can construct the realization of X(denoted |X|) as the colimit
|X|= colim
x∈Xn
|∆n|
6

and thus associate a topological space with a given simplicial set. Con-
versely given a topological space Y we can construct an associated simpli-
cial set S(Y), called the singular set of Y, by de/f_ining
S(Y) : [n] ↦→homTop(|∆n|,Y ).
It is a standard result of classical homotopy theory that the realization func-
tor and singular set functors form an adjunction, and provide the standard
means of translating between topological spaces and simplicial sets. Our
goal will be to adapt these powerful classical results to the case of /f_inite
metric spaces.
We draw signi/f_icant inspiration from Spivak, speci/f_ically [52], where
he extends the classical theory of singular sets and topological realization
to fuzzy singular sets and metric realization. To develop this theory here
we will /f_irst outline a categorical presentation of fuzzy sets, due to [3], that
will make extending classical simplicial sets to fuzzy simplicial sets most
natural.
Classically a fuzzy set [65] is de/f_ined in terms of a carrier setAand a
map µ: A→[0,1] called the membership function. One is to interpret the
value µ(x) for x∈Ato be the membership strengthof xto the set A. /T_hus
membership of a set is no longer a bi-valent true or false property as in
classical set theory, but a fuzzy property taking values in the unit interval.
We wish to formalize this in terms of category theory.
Let I be the unit interval (0,1] ⊆R with topology given by intervals
of the form [0,a) for a∈(0,1]. /T_he category of open sets (with morphisms
given by inclusions) can be imbued with a Grothendieck topology in the
natural way for any poset category.
De/f_inition 3.A presheafP on Iis a functor fromIop to Sets. Afuzzy set
is a presheaf onI such that all mapsP(a≤b) are injections.
Presheaves on I form a category with morphisms given by natural
transformations. We can thus form a category of fuzzy sets by simply re-
stricting to the sub-category of presheaves that are fuzzy sets. We note that
such presheaves are trivially sheaves under the Grothendieck topology on
I. As one might expect, limits (including products) of such sheaves are
well de/f_ined, but care must be taken to de/f_ine colimits (and coproducts) of
sheaves. To link to the classical approach to fuzzy sets one can think of a
section P([0,a)) as the set of all elements with membership strength at
least a. We can now de/f_ine the category of fuzzy sets.
De/f_inition 4./T_he categoryFuzz of fuzzy sets is the full subcategory of
sheaves onI spanned by fuzzy sets.
7

With this categorical presentation in hand, de/f_ining fuzzy simplicial
sets is simply a ma/t_ter of considering presheaves of∆ valued in the cate-
gory of fuzzy sets rather than the category of sets.
De/f_inition 5./T_he category offuzzy simplicial sets sFuzz is the category
with objects given by functors from∆op to Fuzz, and morphisms given by
natural transformations.
Alternatively, a fuzzy simplicial set can be viewed as a sheaf over∆×I,
where ∆ is given the trivial topology and∆ ×Ihas the product topology.
We will use ∆n
<a to denote the sheaf given by the representable functor of
the object ([n],[0,a)). /T_he importance of this fuzzy (shea/f_i/f_ied) version of
simplicial sets is their relationship to metric spaces. We begin by consider-
ing the larger category of extended-pseudo-metric spaces.
De/f_inition 6.An extended-pseudo-metric space (X,d) is a setX and a
map d: X×X →R≥0 ∪{∞}such that
1. d(x,y) ⩾ 0, andx= yimplies d(x,y) = 0;
2. d(x,y) = d(y,x); and
3. d(x,z) ⩽ d(x,y) + d(y,z) or d(x,z) = ∞.
/T_he category of extended-pseudo-metric spacesEPMet has as objects extended-
pseudo-metric spaces and non-expansive maps as morphisms. We denote the
subcategory of /f_inite extended-pseudo-metric spacesFinEPMet.
/T_he choice of non-expansive maps in De/f_inition 6 is due to Spivak, but
we note that it closely mirrors the work of Carlsson and Memoli in [13] on
topological methods for clustering as applied to /f_inite metric spaces. /T_his
choice is signi/f_icant since pure isometries are too strict and do not provide
large enough Hom-sets.
In [52] Spivak constructs a pair of adjoint functors, Real and Sing be-
tween the categories sFuzz and EPMet. /T_hese functors are the natural ex-
tension of the classical realization and singular set functors from algebraic
topology. /T_he functorReal is de/f_ined in terms of standard fuzzy simplices
∆n
<a as
Real(∆n
<a) ≜
{
(t0,...,t n) ∈Rn+1 |
n∑
i=0
ti = −log(a),ti ≥0
}
similarly to the classical realization functor |·|. /T_he metric onReal(∆n
<a)
is simply inherited from Rn+1. A morphism ∆n
<a →∆m
<b exists only if
8

a ≤b, and is determined by a ∆ morphism σ : [n] →[m]. /T_he action of
Real on such a morphism is given by the map
(x0,x1,...,x n) ↦→log(b)
log(a)

 ∑
i0∈σ−1(0)
xi0 ,
∑
i0∈σ−1(1)
xi0 ,...,
∑
i0∈σ−1(m)
xi0

.
Such a map is clearly non-expansive since 0 ≤a ≤b ≤1 implies that
log(b)/log(a) ≤1.
We then extend this to a general simplicial set X via colimits, de/f_ining
Real(X) ≜ colim
∆n
<a→X
Real(∆n
<a).
Since the functor Real preserves colimits, it follows that there exists a
right adjoint functor. Again, analogously to the classical case, we /f_ind the
right adjoint, denoted Sing, is de/f_ined for an extended pseudo metric space
Y in terms of its action on the category ∆ ×I:
Sing(Y) : ([n],[0,a)) ↦→homEPMet(Real(∆n
<a),Y ).
For our case we are only interested in /f_inite metric spaces. To corre-
spond with this we consider the subcategory of bounded fuzzy simplicial
sets Fin-sFuzz. We therefore use the analogous adjoint pair FinReal and
FinSing. Formally we de/f_ine the /f_inite fuzzy realization functor as follows:
De/f_inition 7.De/f_ine the functorFinReal : Fin-sFuzz →FinEPMet by
se/t_ting
FinReal(∆n
<a) ≜ ({x1,x2,...,x n},da),
where
da(xi,xj) =



−log(a) if i̸= j,
0 otherwise
.
and then de/f_ining
FinReal(X) ≜ colim
∆n
<a→X
FinReal(∆n
<a).
Similar to Spivak’s construction, the action ofFinReal on a map∆n
<a →
∆m
<b, where a≤bde/f_ined byσ: ∆n →∆m, is given by
({x1,x2,...,x n},da) ↦→({xσ(1),xσ(2),...,x σ(n)},db),
which is a non-expansive map since a≤bimplies da ≥db.
Since FinReal preserves colimits it admits a right adjoint, the fuzzy sin-
gular set functor FinSing. We can then de/f_ine the (/f_inite) fuzzy singular set
functor in terms of the action of its image on ∆ ×I, analogously to Sing.
9

De/f_inition 8.De/f_ine the functorFinSing : FinEPMet →Fin-sFuzz by
FinSing(Y) : ([n],[0,a)) ↦→homFinEPMet(FinReal(∆n
<a),Y ).
We then have the following theorem.
/T_heorem 1./T_he functorsFinReal : Fin-sFuzz →FinEPMet and FinSing :
FinEPMet →Fin-sFuzz form an adjunction withFinReal the le/f_t adjoint
and FinSing the right adjoint.
/T_he proof of this is by construction. Appendix B provides a full proof
of the theorem.
With the necessary theoretical background in place, the means to han-
dle the family of incompatible metric spaces described above becomes clear.
Each metric space in the family can be translated into a fuzzy simplicial
set via the fuzzy singular set functor, distilling the topological information
while still retaining metric information in the fuzzy structure. Ironing out
the incompatibilities of the resulting family of fuzzy simplicial sets can be
done by simply taking a (fuzzy) union across the entire family. /T_he result
is a single fuzzy simplicial set which captures the relevant topological and
underlying metric structure of the manifold M.
It should be noted, however, that the fuzzy singular set functor applies
to extended-pseudo-metric spaces, which are a relaxation of traditional
metric spaces. /T_he results of Lemma 1 only provide accurate approxima-
tions of geodesic distance local to Xi for distances measured from Xi –
the geodesic distances between other pairs of points within the neighbor-
hood of Xi are not well de/f_ined. In deference to this lack of information we
de/f_ine distances betweenXj and Xk in the extended-pseudo metric space
local to Xi (where i ̸= j and i ̸= k) to be in/f_inite (local neighborhoods of
Xj and Xk will provide suitable approximations).
For real data it is safe to assume that the manifold Mis locally con-
nected. In practice this can be realized by measuring distance in the extended-
pseudo-metric space local to Xi as geodesic distance beyond the nearest
neighbor of Xi. Since this sets the distance to the nearest neighbor to be
equal to 0 this is only possible in the more relaxed se/t_ting of extended-
pseudo-metric spaces. It ensures, however, that each 0-simplex is the face
of some 1-simplex with fuzzy membership strength 1, meaning that the
resulting topological structure derived from the manifold is locally con-
nected. We note that this has a similar practical eﬀect to the truncated
similarity approach of Lee and Verleysen [33], but derives naturally from
the assumption of local connectivity of the manifold.
10

Combining all of the above we can de/f_ine the fuzzy topological repre-
sentation of a dataset.
De/f_inition 9.Let X = {X1,...,X N}be a dataset inRn. Let{(X,di)}i=1...N
be a family of extended-pseudo-metric spaces with common carrier setXsuch
that
di(Xj,Xk) =



dM(Xj,Xk) −ρ if i= jor i= k,
∞ otherwise ,
where ρ is the distance to the nearest neighbor ofXi and dM is geodesic
distance on the manifoldM, either known apriori, or approximated as per
Lemma 1.
/T_he fuzzy topological representation ofX is
n⋃
i=1
FinSing((X,di)).
/T_he (fuzzy set) union provides the means to merge together the diﬀer-
ent metric spaces. /T_his provides a single fuzzy simplicial set as the global
representation of the manifold formed by patching together the many local
representations.
Given the ability to construct such topological structures, either from
a known manifold, or by learning the metric structure of the manifold, we
can perform dimension reduction by simply /f_inding low dimensional rep-
resentations that closely match the topological structure of the source data.
We now consider the task of /f_inding such a low dimensional representation.
2.3 Optimizing a low dimensional representation
Let Y = {Y1,...,Y N}⊆ Rd be a low dimensional (d≪n) representation
of X such that Yi represents the source data point Xi. In contrast to the
source data where we want to estimate a manifold on which the data is
uniformly distributed, a target manifold forY is chosen apriori (usually this
will simply be Rd itself, but other choices such as d-spheres or d-tori are
certainly possible) . /T_herefore we know the manifold and manifold metric
apriori, and can compute the fuzzy topological representation directly. Of
note, we still want to incorporate the distance to the nearest neighbor as per
the local connectedness requirement. /T_his can be achieved by supplying a
parameter that de/f_ines the expected distance between nearest neighbors in
the embedded space.
11

Given fuzzy simplicial set representations ofXand Y, a means of com-
parison is required. If we consider only the 1-skeleton of the fuzzy sim-
plicial sets we can describe each as a fuzzy graph, or, more speci/f_ically, a
fuzzy set of edges. To compare two fuzzy sets we will make use of fuzzy set
cross entropy. For these purposes we will revert to classical fuzzy set no-
tation. /T_hat is, a fuzzy set is given by a reference setAand a membership
strength function µ : A →[0,1]. Comparable fuzzy sets have the same
reference set. Given a sheaf representation P we can translate to classical
fuzzy sets by se/t_tingA= ⋃
a∈(0,1] P([0,a)) and µ(x) = sup{a∈(0,1] |
x∈P([0,a))}.
De/f_inition 10./T_he cross entropyC of two fuzzy sets(A,µ) and (A,ν) is
de/f_ined as
C((A,µ),(A,ν)) ≜
∑
a∈A
(
µ(a) log
(µ(a)
ν(a)
)
+ (1 −µ(a)) log
(1 −µ(a)
1 −ν(a)
))
.
Similar to t-SNE we can optimize the embeddingY with respect to fuzzy
set cross entropy Cby using stochastic gradient descent. However, this re-
quires a diﬀerentiable fuzzy singular set functor. If the expected minimum
distance between points is zero the fuzzy singular set functor is diﬀeren-
tiable for these purposes, however for any non-zero value we need to make
a diﬀerentiable approximation (chosen from a suitable family of diﬀeren-
tiable functions).
/T_his completes the algorithm: by using manifold approximation and
patching together local fuzzy simplicial set representations we construct a
topological representation of the high dimensional data. We then optimize
the layout of data in a low dimensional space to minimize the error between
the two topological representations.
We note that in this case we restricted a/t_tention to comparisons of the
1-skeleton of the fuzzy simplicial sets. One can extend this to ℓ-skeleta by
de/f_ining a cost functionCℓ as
Cℓ(X,Y ) =
ℓ∑
i=1
λiC(Xi,Yi),
where Xi denotes the fuzzy set of i-simplices of X and the λi are suit-
ably chosen real valued weights. While such an approach will capture the
overall topological structure more accurately, it comes at non-negligible
computational cost due to the increasingly large numbers of higher dimen-
sional simplices. For this reason current implementations restrict to the
1-skeleton at this time.
12

3 A Computational View of UMAP
To understand what computations the UMAP algorithm is actually making
from a practical point of view, a less theoretical and more computational
description may be helpful for the reader. /T_his description of the algorithm
lacks the motivation for a number of the choices made. For that motivation
please see Section 2.
/T_he theoretical description of the algorithm works in terms of fuzzy
simplicial sets. Computationally this is only tractable for the one skeleton
which can ultimately be described as a weighted graph. /T_his means that,
from a practical computational perspective, UMAP can ultimately be de-
scribed in terms of, construction of, and operations on, weighted graphs.
In particular this situates UMAP in the class of k-neighbour based graph
learning algorithms such as Laplacian Eigenmaps, Isomap and t-SNE.
As with other k-neighbour graph based algorithms, UMAP can be de-
scribed in two phases. In the /f_irst phase a particular weighted k-neighbour
graph is constructed. In the second phase a low dimensional layout of this
graph is computed. /T_he diﬀerences between all algorithms in this class
amount to speci/f_ic details in how the graph is constructed and how the
layout is computed. /T_he theoretical basis for UMAP as described in Section
2 provides novel approaches to both of these phases, and provides clear
motivation for the choices involved.
Finally, since t-SNE is not usually described as a graph based algorithm,
a direct comparison of UMAP with t-SNE, using the similarity/probability
notation commonly used to express the equations of t-SNE, is given in the
Appendix C.
In section 2 we made a few basic assumptions about our data. From
these assumptions we made use of category theory to derive the UMAP
algorithms. /T_hat said, all these derivations assume these axioms to be true.
1. /T_here exists a manifold on which the data would be uniformly dis-
tributed.
2. /T_he underlying manifold of interest is locally connected.
3. Preserving the topological structure of this manifold is the primary
goal.
/T_he topological theory of Section 2 is driven by these axioms, particularly
the interest in modelling and preserving topological structure. In particular
Section 2.1 highlights the underlying motivation, in terms of topological
theory, of representing a manifold as a k-neighbour graph.
13

As highlighted in Appendix C any algorithm that a/t_tempts to use a
mathematical structure akin to a k-neighbour graph to approximate a man-
ifold must follow a similar basic structure.
• Graph Construction
1. Construct a weighted k-neighbour graph
2. Apply some transform on the edges to ambient local distance.
3. Deal with the inherent asymmetry of the k-neighbour graph.
• Graph Layout
1. De/f_ine an objective function that preserves desired characteris-
tics of this k-neighbour graph.
2. Find a low dimensional representation which optimizes this ob-
jective function.
Many dimension reduction algorithms can be broken down into these
steps because they are fundamental to a particular class of solutions. Choices
for each step must be either chosen through task oriented experimentation
or by selecting a set of believable axioms and building strong theoretical
arguments from these. Our belief is that basing our decisions on a strong
foundational theory will allow for a more extensible and generalizable al-
gorithm in the long run.
We theoretically justify using the choice of using a k-neighbour graph
to represent a manifold in Section 2.1. /T_he choices for our kernel transform
an symmetrization function can be found in Section 2.2. Finally, the justi/f_i-
cations underlying our choices for our graph layout are outlined in Section
2.3.
3.1 Graph Construction
/T_he /f_irst phase of UMAP can be thought of as the construction of a weighted
k-neighbour graph. Let X = {x1,...,x N}be the input dataset, with a
metric (or dissimilarity measure)d: X×X →R≥0. Given an input hyper-
parameter k, for each xi we compute the set{xi1 ,...,x ik}of the knearest
neighbors of xi under the metric d. /T_his computation can be performed via
any nearest neighbour or approximate nearest neighbour search algorithm.
For the purposes of our UMAP implemenation we prefer to use the nearest
neighbor descent algorithm of [18].
For each xi we will de/f_ineρi and σi. Let
ρi = min{d(xi,xij ) |1 ≤j ≤k,d(xi,xij ) >0},
14

and set σi to be the value such that
k∑
j=1
exp
(−max(0,d(xi,xij ) −ρi)
σi
)
= log2(k).
/T_he selection ofρi derives from the local-connectivity constraint described
in Section 2.2. In particular it ensures that xi connects to at least one other
data point with an edge of weight 1; this is equivalent to the resulting fuzzy
simplicial set being locally connected at xi. In practical terms this signif-
icantly improves the representation on very high dimensional data where
other algorithms such as t-SNE begin to suﬀer from the curse of dimen-
sionality.
/T_he selection ofσi corresponds to (a smoothed) normalisation factor,
de/f_ining the Riemannian metric local to the pointxias described in Section
2.1.
We can now de/f_ine a weighted directed graph ¯G = ( V,E,w ). /T_he
vertices V of ¯Gare simply the set X. We can then form the set of directed
edges E = {(xi,xij ) |1 ≤j ≤k,1 ≤i ≤N}, and de/f_ine the weight
function wby se/t_ting
w((xi,xij )) = exp
(−max(0,d(xi,xij ) −ρi)
σi
)
.
For a given pointxi there exists an induced graph ofxi and outgoing edges
incident on xi. /T_his graph is the 1-skeleton of the fuzzy simplicial set as-
sociated to the metric space local to xi where the local metric is de/f_ined
in terms of ρi and σi. /T_he weight associated to the edge is the member-
ship strength of the corresponding 1-simplex within the fuzzy simplicial
set, and is derived from the adjunction of /T_heorem 1 using the right adjoint
(nearest inverse) of the geometric realization of a fuzzy simplicial set. In-
tuitively one can think of the weight of an edge as akin to the probability
that the given edge exists. Section 2 demonstrates why this construction
faithfully captures the topology of the data. Given this set of local graphs
(represented here as a single directed graph) we now require a method to
combine them into a uni/f_ied topological representation. We note that while
patching together incompatible /f_inite metric spaces is challenging, by using
/T_heorem 1 to convert to a fuzzy simplicial set representation, the combin-
ing operation becomes natural.
Let Abe the weighted adjacency matrix of ¯G, and consider the sym-
metric matrix
B = A+ A⊤−A◦A⊤,
15

where ◦is the Hadamard (or pointwise) product. /T_his formula derives from
the use of the probabilistic t-conorm used in unioning the fuzzy simplicial
sets. If one interprets the value of Aij as the probability that the directed
edge from xi to xj exists, then Bij is the probability that at least one of
the two directed edges (from xi to xj and from xj to xi) exists. /T_he UMAP
graph Gis then an undirected weighted graph whose adjacency matrix is
given by B. Section 2 explains this construction in topological terms, pro-
viding the justi/f_ication for why this construction provides an appropriate
fuzzy topological representation of the data – that is, this construction cap-
tures the underlying geometric structure of the data in a faithful way.
3.2 Graph Layout
In practice UMAP uses a force directed graph layout algorithm in low di-
mensional space. A force directed graph layout utilizes of a set of a/t_tractive
forces applied along edges and a set of repulsive forces applied among ver-
tices. Any force directed layout algorithm requires a description of both the
a/t_tractive and repulsive forces. /T_he algorithm proceeds by iteratively ap-
plying a/t_tractive and repulsive forces at each edge or vertex. /T_his amounts
to a non-convex optimization problem. Convergence to a local minima is
guaranteed by slowly decreasing the a/t_tractive and repulsive forces in a
similar fashion to that used in simulated annealing.
In UMAP the a/t_tractive force between two verticesiand j at coordi-
nates yi and yj respectively, is determined by:
−2ab∥yi −yj∥2(b−1)
2
1 + ∥yi −yj∥2
2
w((xi,xj)) (yi −yj)
where aand bare hyper-parameters.
Repulsive forces are computed via sampling due to computational con-
straints. /T_hus, whenever an a/t_tractive force is applied to an edge, one of
that edge’s vertices is repulsed by a sampling of other vertices. /T_he repul-
sive force is given by
2b(
ϵ+ ∥yi −yj∥2
2
)(
1 + a∥yi −yj∥2b
2
)(1 −w((xi,xj))) (yi −yj) .
ϵ is a small number to prevent division by zero (0.001 in the current
implementation).
16

/T_he algorithm can be initialized randomly but in practice, since the sym-
metric Laplacian of the graphGis a discrete approximation of the Laplace-
Beltrami operator of the manifold, we can use a spectral layout to initialize
the embedding. /T_his provides both faster convergence and greater stability
within the algorithm.
/T_he forces described above are derived from gradients optimising the
edge-wise cross-entropy between the weighted graph G, and an equiva-
lent weighted graph Hconstructed from the points {yi}i=1..N. /T_hat is, we
are seeking to position points yi such that the weighted graph induced by
those points most closely approximates the graph G, where we measure
the diﬀerence between weighted graphs by the total cross entropy over all
the edge existence probabilities. Since the weighted graph Gcaptures the
topology of the source data, the equivalent weighted graph Hconstructed
from the points{yi}i=1..N matches the topology as closely as the optimiza-
tion allows, and thus provides a good low dimensional representation of the
overall topology of the data.
4 Implementation and Hyper-parameters
Having completed a theoretical description of the approach, we now turn
our a/t_tention to the practical realization of this theory. We begin by pro-
viding a more detailed description of the algorithm as implemented, and
then discuss a few implementation speci/f_ic details. We conclude this sec-
tion with a discussion of the hyper-parameters for the algorithm and their
practical eﬀects.
4.1 Algorithm description
In overview the UMAP algorithm is relatively straightforward (see Algo-
rithm 1). When performing a fuzzy union over local fuzzy simplicial sets
we have found it most eﬀective to work with the probabilistic t-conorm (as
one would expect if treating membership strengths as a probability that the
simplex exists). /T_he individual functions for constructing the local fuzzy
simplicial sets, determining the spectral embedding, and optimizing the
embedding with regard to fuzzy set cross entropy, are described in more
detail below.
/T_he inputs to Algorithm 1 are: X, the dataset to have its dimension
reduced; n, the neighborhood size to use for local metric approximation;
d, the dimension of the target reduced space; min-dist, an algorithmic pa-
17

Algorithm 1UMAP algorithm
function UMAP(X, n, d, min-dist, n-epochs)
# Construct the relevant weighted graph
for allx∈X do
fs-set[x] ←L/o.sc/c.sc/a.sc/l.scF/u.sc/z.sc/z.sc/y.scS/i.sc/m.sc/p.sc/l.sc/i.sc/c.sc/i.sc/a.sc/l.scS/e.sc/t.sc(X, x, n)
top-rep ←⋃
x∈X fs-set[x] # We recommend the probabilistic t-conorm
# Perform optimization of the graph layout
Y ←S/p.sc/e.sc/c.sc/t.sc/r.sc/a.sc/l.scE/m.sc/b.sc/e.sc/d.sc/d.sc/i.sc/n.sc/g.sc(top-rep, d)
Y ←O/p.sc/t.sc/i.sc/m.sc/i.sc/z.sc/e.scE/m.sc/b.sc/e.sc/d.sc/d.sc/i.sc/n.sc/g.sc(top-rep, Y, min-dist, n-epochs)
return Y
rameter controlling the layout; and n-epochs, controlling the amount of
optimization work to perform.
Algorithm 2 describes the construction of local fuzzy simplicial sets.
To represent fuzzy simplicial sets we work with the fuzzy set images of[0]
and [1] (i.e. the 1-skeleton), which we denote as fs-set0 and fs-set1. One can
work with higher order simplices as well, but the current implementation
does not. We can construct the fuzzy simplicial set local to a given point x
by /f_inding thennearest neighbors, generating the appropriate normalised
distance on the manifold, and then converting the /f_inite metric space to a
simplicial set via the functor FinSing, which translates into exponential of
the negative distance in this case.
Rather than directly using the distance to the nth nearest neighbor as
the normalization, we use a smoothed version of knn-distance that /f_ixes
the cardinality of the fuzzy set of 1-simplices to a /f_ixed value. We selected
log2(n) for this purpose based on empirical experiments. /T_his is described
brie/f_ly in Algorithm 3.
Spectral embedding is performed by considering the 1-skeleton of the
global fuzzy topological representation as a weighted graph and using stan-
dard spectral methods on the symmetric normalized Laplacian. /T_his pro-
cess is described in Algorithm 4.
/T_he /f_inal major component of UMAP is the optimization of the em-
bedding through minimization of the fuzzy set cross entropy. Recall that
18

Algorithm 2Constructing a local fuzzy simplicial set
function L/o.sc/c.sc/a.sc/l.scF/u.sc/z.sc/z.sc/y.scS/i.sc/m.sc/p.sc/l.sc/i.sc/c.sc/i.sc/a.sc/l.scS/e.sc/t.sc(X, x, n)
knn, knn-dists ←A/p.sc/p.sc/r.sc/o.sc/x.scN/e.sc/a.sc/r.sc/e.sc/s.sc/t.scN/e.sc/i.sc/g.sc/h.sc/b.sc/o.sc/r.sc/s.sc(X, x, n)
ρ←knn-dists[1] # Distance to nearest neighbor
σ←S/m.sc/o.sc/o.sc/t.sc/h.scKNND/i.sc/s.sc/t.sc(knn-dists, n, ρ) # Smooth approximator to
knn-distance
fs-set0 ←X
fs-set1 ←{([x,y],0) |y∈X}
for ally∈knn do
dx,y ←max{0,dist(x,y) −ρ}/σ
fs-set1 ←fs-set1 ∪([x,y],exp(−dx,y))
return fs-set
Algorithm 3Compute the normalizing factor for distances σ
function S/m.sc/o.sc/o.sc/t.sc/h.scKNND/i.sc/s.sc/t.sc(knn-dists, n, ρ)
Binary search for σsuch that ∑n
i=1 exp(−(knn-distsi −ρ)/σ) = log2(n)
return σ
Algorithm 4Spectral embedding for initialization
function S/p.sc/e.sc/c.sc/t.sc/r.sc/a.sc/l.scE/m.sc/b.sc/e.sc/d.sc/d.sc/i.sc/n.sc/g.sc(top-rep, d)
A←1-skeleton of top-rep expressed as a weighted adjacency matrix
D←degree matrix for the graph A
L←D1/2(D−A)D1/2
evec ←Eigenvectors of L(sorted)
Y ←evec[1..d+ 1] # 0-base indexing assumed
return Y
19

fuzzy set cross entropy, with respect given membership functionsµand ν,
is given by
C((A,µ),(A,ν)) =
∑
a∈A
µ(a) log
(µ(a)
ν(a)
)
+ (1 −µ(a)) log
(1 −µ(a)
1 −ν(a)
)
=
∑
a∈A
(µ(a) log(µ(a)) + (1−µ(a)) log(1−µ(a)))
−
∑
a∈A
(µ(a) log(ν(a)) + (1−µ(a)) log(1−ν(a))) .
(1)
/T_he /f_irst sum depends only onµwhich takes /f_ixed values during the op-
timization, thus the minimization of cross entropy depends only on the
second sum, so we seek to minimize
−
∑
a∈A
(µ(a) log(ν(a)) + (1−µ(a)) log(1−ν(a))) .
Following both [54] and [41], we take a sampling based approach to the
optimization. We sample 1-simplices with probability µ(a) and update ac-
cording to the value of ν(a), which handles the term µ(a) log(ν(a)). /T_he
term (1 −µ(a)) log(1 −ν(a)) requires negative sampling – rather than
computing this over all potential simplices we randomly sample potential
1-simplices and assume them to be a negative example (i.e. with member-
ship strength 0) and update according to the value of 1 −ν(a). In contrast
to [54] the above formulation provides a vertex sampling distribution of
P(xi) =
∑
{a∈A|d0(a)=xi}1 −µ(a)
∑
{b∈A|d0(b)̸=xi}1 −µ(b)
for negative samples, which can be reasonably approximated by a uniform
distribution for suﬃciently large data sets.
It therefore only remains to /f_ind a diﬀerentiable approximation toν(a)
for a given 1-simplex a so that gradient descent can be applied for opti-
mization. /T_his is done as follows:
De/f_inition 11.De/f_ineΦ : Rd×Rd →[0,1], a smooth approximation of the
membership strength of a 1-simplex between two points inRd, as
Φ(x,y) =
(
1 + a(∥x −y∥2
2)b
)−1
,
20

where aand bare chosen by non-linear least squares /f_i/t_ting against the curve
Ψ : Rd ×Rd →[0,1] where
Ψ(x,y) =
{
1 if ∥x −y∥2 ≤min-dist
exp(−(∥x −y∥2 −min-dist)) otherwise .
/T_he optimization process is now executed by stochastic gradient de-
scent as given by Algorithm 5.
Algorithm 5Optimizing the embedding
function O/p.sc/t.sc/i.sc/m.sc/i.sc/z.sc/e.scE/m.sc/b.sc/e.sc/d.sc/d.sc/i.sc/n.sc/g.sc(top-rep, Y, min-dist, n-epochs)
α←1.0
Fit Φ from Ψ de/f_ined by min-dist
for e←1,..., n-epochs do
for all([a,b],p) ∈top-rep1 do
if R/a.sc/n.sc/d.sc/o.sc/m.sc( ) ≤pthen # Sample simplex with probabilityp
ya ←ya + α·∇(log(Φ))(ya,yb)
for i←1,..., n-neg-samples do
c←random sample from Y
ya ←ya + α·∇(log(1 −Φ))(ya,yc)
α←1.0 −e/n-epochs
return Y
/T_his completes the UMAP algorithm.
4.2 Implementation
Practical implementation of this algorithm requires (approximate)k-nearest-
neighbor calculation and eﬃcient optimization via stochastic gradient de-
scent.
Eﬃcient approximate k-nearest-neighbor computation can be achieved
via the Nearest-Neighbor-Descent algorithm of [18]. /T_he error intrinsic in
a dimension reduction technique means that such approximation is more
than adequate for these purposes. While no theoretical complexity bounds
21

have been established for Nearest-Neighbor-Descent the authors of the
original paper report an empirical complexity of O(N1.14). A further ben-
e/f_it of Nearest-Neighbor-Descent is its generality; it works with any valid
dissimilarity measure, and is eﬃcient even for high dimensional data.
In optimizing the embedding under the provided objective function, we
follow work of [54]; making use of probabilistic edge sampling and nega-
tive sampling [41]. /T_his provides a very eﬃcient approximate stochastic
gradient descent algorithm since there is no normalization requirement.
Furthermore, since the normalized Laplacian of the fuzzy graph represen-
tation of the input data is a discrete approximation of the Laplace-Betrami
operator of the manifold [?, see]]belkin2002laplacian, belkin2003laplacian,
we can provide a suitable initialization for stochastic gradient descent by
using the eigenvectors of the normalized Laplacian. /T_he amount of opti-
mization work required will scale with the number of edges in the fuzzy
graph (assuming a /f_ixed negative sampling rate), resulting in a complexity
of O(kN).
Combining these techniques results in highly eﬃcient embeddings, which
we will discuss in Section 5. /T_he overall complexity is bounded by the ap-
proximate nearest neighbor search complexity and, as mentioned above, is
empirically approximately O(N1.14). A reference implementation can be
found at https://github.com/lmcinnes/umap, and an R implementa-
tion can be found at https://github.com/jlmelville/uwot.
For simplicity these experiments were carried out on a single core ver-
sion of our algorithm. It should be noted that at the time of this publication
that both Nearest-Neighbour-Descent and SGD have been parallelized and
thus the python reference implementation can be signi/f_icantly accelerated.
Our intention in this paper was to introduce the underlying theory behind
our UMAP algorithm and we felt that parallel vs single core discussions
would distract from our intent.
4.3 Hyper-parameters
As described in Algorithm 1, the UMAP algorithm takes four hyper-parameters:
1. n, the number of neighbors to consider when approximating the local
metric;
2. d, the target embedding dimension;
3. min-dist, the desired separation between close points in the embed-
ding space; and
22

4. n-epochs, the number of training epochs to use when optimizing the
low dimensional representation.
/T_he eﬀects of the parametersdand n-epochs are largely self-evident, and
will not be discussed in further detail here. In contrast the eﬀects of the
number of neighbors nand of min-dist are less clear.
One can interpret the number of neighborsnas the local scale at which
to approximate the manifold as roughly /f_lat, with the manifold estimation
averaging over the nneighbors. Manifold features that occur at a smaller
scale than within thennearest-neighbors of points will be lost, while large
scale manifold features that cannot be seen by patching together locally /f_lat
charts at the scale of nnearest-neighbors may not be well detected. /T_hus
nrepresents some degree of trade-oﬀ between /f_ine grained and large scale
manifold features — smaller values will ensure detailed manifold structure
is accurately captured (at a loss of the “big picture” view of the manifold),
while larger values will capture large scale manifold structures, but at a loss
of /f_ine detail structure which will get averaged out in the local approxima-
tions. With smaller nvalues the manifold tends to be broken into many
small connected components (care needs to be taken with the spectral em-
bedding for initialization in such cases).
In contrast min-dist is a hyperparameter directly aﬀecting the output,
as it controls the fuzzy simplicial set construction from the low dimensional
representation. It acts in lieu of the distance to the nearest neighbor used
to ensure local connectivity. In essence this determines how closely points
can be packed together in the low dimensional representation. Low values
on min-dist will result in potentially densely packed regions, but will likely
more faithfully represent the manifold structure. Increasing the value of
min-dist will force the embedding to spread points out more, assisting vi-
sualization (and avoiding potential overplo/t_ting issues). We view min-dist
as an essentially aesthetic parameter, governing the appearance of the em-
bedding, and thus is more important when using UMAP for visualization.
In Figure 1 we provide examples of the eﬀects of varying the hyper-
parameters for a toy dataset. /T_he data is uniform random samples from a
3-dimensional color-cube, allowing for easy visualization of the original 3-
dimensional coordinates in the embedding space by using the correspond-
ing RGB colour. Since the data /f_ills a 3-dimensional cube there is no local
manifold structure, and hence for such data we expect largernvalues to be
more useful. Low values will interpret the noise from random sampling as
/f_ine scale manifold structure, producing potentially spurious structure1.
1See the discussion of the constellation eﬀect in Section 6
23

Figure 1: Variation of UMAP hyperparameters nand min-dist result in diﬀerent
embeddings. /T_he data is uniform random samples from a 3-dimensional color-
cube, allowing for easy visualization of the original 3-dimensional coordinates
in the embedding space by using the corresponding RGB colour. Low values of
nspuriously interpret structure from the random sampling noise – see Section 6
for further discussion of this phenomena.
24

In Figure 2 we provides examples of the same hyperparamter choices
as Figure 1, but for the PenDigits dataset 2. In this case we expect small
to medium nvalues to be most eﬀective, since there is signi/f_icant cluster
structure naturally present in the data. /T_he min-dist parameter expands out
tightly clustered groups, allowing more of the internal structure of densely
packed clusters to be seen.
Finally, in Figure 3 we provide an equivalent example of hyperparame-
ter choices for the MNIST dataset3. Again, since this dataset is expected to
have signifcant cluster structure we expect medium sized values ofnto be
most eﬀective. We note that large values of min-dist result in the distinct
clusters being compressed together, making the distinctions between the
clusters less clear.
5 Practical Eﬃcacy
While the strong mathematical foundations of UMAP were the motivation
for its development, the algorithm must ultimately be judged by its prac-
tical eﬃcacy. In this section we examine the /f_idelity and performance of
low dimensional embeddings of multiple diverse real world data sets under
UMAP. /T_he following datasets were considered:
Pen digits[1, 10] is a set of 1797 grayscale images of digits entered using
a digitiser tablet. Each image is an 8x8 image which we treat as a single 64
dimensional vector, assumed to be in Euclidean vector space.
COIL 20[43] is a set of 1440 greyscale images consisting of 20 objects un-
der 72 diﬀerent rotations spanning 360 degrees. Each image is a 128x128
image which we treat as a single 16384 dimensional vector for the purposes
of computing distance between images.
COIL 100[44] is a set of 7200 colour images consisting of 100 objects un-
der 72 diﬀerent rotations spanning 360 degrees. Each image consists of 3
128x128 intensity matrices (one for each color channel). We treat this as
a single 49152 dimensional vector for the purposes of computing distance
between images.
Mouse scRNA-seq[11] is pro/f_iled gene expression data for 20,921 cells
from an adult mouse. Each sample consists of a vector of 26,774 measure-
ments.
Statlog (Shuttle)[35] is a NASA dataset consisting of various data associ-
ated to the positions of radiators in the space shu/t_tle, including a timestamp.
2See Section 5 for a description of the PenDigits dataset
3See section 5 for details on the MNIST dataset
25

Figure 2: Variation of UMAP hyperparameters n and min-dist result in diﬀer-
ent embeddings. /T_he data is the PenDigits dataset, where each point is an 8x8
grayscale image of a hand-wri/t_ten digit.
26

Figure 3: Variation of UMAP hyperparameters n and min-dist result in diﬀer-
ent embeddings. /T_he data is the MNIST dataset, where each point is an 28x28
grayscale image of a hand-wri/t_ten digit.
27

/T_he dataset has 58000 points in a 9 dimensional feature space.
MNIST [32] is a dataset of 28x28 pixel grayscale images of handwri/t_ten
digits. /T_here are 10 digit classes (0 through 9) and 70000 total images. /T_his
is treated as 70000 diﬀerent 784 dimensional vectors.
F-MNIST [63] or Fashion MNIST is a dataset of 28x28 pixel grayscale im-
ages of fashion items (clothing, footwear and bags). /T_here are 10 classes
and 70000 total images. As with MNIST this is treated as 70000 diﬀerent
784 dimensional vectors.
Flow cytometry[51, 9] is a dataset of /f_low cytometry measurements of
CDT4 cells comprised of 1,000,000 samples, each with 17 measurements.
GoogleNews word vectors[41] is a dataset of 3 million words and phrases
derived from a sample of Google News documents and embedded into a 300
dimensional space via word2vec.
For all the datasets except GoogleNews we use Euclidean distance be-
tween vectors. For GoogleNews, as per [41], we use cosine distance (or
angular distance in t-SNE which does support non-metric distances, in con-
trast to UMAP).
5.1 /Q_ualitative Comparison of Multiple Algorithms
We compare a number of algorithms–UMAP, t-SNE [60, 58], LargeVis [54],
Laplacian Eigenmaps [7], and Principal Component Analysis [27]–on the
COIL20 [43], MNIST [32], Fashion-MNIST [63], and GoogleNews [41] datasets.
/T_he Isomap algorithm was also tested, but failed to complete in any reason-
able time for any of the datasets larger than COIL20.
/T_he Multicore t-SNE package [57] was used for t-SNE. /T_he reference
implementation [53] was used for LargeVis. /T_he scikit-learn [10] imple-
mentations were used for Laplacian Eigenmaps and PCA. Where possible
we a/t_tempted to tune parameters for each algorithm to give good embed-
dings.
Historically t-SNE and LargeVis have oﬀered a dramatic improvement
in /f_inding and preserving local structure in the data. /T_his can be seen qual-
itatively by comparing their embeddings to those generated by Laplacian
Eigenmaps and PCA in Figure 4. We claim that the quality of embeddings
produced by UMAP is comparable to t-SNE when reducing to two or three
dimensions. For example, Figure 4 shows both UMAP and t-SNE embed-
dings of the COIL20, MNIST, Fashion MNIST, and Google News datasets.
While the precise embeddings are diﬀerent, UMAP distinguishes the same
structures as t-SNE and LargeVis.
28

Figure 4: A comparison of several dimension reduction algorithms. We note
that UMAP successfully re/f_lects much of the large scale global structure that is
well represented by Laplacian Eigenmaps and PCA (particularly for MNIST and
Fashion-MNIST), while also preserving the local /f_ine structure similar to t-SNE
and LargeVis.
29

It can be argued that UMAP has captured more of the global and topo-
logical structure of the datasets than t-SNE [4, 62]. More of the loops in the
COIL20 dataset are kept intact, including the intertwined loops. Similarly
the global relationships among diﬀerent digits in the MNIST digits dataset
are more clearly captured with 1 (red) and 0 (dark red) at far corners of
the embedding space, and 4,7,9 (yellow, sea-green, and violet) and 3,5,8 (or-
ange, chartreuse, and blue) separated as distinct clumps of similar digits.
In the Fashion MNIST dataset the distinction between clothing (dark red,
yellow, orange, vermilion) and footwear (chartreuse, sea-green, and violet)
is made more clear. Finally, while both t-SNE and UMAP capture groups of
similar word vectors, the UMAP embedding arguably evidences a clearer
global structure among the various word clusters.
5.2 /Q_uantitative Comparison of Multiple Algorithms
We compare UMAP, t-SNE, LargeVis, Laplacian Eigenmaps and PCA em-
beddings with respect to the performance of a k-nearest neighbor clas-
si/f_ier trained on the embedding space for a variety of datasets. /T_he k-
nearest neighbor classi/f_ier accuracy provides a clear quantitative measure
of how well the embedding has preserved the important local structure of
the dataset. By varying the hyper-parameter k used in the training we
can also consider how structure preservation varies under transition from
purely local to non-local, to more global structure. /T_he embeddings used
for training thekNN classi/f_ier are for those datasets that come with de/f_ined
training labels: PenDigits, COIL-20, Shu/t_tle, MNIST, and Fashion-MNIST.
We divide the datasets into two classes: smaller datasets (PenDigits and
COIL-20), for which a smaller range of k values makes sense, and larger
datasets, for which much larger values of k are reasonable. For each of
the small datasets a strati/f_ied 10-fold cross-validation was used to derive
a set of 10 accuracy scores for each embedding. For the Shu/t_tle dataset a
10-fold cross-validation was used due to constraints imposed by class sizes
and the strati/f_ied sampling. For MNIST and Fashion-MNIST a 20-fold cross
validation was used, producing 20 accuracy scores.
In Table 1 we present the average accuracy across the 10-folds for the
PenDigits and COIL-20 datasets. UMAP performs at least as well as t-SNE
and LargeVis (given the con/f_idence bounds on the accuracy) for k in the
range 10 to 40, but for largerkvalues of 80 and 160 UMAP has signi/f_icantly
higher accuracy on COIL-20, and shows evidence of higher accuracy on
PenDigits. Figure 5 provides swarm plots of the accuracy results across the
COIL-20 and PenDigits datasets.
30

In Table 2 we present the average cross validation accuracy for the Shut-
tle, MNIST and Fashion-MNIST datasets. UMAP performs at least as well
as t-SNE and LargeVis (given the con/f_idence bounds on the accuracy) fork
in the range 100 to 400 on the Shu/t_tle and MNIST datasets (but notably un-
derperforms on the Fashion-MNIST dataset), but for larger kvalues of 800
and 3200 UMAP has signi/f_icantly higher accuracy on the Shu/t_tle dataset,
and shows evidence of higher accuracy on MNIST. Forkvalues of 1600 and
3200 UMAP establishes comparable performance on Fashion-MNIST. Fig-
ure 6 provides swarm plots of the accuracy results across the Shu/t_tle and
MNIST and Fashion-MNIST datasets.
k t-SNE UMAP LargeVis Eigenmaps PCA
COIL-20
10 0.934 (± 0.115) 0.921 (± 0.075) 0.888 (± 0.092) 0.629 (± 0.153) 0.667 (± 0.179)
20 0.901 (± 0.133) 0.907 (± 0.064) 0.870 (± 0.125) 0.605 (± 0.185) 0.663 (± 0.196)
40 0.857 (± 0.125) 0.904 (± 0.056) 0.833 (± 0.106) 0.578 (± 0.159) 0.620 (± 0.230)
80 0.789 (± 0.118) 0.899 (± 0.058) 0.803 (± 0.100) 0.565 (± 0.119) 0.531 (± 0.294)
160 0.609 (± 0.067) 0.803 (± 0.138) 0.616 (± 0.066) 0.446 (± 0.110) 0.375 (± 0.111)
PenDigits
10 0.977 (± 0.033) 0.973 (± 0.044) 0.966 (± 0.053) 0.778 (± 0.113) 0.622 (± 0.092)
20 0.973 (± 0.033) 0.976 (± 0.035) 0.973 (± 0.044) 0.778 (± 0.116) 0.633 (± 0.082)
40 0.956 (± 0.064) 0.954 (± 0.060) 0.959 (± 0.066) 0.778 (± 0.112) 0.636 (± 0.078)
80 0.948 (± 0.060) 0.951 (± 0.072) 0.949 (± 0.072) 0.767 (± 0.111) 0.643 (± 0.085)
160 0.949 (± 0.065) 0.951 (± 0.085) 0.921 (± 0.085) 0.747 (± 0.108) 0.629 (± 0.107)
Table 1: kNN Classi/f_ier accuracy for varying values of k over the embedding
spaces of COIL-20 and PenDigits datasets. Average accuracy scores are given
over a 10-fold cross-validation for each of PCA, Laplacian Eigenmaps, LargeVis,
t-SNE and UMAP.
As evidenced by this comparison UMAP provides largely comparable
perfomance in embedding quality to t-SNE and LargeVis at local scales, but
performs markedly be/t_ter than t-SNE or LargeVis at non-local scales. /T_his
bears out the visual qualitative assessment provided in Subsection 5.1.
5.3 Embedding Stability
Since UMAP makes use of both stochastic approximate nearest neighbor
search, and stochastic gradient descent with negative sampling for opti-
mization, the resulting embedding is necessarily diﬀerent from run to run,
and under sub-sampling of the data. /T_his is potentially a concern for a
31

k t-SNE UMAP LargeVis Eigenmaps PCA
Shu/t_tle
100 0.994 (± 0.002) 0.993 (± 0.002) 0.992 (± 0.003) 0.962 (± 0.004) 0.833 (± 0.013)
200 0.992 (± 0.002) 0.990 (± 0.002) 0.987 (± 0.003) 0.957 (± 0.006) 0.821 (± 0.007)
400 0.990 (± 0.002) 0.988 (± 0.002) 0.976 (± 0.003) 0.949 (± 0.006) 0.815 (± 0.007)
800 0.969 (± 0.005) 0.988 (± 0.002) 0.957 (± 0.004) 0.942 (± 0.006) 0.804 (± 0.003)
1600 0.927 (± 0.005) 0.981 (± 0.002) 0.904 (± 0.007) 0.918 (± 0.006) 0.792 (± 0.003)
3200 0.828 (± 0.004) 0.957 (± 0.005) 0.850 (± 0.008) 0.895 (± 0.006) 0.786 (± 0.001)
MNIST
100 0.967 (± 0.015) 0.967 (± 0.014) 0.962 (± 0.015) 0.668 (± 0.016) 0.462 (± 0.023)
200 0.966 (± 0.015) 0.967 (± 0.014) 0.962 (± 0.015) 0.667 (± 0.016) 0.467 (± 0.023)
400 0.964 (± 0.015) 0.967 (± 0.014) 0.961 (± 0.015) 0.664 (± 0.016) 0.468 (± 0.024)
800 0.963 (± 0.016) 0.967 (± 0.014) 0.961 (± 0.015) 0.660 (± 0.017) 0.468 (± 0.023)
1600 0.959 (± 0.016) 0.966 (± 0.014) 0.947 (± 0.015) 0.651 (± 0.014) 0.467 (± 0.0233)
3200 0.946 (± 0.017) 0.964 (± 0.014) 0.920 (± 0.017) 0.639 (± 0.017) 0.459 (± 0.022)
Fashion-MNIST
100 0.818 (± 0.012) 0.790 (± 0.013) 0.808 (± 0.014) 0.631 (± 0.010) 0.564 (± 0.018)
200 0.810 (± 0.013) 0.785 (± 0.014) 0.805 (± 0.013) 0.624 (± 0.013) 0.565 (± 0.016)
400 0.801 (± 0.013) 0.780 (± 0.013) 0.796 (± 0.013) 0.612 (± 0.011) 0.564 (± 0.017)
800 0.784 (± 0.011) 0.767 (± 0.014) 0.771 (± 0.014) 0.600 (± 0.012) 0.560 (± 0.017)
1600 0.754 (± 0.011) 0.747 (± 0.013) 0.742 (± 0.013) 0.580 (± 0.014) 0.550 (± 0.017)
3200 0.727 (± 0.011) 0.730 (± 0.011) 0.726 (± 0.012) 0.542 (± 0.014) 0.533 (± 0.017)
Table 2: kNN Classi/f_ier accuracy for varying values of k over the embedding
spaces of Shu/t_tle, MNIST and Fashion-MNIST datasets. Average accuracy scores
are given over a 10-fold or 20-fold cross-validation for each of PCA, Laplacian
Eigenmaps, LargeVis, t-SNE and UMAP.
32

Figure 5: kNN Classi/f_ier accuracy for varying values ofk over the embedding
spaces of COIL-20 and PenDigits datasets. Accuracy scores are given for each
fold of a 10-fold cross-validation for each of PCA, Laplacian Eigenmaps, LargeVis,
t-SNE and UMAP. We note that UMAP produces competitive accuracy scores to
t-SNE and LargeVis for most cases, and outperforms both t-SNE and LargeVis for
larger kvalues on COIL-20.
variety of uses cases, so establishing some measure of how stable UMAP
embeddings are, particularly under sub-sampling, is of interest. In this sub-
section we compare the stability under subsampling of UMAP, LargeVis and
t-SNE (the three stochastic dimension reduction techniques considered).
To measure the stability of an embedding we make use of the nor-
malized Procrustes distance to measure the distance between two poten-
tially comparable distributions. Given two datasetsX = {x1,...,x N}and
Y = {y1,...,y N}such that xi corresponds to yi, we can de/f_ine the Pro-
custes distance between the datasets dP(X,Y ) in the following manner.
Determine Y′ = {y1′,...,y N′}the optimal translation, uniform scaling,
and rotation of Y that minimizes the squared error ∑N
i=1(xi −yi′)2, and
de/f_ine
dP(X,Y ) =
√
N∑
i=1
(xi −yi′)2.
Since any measure that makes use of distances in the embedding space is
potentially sensitive to the extent or scale of the embedding, we normal-
ize the data before computing the Procrustes distance by dividing by the
average norm of the embedded dataset. In Figure 7 we visualize the re-
sults of using Procrustes alignment of embedding of sub-samples for both
33

Figure 6: kNN Classi/f_ier accuracy for varying values ofk over the embedding
spaces of Shu/t_tle, MNIST and Fashion-MNIST datasets. Accuracy scores are
given for each fold of a 10-fold cross-validation for Shu/t_tle, and 20-fold cross-
validation for MNIST and Fashion-MNIST, for each of PCA, Laplacian Eigen-
maps, LargeVis, t-SNE and UMAP. UMAP performs be/t_ter than the other algo-
rithms for largek, particularly on the Shu/t_tle dataset. For Fashion-MNIST UMAP
provides slightly poorer accuracy than t-SNE and LargeVis at small scales, but is
competitive at larger kvalues.
34

(a) UMAP
 (b) t-SNE
Figure 7: Procrustes based alignment of a 10% subsample (red) against the full
dataset (blue) for the /f_low cytometry dataset for both UMAP and t-SNE.
UMAP and t-SNE, demonstrating how Procrustes distance can measure the
stability of the overall structure of the embedding.
Given a measure of distance between diﬀerent embeddings we can ex-
amine stability under sub-sampling by considering the normalized Pro-
crustes distance between the embedding of a sub-sample, and the corre-
sponding sub-sample of an embedding of the full dataset. As the size of
the sub-sample increases the average distance per point between the sub-
sampled embeddings should decrease, potentially toward some asymptote
of maximal agreement under repeated runs. Ideally this asymptotic value
would be zero error, but for stochastic embeddings such as UMAP and t-
SNE this is not achievable.
We performed an empirical comparison of algorithms with respect to
stability using the Flow Cytometry dataset due its large size, interesting
structure, and low ambient dimensionality (aiding runtime performance
for t-SNE). We note that for a dataset this large we found it necessary to
increase the defaultn_iter value for t-SNE from 1000 to 1500 to ensure bet-
ter convergence. While this had an impact on the runtime, it signi/f_icantly
improved the Procrustes distance results by providing more stable and con-
sistent embeddings. Figure 8 provides a comparison between UMAP and
t-SNE, demonstrating that UMAP has signifcantly more stable results than
35

t-SNE. In particular, a/f_ter sub-sampling on 5% of the million data points, the
per point error for UMAP was already below any value achieved by t-SNE.
5.4 Computational Performance Comparisons
Benchmarks against the real world datasets were performed on a Macbook
Pro with a 3.1 GHz Intel Core i7 and 8GB of RAM for Table 3, and on a
server with Intel Xeon E5-2697v4 processors and 512GB of RAM for the
large scale benchmarking in Subsections 5.4.1, 5.4.2, and 5.4.3.
For t-SNE we chose MulticoreTSNE [57], which we believe to be the
fastest extant implementation of Barnes-Hut t-SNE at this time, even when
run in single core mode. It should be noted that MulticoreTSNE is a heav-
ily optimized implementation wri/t_ten in C++ based on Van der Maaten’s
bhtsne [58] code.
As a fast alternative approach to t-SNE we also consider the FIt-SNE
algorithm [37]. We used the reference implementation [36], which, like
MulticoreTNSE is an optimized C++ implementation. We also note that
FIt-SNE makes use of multiple cores.
LargeVis [54] was benchmarked using the reference implementation
[53]. It was run with default parameters including use of 8 threads on the 4-
core machine. /T_he only exceptions were small datasets where we explicitly
set the -samples parameter to n_samples/100 as per the recommended
values in the documentation of the reference implementation.
/T_he Isomap [55] and Laplacian Eigenmaps [7] implementations in scikit-
learn [10] were used. We suspect the Laplacian eigenmaps implementation
may not be well optimized for large datasets but did not /f_ind a be/t_ter per-
forming implementation that provided comparable quality results. Isomap
failed to complete for the Shu/t_tle, Fashion-MNIST, MNIST and Google-
News datasets, while Laplacian Eigenmaps failed to run for the Google-
News dataset.
To allow a broader range of algorithms to run some of the datasets
where subsampled or had their dimension reduced by PCA. /T_he Flow Cy-
tometry dataset was benchmarked on a 10% sample and the GoogleNews
was subsampled down to 200,000 data points. Finally, the Mouse scRNA
dataset was reduced to 1,000 dimensions via PCA.
Timing were performed for the COIL20 [43], COIL100 [44], Shu/t_tle [35],
MNIST [32], Fashion-MNIST [63], and GoogleNews [41] datasets. Results
can be seen in Table 3. UMAP consistently performs faster than any of
the other algorithms aside from on the very small Pendigits dataset, where
Laplacian Eigenmaps and Isomap have a small edge.
36

Figure 8: Comparison of average Procustes distance per point for t-SNE, LargeVis
and UMAP over a variety of sizes of subsamples from the full Flow Cytometry
dataset. UMAP sub-sample embeddings are very close to the full embedding even
for subsamples of 5% of the full dataset, outperforming the results of t-SNE and
LargeVis even when they use the full Flow Cytometry dataset.
37

UMAP FIt-SNE t-SNE LargeVis Eigenmaps Isomap
Pen Digits 9s 48s 17s 20s 2s 2s
(1797x64)
COIL20 12s 75s 22s 82s 47s 58s
(1440x16384)
COIL100 85s 2681s 810s 3197s 3268s 3210s
(7200x49152)
scRNA 28s 131s 258s 377s 470s 923s
(21086x1000)
Shuttle 94s 108s 714s 615s 133s –
(58000x9)
MNIST 87s 292s 1450s 1298s 40709s –
(70000x784)
F-MNIST 65s 278s 934s 1173s 6356s –
(70000x784)
Flow 102s 164s 1135s 1127s 30654s –
(100000x17)
Google News 361s 652s 16906s 5392s – –
(200000x300)
Table 3: Runtime of several dimension reduction algorithms on various datasets.
To allow a broader range of algorithms to run some of the datasets where sub-
sampled or had their dimension reduced by PCA. /T_he Flow Cytometry dataset
was benchmarked on a 10% sample and the GoogleNews was subsampled down
to 200,000 data points. Finally, the Mouse scRNA dataset was reduced to 1,000
dimensions via PCA. /T_he fastest runtime for each dataset has been bolded.
38

5.4.1 Scaling with Embedding Dimension
UMAP is signi/f_icantly more performant than t-SNE4 when embedding into
dimensions larger than 2. /T_his is particularly important when the intention
is to use the low dimensional representation for further machine learning
tasks such as clustering or anomaly detection rather than merely for visu-
alization. /T_he computation performance of UMAP is far more eﬃcient than
t-SNE, even for very small embedding dimensions of 6 or 8 (see Figure 9).
/T_his is largely due to the fact that UMAP does not require global normali-
sation (since it represents data as a fuzzy topological structure rather than
as a probability distribution). /T_his allows the algorithm to work without
the need for space trees —such as the quad-trees and oct-trees that t-SNE
uses [58]—. Such space trees scale exponentially in dimension, resulting
in t-SNE’s relatively poor scaling with respect to embedding dimension.
By contrast, we see that UMAP consistently scales well in embedding di-
mension, making the algorithm practical for a wider range of applications
beyond visualization.
5.4.2 Scaling with Ambient Dimension
/T_hrough a combination of the local-connectivity constraint and the approx-
imate nearest neighbor search, UMAP can perform eﬀective dimension re-
duction even for very high dimensional data (see Figure 13 for an example
of UMAP operating directly on 1.8 million dimensional data). /T_his stands in
contrast to many other manifold learning techniques, including t-SNE and
LargeVis, for which it is generally recommended to reduce the dimension
with PCA before applying these techniques (see [59] for example).
To compare runtime performance scaling with respect to the ambient
dimension of the data we chose to use the Mouse scRNA dataset, which
is high dimensional, but is also amenable to the use of PCA to reduce the
dimension of the data as a pre-processing step without losing too much
of the important structure 5. We compare the performance of UMAP, FIt-
SNE, MulticoreTSNE, and LargeVis on PCA reductions of the Mouse scRNA
dataset to varying dimensionalities, and on the original dataset, in Figure
10.
While all the implementations tested show a signi/f_icant increase in run-
time with increasing dimension, UMAP is dramatically more eﬃcient for
4Comparisons were performed against MulticoreTSNE as the current implementation of FIt-
SNE does not support embedding into any dimension larger than 2.
5In contrast to COIL100, on which PCA destroys much of the manifold structure
39

(a) A comparison of run time for UMAP,
t-SNE and LargeVis with respect to em-
bedding dimension on the Pen digits
dataset. We see that t-SNE scales worse
than exponentially while UMAP and
LargeVis scale linearly with a slope so
slight to be undetectable at this scale.
(b) Detail of scaling for embedding di-
mension of six or less. We can see that
UMAP and LargeVis are essentially /f_lat.
In practice they appear to scale linearly,
but the slope is essentially undetectable
at this scale.
Figure 9: Scaling performance with respect to embedding dimension of UMAP,
t-SNE and LargeVis on the Pen digits dataset.
40

Figure 10: Runtime performance scaling of UMAP, t-SNE, FIt-SNE and Largevis
with respect to the ambient dimension of the data. As the ambient dimension
increases beyond a few thousand dimensions the computational cost of t-SNE,
FIt-SNE, and LargeVis all increase dramatically, while UMAP continues to per-
form well into the tens-of-thousands of dimensions.
41

large ambient dimensions, easily scaling to run on the original unreduced
dataset. /T_he ability to run manifold learning on raw source data, rather than
dimension reduced data that may have lost important manifold structure in
the pre-processing, is a signi/f_icant advantage. /T_his advantage comes from
the local connectivity assumption which ensures good topological repre-
sentation of high dimensional data, particularly with smaller numbers of
near neighbors, and the eﬃciency of the NN-Descent algorithm for approx-
imate nearest neighbor search even in high dimensions.
Since UMAP scales well with ambient dimension the python implemen-
tation also supports input in sparse matrix format, allowing scaling to ex-
tremely high dimensional data, such as the integer data shown in Figures
13 and 14.
5.4.3 Scaling with the Number of Samples
For dataset size performance comparisons we chose to compare UMAP with
FIt-SNE [37], a version of t-SNE that uses approximate nearest neighbor
search and a Fourier interpolation optimisation approach; MulticoreTSNE
[57], which we believe to be the fastest extant implementation of Barnes-
Hut t-SNE; and LargeVis [54]. It should be noted that FIt-SNE, MulticoreT-
SNE, and LargeVis are all heavily optimized implementations wri/t_ten in
C++. In contrast our UMAP implementation was wri/t_ten in Python — mak-
ing use of the numba [31] library for performance. MulticoreTSNE and
LargeVis were run in single threaded mode to make fair comparisons to
our single threaded UMAP implementation.
We benchmarked all four implementations using subsamples of the Google-
News dataset. /T_he results can be seen in Figure 11. /T_his demonstrates that
UMAP has superior scaling performance in comparison to Barnes-Hut t-
SNE, even when Barnes-Hut t-SNE is given multiple cores. Asymptotic
scaling of UMAP is comparable to that of FIt-SNE (and LargeVis). On this
dataset UMAP demonstrated somewhat faster absolute performance com-
pared to FIt-SNE, and was dramatically faster than LargeVis.
/T_he UMAP embedding of the full GoogleNews dataset of 3 million word
vectors, as seen in Figure 12, was completed in around 200 minutes, as com-
pared with several days required for MulticoreTSNE, even using multiple
cores.
To scale even further we were inspired by the work of John Williamson
on embedding integers [61], as represented by (sparse) binary vectors of
their prime divisibility. /T_his allows the generation of arbitrarily large, ex-
tremely high dimension datasets that still have meaningful structure to be
42

Figure 11: Runtime performance scaling of t-SNE and UMAP on various sized
sub-samples of the full Google News dataset. /T_he lower t-SNE line is the wall
clock runtime for Multicore t-SNE using 8 cores.
43

Figure 12: Visualization of the full 3 million word vectors from the GoogleNews
dataset as embedded by UMAP.
44

explored. In Figures 13 and 14 we show an embedding of 30,000,000 data
samples from an ambient space of approximately 1.8 million dimensions.
/T_his computation took approximately 2 weeks on a large memory SMP.
Note that despite the high ambient dimension, and vast amount of data,
UMAP is still able to /f_ind and display interesting structure. In Figure 15 we
show local regions of the embedding, demonstrating the /f_ine detail struc-
ture that was captured.
6 Weaknesses
While we believe UMAP to be a very eﬀective algorithm for both visualiza-
tion and dimension reduction, most algorithms must make trade-oﬀs and
UMAP is no exception. In this section we will brie/f_ly discuss those areas or
use cases where UMAP is less eﬀective, and suggest potential alternatives.
For a number of use cases the interpretability of the reduced dimension
results is of critical importance. Similarly to most non-linear dimension re-
duction techniques (including t-SNE and Isomap), UMAP lacks the strong
interpretability of Principal Component Analysis (PCA) and related tech-
niques such a Non-Negative Matrix Factorization (NMF). In particular the
dimensions of the UMAP embedding space have no speci/f_ic meaning, un-
like PCA where the dimensions are the directions of greatest variance in
the source data. Furthermore, since UMAP is based on the distance between
observations rather than the source features, it does not have an equivalent
of factor loadings that linear techniques such as PCA, or Factor Analysis
can provide. If strong interpretability is critical we therefore recommend
linear techniques such as PCA, NMF or pLSA.
One of the core assumptions of UMAP is that there exists manifold
structure in the data. Because of this UMAP can tend to /f_ind manifold
structure within the noise of a dataset – similar to the way the human mind
/f_inds structured constellations among the stars. As more data is sampled
the amount of structure evident from noise will tend to decrease and UMAP
becomes more robust, however care must be taken with small sample sizes
of noisy data, or data with only large scale manifold structure. Detecting
when a spurious embedding has occurred is a topic of further research.
UMAP is derived from the axiom that local distance is of more im-
portance than long range distances (similar to techniques like t-SNE and
LargeVis). UMAP therefore concerns itself primarily with accurately rep-
resenting local structure. While we believe that UMAP can capture more
global structure than these other techniques, it remains true that if global
45

Figure 13: Visualization of 30,000,000 integers as represented by binary vectors
of prime divisibility, colored by density of points.
46

Figure 14: Visualization of 30,000,000 integers as represented by binary vectors
of prime divisibility, colored by integer value of the point (larger values are green
or yellow, smaller values are blue or purple).
47

(a) Upper right spiral
 (b) Lower right spiral and starbursts
(c) Central cloud
Figure 15: Zooming in on various regions of the integer embedding reveals fur-
ther layers of /f_ine structure have been preserved.
48

structure is of primary interest then UMAP may not be the best choice for
dimension reduction. Multi-dimensional scaling speci/f_ically seeks to pre-
serve the full distance matrix of the data, and as such is a good candidate
when all scales of structure are of equal importance. PHATE [42] is a good
example of a hybrid approach that begins with local structure information
and makes use of MDS to a/t_tempt to preserve long scale distances as well. It
should be noted that these techniques are more computationally intensive
and thus rely on landmarking approaches for scalability.
It should also be noted that a signi/f_icant contributor to UMAP’s relative
global structure preservation is derived from the Laplacian Eigenmaps ini-
tialization (which, in turn, followed from the theoretical foundations). /T_his
was noted in, for example, [29]. /T_he authors of that paper demonstrate
that t-SNE, with similar initialization, can perform equivalently to UMAP
in a particular measure of global structure preservation. However, the ob-
jective function derived for UMAP (cross-entropy) is signi/f_icantly diﬀerent
from that of t-SNE (KL-divergence), in how it penalizes failures to preserve
non-local and global structure, and is also a signi/f_icant contributor6.
It is worth noting that, in combining the local simplicial set structures,
pure nearest neighbor structure in the high dimensional space is not ex-
plicitly preserved. In particular it introduces so called ”reverse-nearest-
neighbors” into the classical knn-graph. /T_his, combined with the fact that
UMAP is preserving topology rather than pure metric structures, mean that
UMAP will not perform as well as some methods on quality measures based
on metric structure preservation – particularly methods, such as MDS –
which are explicitly designed to optimize metric structure preservation.
UMAP a/t_tempts to discover a manifold on which your data is uniformly
distributed. If you have strong con/f_idence in the ambient distances of your
data you should make use of a technique that explicitly a/t_tempts to preserve
these distances. For example if your data consisted of a very loose structure
in one area of your ambient space and a very dense structure in another
region region UMAP would a/t_tempt to put these local areas on an even
footing.
Finally, to improve the computational eﬃciency of the algorithm a num-
ber of approximations are made. /T_his can have an impact on the results
of UMAP for small (less than 500 samples) dataset sizes. In particular the
use of approximate nearest neighbor algorithms, and the negative sampling
used in optimization, can result in suboptimal embeddings. For this reason
6/T_he authors would like to thank Nikolay Oskolkov for his article (tSNE vs. UMAP: Global
Structure) which does an excellent job of highlighting these aspects from an empirical and the-
oretical basis.
49

we encourage users to take care with particularly small datasets. A slower
but exact implementation of UMAP for small datasets is a future project.
7 Future Work
Having established both relevant mathematical theory and a concrete im-
plementation, there still remains signi/f_icant scope for future developments
of UMAP.
A comprehensive empirical study which examines the impact of the
various algorithmic components, choices, and hyper-parameters of the al-
gorithm would be bene/f_icial. While the structure and choices of the algo-
rithm presented were derived from our foundational mathematical frame-
work, examining the impacts that these choices have on practical results
would be enlightening and a signi/f_icant contribution to the literature.
As noted in the weaknesses section there is a great deal of uncertainty
surrounding the preservation of global structure among the /f_ield of man-
ifold learning algorithms. In particular this is hampered by the lack clear
objective measures, or even de/f_initions, of global structure preservation.
While some metrics exist, they are not comprehensive, and are o/f_ten spe-
ci/f_ic to various downstream tasks. A systematic study of both metrics of
non-local and global structure preservation, and performance of various
manifold learning algorithms with respect to them, would be of great ben-
e/f_it. We believe this would aid in be/t_ter understanding UMAP’s success in
various downstream tasks.
Making use of the fuzzy simplicial set representation of data UMAP can
potentially be extended to support (semi-)supervised dimension reduction,
and dimension reduction for datasets with heterogeneous data types. Each
data type (or prediction variables in the supervised case) can be seen as an
alternative view of the underlying structure, each with a diﬀerent associ-
ated metric – for example categorical data may use Jaccard or Dice distance,
while ordinal data might use Manha/t_tan distance. Each view and metric can
be used to independently generate fuzzy simplicial sets, which can then be
intersected together to create a single fuzzy simplicial set for embedding.
Extending UMAP to work with mixed data types would vastly increase the
range of datasets to which it can be applied. Use cases for (semi-)supervised
dimension reduction include semi-supervised clustering, and interactive la-
belling tools.
/T_he computational framework established for UMAP allows for the po-
tential development of techniques to add new unseen data points into an
50

existing embedding, and to generate high dimensional representations of
arbitrary points in the embedded space. Furthermore, the combination of
supervision and the addition of new samples to an existing embedding pro-
vides avenues for metric learning. /T_he addition of new samples to an ex-
isting embedding would allow UMAP to be used as a feature engineering
tool as part of a general machine learning pipeline for either clustering or
classi/f_ication tasks. Pulling points back to the original high dimensional
space from the embedded space would potentially allow UMAP to be used
as a generative model similar to some use cases for autoencoders. Finally,
there are many use cases for metric learning; see [64] or [8] for example.
/T_here also remains signi/f_icant scope to develop techniques to both de-
tect and mitigate against potentially spurious embeddings, particularly for
small data cases. /T_he addition of such techniques would make UMAP far
more robust as a tool for exploratory data analysis, a common use case
when reducing to two dimensions for visualization purposes.
Experimental versions of some of this work are already available in the
referenced implementations.
8 Conclusions
We have developed a general purpose dimension reduction technique that
is grounded in strong mathematical foundations. /T_he algorithm imple-
menting this technique is demonstrably faster than t-SNE and provides
be/t_ter scaling. /T_his allows us to generate high quality embeddings of larger
data sets than had previously been a/t_tainable. /T_he use and eﬀectiveness
of UMAP in various scienti/f_ic /f_ields demonstrates the strength of the algo-
rithm.
Acknowledgements /T_he authors would like to thank Colin Weir, Rick
Jardine, Brendan Fong, David Spivak and Dmitry Kobak for discussion and
useful commentary on various dra/f_ts of this paper.
A Proof of Lemma 1
Lemma 1.Let (M,g) be a Riemannian manifold in an ambientRn, and let
p∈M be a point. Ifgis locally constant aboutpin an open neighbourhood
U such thatgis a constant diagonal matrix in ambient coordinates, then in a
ball B ⊆Ucentered atpwith volume πn/2
Γ(n/2+1) with respect tog, the geodesic
51

distance frompto any pointq∈Bis 1
rdRn(p,q), whereris the radius of the
ball in the ambient space anddRn is the existing metric on the ambient space.
Proof. Let x1,...,x n be the coordinate system for the ambient space. A
ball Bin Munder Riemannian metric ghas volume given by
∫
B
√
det(g)dx1 ∧···∧ dxn.
If Bis contained in U, then gis constant in Band hence
√
det(g) is con-
stant and can be brought outside the integral. /T_hus, the volume ofBis
√
det(g)
∫
B
dx1 ∧...∧dxn =
√
det(g) πn/2rn
Γ(n/2 + 1),
where ris the radius of the ball in the ambient Rn. If we /f_ix the volume of
the ball to be πn/2
Γ(n/2+1) we arrive at the requirement that
det(g) = 1
r2n.
Now, since gis assumed to be diagonal with constant entries we can solve
for gitself as
gij =



1
r2 if i= j,
0 otherwise
. (2)
/T_he geodesic distance onMunder gfrom pto q(where p,q ∈B) is de/f_ined
as
inf
c∈C
∫ b
a
√
g( ˙c(t),˙c(t))dt,
where C is the class of smooth curves c on Msuch that c(a) = p and
c(b) = q, and ˙cdenotes the /f_irst derivative ofcon M. Given that g is as
de/f_ined in (2) we see that this can be simpli/f_ied to
1
r inf
c∈C
∫ b
a
⟨
√
˙c(t),˙c(t)⟩dt
=1
r inf
c∈C
∫ b
a
⟨∥˙c(t),˙c(t)∥dt
=1
rdRn(p,q).
(3)
52

B Proof that FinReal and FinSing are adjoint
/T_heorem 2./T_he functorsFinReal : Fin-sFuzz →FinEPMet and FinSing :
FinEPMet →Fin-sFuzz form an adjunction withFinReal the le/f_t adjoint
and FinSing the right adjoint.
Proof. /T_he adjunction is evident by construction, but can be made more
explicit as follows. De/f_ine a functorF : ∆ ×I →FinEPMet by
F([n],[0,a)) = ({x1,x2,...,x n},da),
where
da(xi,xj) =



−log(a) if i̸= j,
0 otherwise
.
Now FinSing can be de/f_ined in terms ofF as
FinSing(Y) : ([n],[0,a)) ↦→homFinEPMet(F([n],[0,a)),Y ).
where the face maps di are given by pre-composition with Fdi, and sim-
ilarly for degeneracy maps, at any given value of a. Furthermore post-
composition with F level-wise for each ade/f_ines maps of fuzzy simplicial
sets making FinSing a functor.
We now construct FinReal as the le/f_t Kan extension of F along the
Yoneda embedding:
Fin-sFuzz
FinReal
→ → 
∆ ×I
→ ↓ 
y
↗ ↗ 
F
→ → FinEPMet
Explicitly this results in a de/f_inition ofFinReal at a fuzzy simplicial set X
as a colimit:
FinReal(X) = colim
y([n],[0,a))→X
F([n]).
Further, it follows from the Yoneda lemma thatFinReal(∆n
<a) ∼= F([n],[0,a)),
and hence this de/f_inition as a le/f_t Kan extension agrees with De/f_inition 7,
and the de/f_inition ofFinSing above agrees with that of De/f_inition 8. To see
that FinReal and FinSing are adjoint we note that
homFin-sFuzz(∆n
<a,FinSing(Y)) ∼= FinSing(Y)n
<a
= homFinEPMet(F([n],[0,a)),Y )
∼= homFinEPMet(FinReal(∆n
<a),Y )).
(4)
53

/T_he /f_irst isomorphism follows from the Yoneda lemma, the equality is by
construction, and the /f_inal isomorphism follows by another application of
the Yoneda lemma. Since every simplicial set can be canonically expressed
as a colimit of standard simplices and FinReal commutes with colimits (as
it was de/f_ined via a colimit formula), it follows thatFinReal is completely
determined by its image on standard simplices. As a result the isomor-
phism of equation (4) extends to the required isomorphism demonstrating
the adjunction.
C From t-SNE to UMAP
As an aid to implementation of UMAP and to illuminate the algorithmic
similarities with t-SNE and LargeVis, here we review the main equations
used in those methods, and then present the equivalent UMAP expressions
in a notation which may be more familiar to users of those other methods.
In what follows we are concerned with de/f_ining similarities between
two objects i and j in the high dimensional input space X and low di-
mensional embedded space Y. /T_hese are normalized and symmetrized in
various ways. In a typical implementation, these pair-wise quantities are
stored and manipulated as (potentially sparse) matrices. /Q_uantities with
the subscript ij are symmetric, i.e. vij = vji. Extending the conditional
probability notation used in t-SNE,j |iindicates an asymmetric similarity,
i.e. vj|i ̸= vi|j.
t-SNE de/f_ines input probabilities in three stages. First, for each pair of
points, iand j, in X, a pair-wise similarity,vij, is calculated, Gaussian with
respect to the Euclidean distance between xi and xj:
vj|i = exp(−∥xi −xj∥2
2 /2σ2
i) (5)
where σ2
i is the variance of the Gaussian.
Second, the similarities are converted into N conditional probability
distributions by normalization:
pj|i = vj|i∑
k̸=ivk|i
(6)
σi is chosen by searching for a value such that the perplexity of the proba-
bility distribution p·|i matches a user-speci/f_ied value.
/T_hird, these probability distributions are symmetrized and then further
normalized over the entire matrix of values to give a joint probability dis-
tribution:
54

pij = pj|i + pi|j
2N (7)
We note that this is a heuristic de/f_inition and not in accordance with stan-
dard relationship between conditional and joint probabilities that would be
expected under probability semantics usually used to describe t-SNE.
Similarities between pairs of points in the output space Y are de/f_ined
using a Student t-distribution with one degree of freedom on the squared
Euclidean distance:
wij =
(
1 + ∥yi −yj∥2
2
)−1
(8)
followed by the matrix-wise normalization, to form qij:
qij = wij∑
k̸=lwkl
(9)
/T_he t-SNE cost is the Kullback-Leibler divergence between the two proba-
bility distributions:
Ct−SNE =
∑
i̸=j
pij log pij
qij
(10)
this can be expanded into constant and non-constant contributions:
Ct−SNE =
∑
i̸=j
pij log pij −pij log qij (11)
Because both pij and qij require calculations over all pairs of points, im-
proving the eﬃciency of t-SNE algorithms has involved separate strategies
for approximating these quantities. Similarities in the high dimensions are
eﬀectively zero outside of the nearest neighbors of each point due to the
calibration of thepj|ivalues to reproduce a desired perplexity. /T_herefore an
approximation used in Barnes-Hut t-SNE is to only calculatevj|ifor nnear-
est neighbors of i, where nis a multiple of the user-selected perplexity and
to assume vj|i = 0 for all other j. Because the low dimensional coordinates
change with each iteration, a diﬀerent approach is used to approximate
qij. In Barnes-Hut t-SNE and related methods this usually involves group-
ing together points whose contributions can be approximated as a single
point.
A further heuristic algorithm optimization technique employed by t-
SNE implementations is the use ofearly exaggerationwhere, for some num-
ber of initial iterations, thepij are multiplied by some constant greater than
55

1.0 (usually 12.0). In theoretical analyses of t-SNE such as [38] results are
obtained only under an early exaggerationregimen with either large con-
stant (of order of the number of samples), or in the limit of in/f_inite exagger-
ation. Further papers such as [37], and [28], suggest the option of using ex-
aggeration for all iterations rather than just early ones, and demonstrate the
utility of this. /T_he eﬀectiveness of these analyses and practical approaches
suggests that KL-divergence as a measure betweenprobability distributions
is not what makes the t-SNE algorithm work, since, under exaggeration, the
pij are manifestly not a probability distribution. /T_his is another example
of the probability semantics used to describe t-SNE are primarily descrip-
tive rather than foundational. None the less, t-SNE is highly eﬀective and
clearly produces useful results on a very wide variety of tasks.
LargeVis uses a similar approach to Barnes-Hut t-SNE when approxi-
mating pij, but further improves eﬃciency by only requiring approximate
nearest neighbors for each point. For the low dimensional coordinates,
it abandons normalization of wij entirely. Rather than use the Kullback-
Leibler divergence, it optimizes a likelihood function, and hence is maxi-
mized, not minimized:
CLV =
∑
i̸=j
pij log wij + γ
∑
i̸=j
log (1−wij) (12)
pij and wij are de/f_ined as in Barnes-Hut t-SNE (apart from the use of
approximate nearest neighbors forpij, and the fact that, in implementation,
LargeVis does not normalize the pij by N) and γis a user-chosen positive
constant which weights the strength of the the repulsive contributions (sec-
ond term) relative to the a/t_tractive contribution (/f_irst term). Note also that
the /f_irst term resembles the optimizable part of the Kullback-Leibler diver-
gence but usingwij instead of qij. Abandoning calculation ofqij is a crucial
change, because the LargeVis cost function is amenable to optimization via
stochastic gradient descent.
Ignoring speci/f_ic de/f_initions ofvij and wij, the UMAP cost function,
the cross entropy, is:
CUMAP =
∑
i̸=j
vij log
(vij
wij
)
+ (1 −vij) log
(1 −vij
1 −wij
)
(13)
Like the Kullback-Leibler divergence, this can be arranged into two con-
stant contributions (those containing vij only) and two optimizable contri-
butions (containing wij):
56

CUMAP =
∑
i̸=j
vij log vij + (1 −vij) log (1−vij)
−vij log wij −(1 −vij) log (1−wij)
(14)
Ignoring the two constant terms, the UMAP cost function has a very
similar form to that of LargeVis, but without a γ term to weight the re-
pulsive component of the cost function, and without requiring matrix-wise
normalization in the high dimensional space. /T_he cost function for UMAP
can therefore be optimized (in this case, minimized) with stochastic gradi-
ent descent in the same way as LargeVis.
Although the above discussion places UMAP in the same family of meth-
ods as t-SNE and LargeVis, it does not use the same de/f_initions forvij and
wij. Using the notation established above, we now provide the equivalent
expressions for the UMAP similarities. In the high dimensional space, the
similarities vj|iare the local fuzzy simplicial set memberships, based on the
smooth nearest neighbors distances:
vj|i = exp[(−d(xi,xj) −ρi)/σi] (15)
As with LargeVis,vj|iis calculated only fornapproximate nearest neigh-
bors and vj|i = 0 for all other j. d(xi,xj) is the distance between xi and
xj, which UMAP does not require to be Euclidean. ρi is the distance to the
nearest neighbor of i. σi is the normalizing factor, which is chosen by Al-
gorithm 3 and plays a similar role to the perplexity-based calibration of σi
in t-SNE. Calculation of vj|i with Equation 15 corresponds to Algorithm 2.
Symmetrization is carried out by fuzzy set union using the probabilistic
t-conorm and can be expressed as:
vij =
(
vj|i + vi|j
)
−vj|ivi|j (16)
Equation 16 corresponds to forming top-rep in Algorithm 1. Unlike t-SNE,
further normalization is not carried out.
/T_he low dimensional similarities are given by:
wij =
(
1 + a∥yi −yj∥2b
2
)−1
(17)
where aand bare user-de/f_ined positive values. /T_he procedure for /f_inding
them is given in De/f_inition 11. Use of this procedure with the default values
in the UMAP implementation results in a≈1.929 and b≈0.7915. Se/t_ting
a= 1 and b= 1 results in the Student t-distribution used in t-SNE.
57

References
[1] E Alpaydin and Fevzi Alimoglu. Pen-based recognition of handwrit-
ten digits data set. university of california, irvine. Machine Learning
Repository. Irvine: University of California, 4(2), 1998.
[2] Frederik Otzen Bagger, Savvas Kinalis, and Nicolas Rapin. Bloodspot:
a database of healthy and malignant haematopoiesis updated with pu-
ri/f_ied and single cell mrna sequencing pro/f_iles.Nucleic Acids Research,
2018.
[3] Michael Barr. Fuzzy set theory and topos theory. Canad. Math. Bull,
29(4):501–508, 1986.
[4] Etienne Becht, Charles-Antoine Dutertre, Immanuel W.H. Kwok,
Lai Guan Ng, Florent Ginhoux, and Evan W Newell. Evaluation of
umap as an alternative to t-sne for single-cell data. bioRxiv, 2018.
[5] Etienne Becht, Leland McInnes, John Healy, Charles-Antoine
Dutertre, Immanuel WH Kwok, Lai Guan Ng, Florent Ginhoux, and
Evan W Newell. Dimensionality reduction for visualizing single-cell
data using umap. Nature biotechnology, 37(1):38, 2019.
[6] Mikhail Belkin and Partha Niyogi. Laplacian eigenmaps and spec-
tral techniques for embedding and clustering. In Advances in neural
information processing systems, pages 585–591, 2002.
[7] Mikhail Belkin and Partha Niyogi. Laplacian eigenmaps for dimen-
sionality reduction and data representation. Neural computation,
15(6):1373–1396, 2003.
[8] Aur ´elien Bellet, Amaury Habrard, and Marc Sebban. A survey on
metric learning for feature vectors and structured data.arXiv preprint
arXiv:1306.6709, 2013.
[9] Tess Brodie, Elena Brenna, and Federica Sallusto. Omip-018:
Chemokine receptor expression on human t helper cells. Cytometry
Part A, 83(6):530–532, 2013.
[10] Lars Buitinck, Gilles Louppe, Mathieu Blondel, Fabian Pedregosa,
Andreas Mueller, Olivier Grisel, Vlad Niculae, Peter Pre/t_tenhofer,
Alexandre Gramfort, Jaques Grobler, Robert Layton, Jake VanderPlas,
Arnaud Joly, Brian Holt, and Ga¨el Varoquaux. API design for machine
learning so/f_tware: experiences from the scikit-learn project. InECML
PKDD Workshop: Languages for Data Mining and Machine Learning,
pages 108–122, 2013.
58

[11] John N Campbell, Evan Z Macosko, Henning Fenselau, Tune H Pers,
Anna Lyubetskaya, Danielle Tenen, Melissa Goldman, Anne MJ Ver-
stegen, Jon M Resch, Steven A McCarroll, et al. A molecular census of
arcuate hypothalamus and median eminence cell types. Nature neu-
roscience, 20(3):484, 2017.
[12] Junyue Cao, Malte Spielmann, Xiaojie Qiu, Xingfan Huang, Daniel M
Ibrahim, Andrew J Hill, Fan Zhang, Stefan Mundlos, Lena Chris-
tiansen, Frank J Steemers, et al. /T_he single-cell transcriptional land-
scape of mammalian organogenesis. Nature, page 1, 2019.
[13] Gunnar Carlsson and Facundo M ´emoli. Classifying clustering
schemes. Foundations of Computational Mathematics, 13(2):221–252,
2013.
[14] Shan Carter, Zan Armstrong, Ludwig Schubert, Ian John-
son, and Chris Olah. Activation atlas. Distill, 2019.
h/t_tps://distill.pub/2019/activation-atlas.
[15] Brian Clark, Genevieve Stein-O’Brien, Fion Shiau, Gabrielle Can-
non, Emily Davis, /T_homas Sherman, Fatemeh Rajaii, Rebecca James-
Esposito, Richard Gronostajski, Elana Fertig, et al. Comprehensive
analysis of retinal development at single cell resolution identi/f_ies n/f_i
factors as essential for mitotic exit and speci/f_ication of late-born cells.
bioRxiv, page 378950, 2018.
[16] Ronald R Coifman and St ´ephane Lafon. Diﬀusion maps. Applied and
computational harmonic analysis, 21(1):5–30, 2006.
[17] Alex Diaz-Papkovich, Luke Anderson-Trocme, and Simon Gravel. Re-
vealing multi-scale population structure in large cohorts. bioRxiv,
page 423632, 2018.
[18] Wei Dong, Charikar Moses, and Kai Li. Eﬃcient k-nearest neighbor
graph construction for generic similarity measures. In Proceedings
of the 20th International Conference on World Wide Web, WWW ’11,
pages 577–586, New York, NY, USA, 2011. ACM.
[19] Carlos Escolano, Marta R Costa-juss `a, and Jos ´e AR Fonollosa. (self-
a/t_tentive) autoencoder-based universal language representation for
machine translation. arXiv preprint arXiv:1810.06351, 2018.
[20] Mateus Espadoto, Nina ST Hirata, and Alexandru C Telea. Deep learn-
ing multidimensional projections. arXiv preprint arXiv:1902.07958,
2019.
59

[21] Mateus Espadoto, Francisco Caio M Rodrigues, and Alexandru C
Telea. Visual analytics of multidimensional projections for construct-
ing classi/f_ier decision boundary maps.
[22] Greg Friedman et al. Survey article: an elementary illustrated intro-
duction to simplicial sets. Rocky Mountain Journal of Mathematics,
42(2):353–423, 2012.
[23] Lukas Fuhrimann, Vahid Moosavi, Patrick Ole Ohlbrock, and Pierluigi
Dacunto. Data-driven design: Exploring new structural forms using
machine learning and graphic statics. arXiv preprint arXiv:1809.08660,
2018.
[24] Benoit Gaujac, Ilya Feige, and David Barber. Gaussian mixture models
with wasserstein distance. arXiv preprint arXiv:1806.04465, 2018.
[25] Paul G Goerss and John F Jardine. Simplicial homotopy theory.
Springer Science & Business Media, 2009.
[26] Ma/t_thias Hein, Jean-Yves Audibert, and Ulrike von Luxburg. Graph
laplacians and their convergence on random neighborhood graphs.
Journal of Machine Learning Research, 8(Jun):1325–1368, 2007.
[27] Harold Hotelling. Analysis of a complex of statistical variables into
principal components. Journal of educational psychology, 24(6):417,
1933.
[28] Dmitry Kobak and Philipp Berens. /T_he art of using t-sne for single-cell
transcriptomics. Nature communications, 10(1):1–14, 2019.
[29] Dmitry Kobak and George C Linderman. Umap does not preserve
global structure any be/t_ter than t-sne when using the same initializa-
tion. bioRxiv, 2019.
[30] J. B. Kruskal. Multidimensional scaling by optimizing goodness of /f_it
to a nonmetric hypothesis. Psychometrika, 29(1):1–27, Mar 1964.
[31] Siu Kwan Lam, Antoine Pitrou, and Stanley Seibert. Numba: A llvm-
based python jit compiler. In Proceedings of the Second Workshop on
the LLVM Compiler Infrastructure in HPC, LLVM ’15, pages 7:1–7:6,
New York, NY, USA, 2015. ACM.
[32] Yann Lecun and Corinna Cortes. /T_he MNIST database of handwri/t_ten
digits.
[33] John A Lee and Michel Verleysen. Shi/f_t-invariant similarities circum-
vent distance concentration in stochastic neighbor embedding and
variants. Procedia Computer Science, 4:538–547, 2011.
60

[34] Xin Li, Ondrej E Dyck, Mark P Oxley, Andrew R Lupini, Leland
McInnes, John Healy, Stephen Jesse, and Sergei V Kalinin. Mani-
fold learning of four-dimensional scanning transmission electron mi-
croscopy. npj Computational Materials, 5(1):5, 2019.
[35] M. Lichman. UCI machine learning repository, 2013.
[36] George Linderman. Fit-sne. https://github.com/KlugerLab/
FIt-SNE, 2018.
[37] George C Linderman, Manas Rachh, Jeremy G Hoskins, Stefan
Steinerberger, and Yuval Kluger. Eﬃcient algorithms for t-distributed
stochastic neighborhood embedding. arXiv preprint arXiv:1712.09005,
2017.
[38] George C Linderman and Stefan Steinerberger. Clustering with t-sne,
provably. SIAM Journal on Mathematics of Data Science, 1(2):313–332,
2019.
[39] Saunders Mac Lane. Categories for the working mathematician, vol-
ume 5. Springer Science & Business Media, 2013.
[40] J Peter May. Simplicial objects in algebraic topology, volume 11. Uni-
versity of Chicago Press, 1992.
[41] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeﬀ
Dean. Distributed representations of words and phrases and their
compositionality. In Advances in neural information processing sys-
tems, pages 3111–3119, 2013.
[42] Kevin R Moon, David van Dijk, Zheng Wang, Sco/t_t Gigante, Daniel B
Burkhardt, William S Chen, Kristina Yim, Antonia van den Elzen,
Ma/t_thew J Hirn, Ronald R Coifman, et al. Visualizing structure and
transitions in high-dimensional biological data. Nature biotechnology,
37(12):1482–1492, 2019.
[43] Sameer A. Nene, Shree K. Nayar, and Hiroshi Murase. Columbia object
image library (coil-20. Technical report, 1996.
[44] Sameer A. Nene, Shree K. Nayar, and Hiroshi Murase. object image
library (coil-100. Technical report, 1996.
[45] Karolyn A Oetjen, Katherine E Lindblad, Meghali Goswami, Gege Gui,
Pradeep K Dagur, Catherine Lai, Laura W Dillon, J Philip McCoy, and
Christopher S Hourigan. Human bone marrow assessment by single
cell rna sequencing, mass cytometry and /f_low cytometry. bioRxiv,
2018.
61

[46] Jong-Eun Park, Krzysztof Polanski, Kerstin Meyer, and Sarah A Te-
ichmann. Fast batch alignment of single cell transcriptomes uni/f_ies
multiple mouse cell atlases into an integrated landscape.bioRxiv, page
397042, 2018.
[47] Jose Daniel Gallego Posada. Simplicial autoencoders. 2018.
[48] Emily Riehl. A leisurely introduction to simplicial sets. Unpublished
expository article available online at h/t_tp://www. math. harvard. edu/˜
eriehl, 2011.
[49] Emily Riehl. Category theory in context. Courier Dover Publications,
2017.
[50] John W Sammon. A nonlinear mapping for data structure analysis.
IEEE Transactions on computers, 100(5):401–409, 1969.
[51] Josef Spidlen, Karin Breuer, Chad Rosenberg, Nikesh Kotecha, and
Ryan R Brinkman. Flowrepository: A resource of annotated /f_low cy-
tometry datasets associated with peer-reviewed publications. Cytom-
etry Part A, 81(9):727–731, 2012.
[52] David I Spivak. Metric realization of fuzzy simplicial sets. Self pub-
lished notes, 2012.
[53] Jian Tang. Largevis. https://github.com/lferry007/LargeVis,
2016.
[54] Jian Tang, Jingzhou Liu, Ming Zhang, and Qiaozhu Mei. Visualizing
large-scale and high-dimensional data. InProceedings of the 25th Inter-
national Conference on World Wide Web, pages 287–297. International
World Wide Web Conferences Steering Commi/t_tee, 2016.
[55] Joshua B. Tenenbaum. Mapping a manifold of perceptual observa-
tions. In M. I. Jordan, M. J. Kearns, and S. A. Solla, editors,Advances in
Neural Information Processing Systems 10, pages 682–688. MIT Press,
1998.
[56] Joshua B Tenenbaum, Vin De Silva, and John C Langford. A global
geometric framework for nonlinear dimensionality reduction.science,
290(5500):2319–2323, 2000.
[57] Dmitry Ulyanov. Multicore-tsne. https://github.com/
DmitryUlyanov/Multicore-TSNE, 2016.
[58] Laurens van der Maaten. Accelerating t-sne using tree-based algo-
rithms. Journal of machine learning research, 15(1):3221–3245, 2014.
62

[59] Laurens van der Maaten and Geoﬀrey Hinton. Visualizing data using
t-sne. Journal of machine learning research, 9(Nov):2579–2605, 2008.
[60] Laurens van der Maaten and Geoﬀrey Hinton. Visualizing data using
t-SNE. Journal of Machine Learning Research, 9:2579–2605, 2008.
[61] John Williamson. What do numbers look like? https://johnhw.
github.io/umap_primes/index.md.html, 2018.
[62] Duoduo Wu, Joe Yeong, Grace Tan, Marion Chevrier, Josh Loh, Tony
Lim, and Jinmiao Chen. Comparison between umap and t-sne for
multiplex-immuno/f_luorescence derived single-cell data from tissue
sections. bioRxiv, page 549659, 2019.
[63] Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel
image dataset for benchmarking machine learning algorithms. CoRR,
abs/1708.07747, 2017.
[64] Liu Yang and Rong Jin. Distance metric learning: A comprehensive
survey. Michigan State Universiy, 2(2):4, 2006.
[65] Lo/f_ti A Zadeh. Information and control.Fuzzy sets, 8(3):338–353, 1965.
63

