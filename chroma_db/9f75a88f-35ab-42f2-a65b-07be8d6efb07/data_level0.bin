James Phoenix  
& Mike Taylor
Prompt 
Engineering for 
Generative AI
Future-Proof Inputs for Reliable AI Outputs
Free 
ChapterThis excerpt contains Chapter 1. The complete book is
available on the O’Reilly Online Learning Platform and
through other retailers.
James Phoenix and Mike Taylor
Prompt Engineering for
Generative AI
Future-Proof Inputs for Reliable
AI Outputs at Scale
Boston Farnham Sebastopol TokyoBeijing Boston Farnham Sebastopol TokyoBeijing978-1-098-15343-4
[LSI]
Prompt Engineering for Generative AI
by James Phoenix and Mike Taylor
Copyright © 2024 Saxifrage, LLC and Just Understanding Data LTD. All rights reserved.
Printed in the United States of America.
Published by O’Reilly Media, Inc., 1005 Gravenstein Highway North, Sebastopol, CA 95472.
O’Reilly books may be purchased for educational, business, or sales promotional use. Online editions are
also available for most titles (http://oreilly.com). For more information, contact our corporate/institutional
sales department: 800-998-9938 or corporate@oreilly.com.
Acquisitions Editor: Nicole Butterfield
Development Editor: Corbin Collins
Copyeditor: Piper Editorial Consulting, LLC
Proofreader: Kim Wimpsett
Indexer: nSight, Inc.
Interior Designer: David Futato
Cover Designer: Karen Montgomery
Illustrator: Kate Dullea
May 2024:  First Edition
Revision History for the First Edition
2024-05-15: First Release
See http://oreilly.com/catalog/errata.csp?isbn=9781098153434 for release details.
The O’Reilly logo is a registered trademark of O’Reilly Media, Inc. Prompt Engineering for Generative AI,
the cover image, and related trade dress are trademarks of O’Reilly Media, Inc.
The views expressed in this work are those of the authors and do not represent the publisher’s views.
While the publisher and the authors have used good faith efforts to ensure that the information and
instructions contained in this work are accurate, the publisher and the authors disclaim all responsibility
for errors or omissions, including without limitation responsibility for damages resulting from the use
of or reliance on this work. Use of the information and instructions contained in this work is at your
own risk. If any code samples or other technology this work contains or describes is subject to open
source licenses or the intellectual property rights of others, it is your responsibility to ensure that your use
thereof complies with such licenses and/or rights.Table of Contents
1. The Five Principles of Prompting. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  1
Overview of the Five Principles of Prompting                                                              4
1. Give Direction                                                                                                                8
2. Specify Format                                                                                                              14
3. Provide Examples                                                                                                        17
4. Evaluate Quality                                                                                                           20
5. Divide Labor                                                                                                                 32
Summary                                                                                                                           38
iiiCHAPTER 1
The Five Principles of Prompting
Prompt engineering is the process of discovering prompts that reliably yield useful or
desired results.
A prompt is the input you provide, typically text, when interfacing with an AI model
like ChatGPT or Midjourney. The prompt serves as a set of instructions the model
uses to predict the desired response: text from large language models  (LLMs) like
ChatGPT, or images from diffusion models like Midjourney.
Here is a simple example of a prompt input for a product name generator (inspired
by one of OpenAI’s examples), and the resulting output from ChatGPT.
Input:
Can I have a list of product names for a pair of shoes that
can fit any foot size?
Output:
Certainly! Here's a list of product names for a pair of shoes
that can fit any foot size:
UniFit SoleStrides
FlexiSize All-Fit Shoes
OmniFit FootFlex
...
AnyStep AdaptiWalkers
FlexiRange UniversalSteps
PerfectGlide MultiFit Shoes
1This is already a remarkable response for a naive prompt, which feels like magic
because we got here with very little effort. As the state-of-the-art models improve,
the likelihood you will get good enough  results on your first try goes up. For any
throwaway interactions with an AI, where you don’t plan to do the same task again,
the naive approach is all you need.
However, if you planned to put this prompt into production, you’ d benefit from
investing more work into getting it right. Mistakes cost you money in terms of the
fees OpenAI charges based on the length of the prompt and response, as well as
the time spent fixing mistakes. If you were building a product name generator with
thousands of users, there are some obvious issues you’ d want attempt to fix:
Vague direction
Y ou’re not briefing the AI on what style of name you want, or what attributes it
should have. Do you want a single word or a concatenation? Can the words be
made up, or is it important that they’re in real English? Do you want the AI to
emulate somebody you admire who is famous for great product names?
Unformatted output
Y ou’re getting back a list of separated names line by line, of unspecified length.
When you run this prompt multiple times, you’ll see sometimes it comes back
with a numbered list, and often it has text at the beginning, which makes it hard
to parse programmatically.
Missing examples
Y ou haven’t given the AI any examples of what good names look like. It’s auto‐
completing using an average of its training data, i.e., the entire internet (with all
its inherent bias), but is that what you want? Ideally you’ d feed it examples of
successful names, common names in an industry, or even just other names you
like.
Limited evaluation
Y ou have no consistent or scalable way to define which names are good or bad, so
you have to manually review each response. If you can institute a rating system
or other form of measurement, you can optimize the prompt to get better results
and identify how many times it fails.
No task division
Y ou’re asking a lot of a single prompt here: there are lots of factors that go into
product naming, and this important task is being naively outsourced to the AI all
in one go, with no task specialization or visibility into how it’s handling this task
for you.
2 | Chapter 1: The Five Principles of PromptingAddressing these problems is the basis for the core principles we use throughout
this book. There are many different ways to ask an AI model to do the same task,
and even slight changes can make a big difference. LLMs work by continuously
predicting the next token (approximately three-fourths of a word), starting from what
was in your prompt. Each new token is selected based on its probability of appearing
next, with an element of randomness (controlled by the temperature parameter). As
demonstrated in Figure 1-1, the word shoes had a lower probability of coming after
the start of the name AnyFit (0.88%), where a more predictable response would be
Athletic (72.35%).
Figure 1-1. How the response breaks down into tokens
LLMs are trained on essentially the entire text of the internet, and are then further
fine-tuned to give helpful responses. Average prompts will return average responses,
leading some to be underwhelmed when their results don’t live up to the hype.
What you put in your prompt changes the probability of every word generated, so
it matters a great deal to the results you’ll get. These models have seen the best and
worst of what humans have produced and are capable of emulating almost anything
if you know the right way to ask. OpenAI charges based on the number of tokens
The Five Principles of Prompting | 3used in the prompt and the response, so prompt engineers need to make these tokens
count by optimizing prompts for cost, quality, and reliability.
Here’s the same example with the application of several prompt engineering tech‐
niques. We ask for names in the style of Steve Jobs, state that we want a comma-
separated list, and supply examples of the task done well.
Input:
Brainstorm a list of product names for a shoe that fits any
foot size, in the style of Steve Jobs.
Return the results as a comma-separated list, in this format:
Product description: A shoe that fits any foot size
Product names: [list of 3 product names]
## Examples
Product description: A refrigerator that dispenses beer
Product names: iBarFridge, iFridgeBeer, iDrinkBeerFridge
Product description: A watch that can tell accurate time in
space
Product names: iNaut, iSpace, iTime
Product description: A home milkshake maker
Product names: iShake, iSmoothie, iShake Mini
Output:
Product description: A shoe that fits any foot size
Product names: iFitFoot, iPerfectFit, iShoeSize
While no prompt is ever perfect, this prompt is optimized to reliably deliver solid
product names in the right format. The user of your product name generator can
choose somebody other than Steve Jobs to get the types of names they like, they can
change the response format if needed, and the output of this prompt can become the
input of another. Finally, you could periodically update the examples you use in the
prompt based on user feedback, making your system smarter over time.
Overview of the Five Principles of Prompting
The process for optimizing this prompt follows the Five Principles of Prompting ,
which we will dissect using this example in the remainder of this chapter, and recall
throughout the book. They map exactly to the five issues we raised when discussing
the naive text prompt. Y ou’ll find references back to these principles throughout the
rest of the book to help you connect the dots to how they’re used in practice. The Five
Principles of Prompting are as follows:
4 | Chapter 1: The Five Principles of PromptingGive Direction
Describe the desired style in detail, or reference a relevant persona
Specify Format
Define what rules to follow, and the required structure of the response
Provide Examples
Insert a diverse set of test cases where the task was done correctly
Evaluate Quality
Identify errors and rate responses, testing what drives performance.
Divide Labor
Split tasks into multiple steps, chained together for complex goals
These principles are not short-lived tips or hacks but are generally accepted conven‐
tions that are useful for working with any level of intelligence, biological or artificial.
These principles are model-agnostic and should work to improve your prompt no
matter which generative text or image model you’re using. We first published these
principles in July 2022 in the blog post “Prompt Engineering: From Words to Art
and Copy” , and they have stood the test of time, including mapping quite closely
to OpenAI’s own Prompt Engineering Guide , which came a year later. Anyone who
works closely with generative AI models is likely to converge on a similar set of
strategies for solving common issues, and throughout this book you’ll see hundreds
of demonstrative examples of how they can be useful for improving your prompts.
We have provided downloadable one-pagers for text and image generation you can
use as a checklist when applying these principles. These were created for our popu‐
lar Udemy course The Complete Prompt Engineering for AI Bootcamp  (70,000+
students), which was based on the same principles but with different material to this
book.
• Text Generation One-Pager•
• Image Generation One-Pager•
To show these principles apply equally well to prompting image models, let’s use
the following example, and explain how to apply each of the Five Principles of
Prompting to this specific scenario. Copy and paste the entire input prompt into the
Midjourney Bot in Discord, including the link to the image at the beginning, after
typing /imagine to trigger the prompt box to appear (requires a free Discord account,
and a paid Midjourney account).
Overview of the Five Principles of Prompting | 5Input:
https://s.mj.run/TKAsyhNiKmc stock photo of business meeting
of 4 people watching on white MacBook on top of glass-top
table, Panasonic, DC-GH5
Figure 1-2 shows the output.
Figure 1-2. Stock photo of business meeting
This prompt takes advantage of Midjourney’s ability to take a base image as an
example by uploading the image to Discord and then copy and pasting the URL into
the prompt ( https://s.mj.run/TKAsyhNiKmc), for which the royalty-free image from
Unsplash is used (Figure 1-3). If you run into an error with the prompt, try uploading
the image yourself and reviewing Midjourney’s documentation for any formatting
changes.
6 | Chapter 1: The Five Principles of PromptingFigure 1-3. Photo by Mimi Thian on Unsplash
Let’s compare this well-engineered prompt to what you get back from Midjourney if
you naively ask for a stock photo in the simplest way possible. Figure 1-4 shows an
example of what you get without prompt engineering, an image with a darker, more
stylistic take on a stock photo than you’ d typically expect.
Input:
people in a business meeting
Figure 1-4 shows the output.
Although less prominent an issue in v5 of Midjourney onwards, community feedback
mechanisms (when users select an image to resize to a higher resolution, that choice
may be used to train the model) have reportedly biased the model toward a fantasy
aesthetic, which is less suitable for the stock photo use case. The early adopters of
Midjourney came from the digital art world and naturally gravitated toward fantasy
and sci-fi styles, which can be reflected in the results from the model even when this
aesthetic is not suitable.
Overview of the Five Principles of Prompting | 7Figure 1-4. People in a business meeting
Throughout this book the examples used will be compatiable with ChatGPT Plus
(GPT-4) as the text model and Midjourney v6 or Stable Diffusion XL as the image
model, though we will specify if it’s important. These foundational models are the
current state of the art and are good at a diverse range of tasks. The principles are
intended to be future-proof as much as is possible, so if you’re reading this book
when GPT-5, Midjourney v7, or Stable Diffusion XXL is out, or if you’re using
another vendor like Google, everything you learn here should still prove useful.
1. Give Direction
One of the issues with the naive text prompt discussed earlier was that it wasn’t
briefing the AI on what types of product names you wanted. To some extent, naming
a product is a subjective endeavor, and without giving the AI an idea of what names
you like, it has a low probability of guessing right.
8 | Chapter 1: The Five Principles of PromptingBy the way, a human would also struggle to complete this task without a good brief,
which is why creative and branding agencies require a detailed briefing on any task
from their clients.
Although it’s not a perfect mapping, it can be helpful to imagine
what context a human might need for this task and try including it
in the prompt.
In the example prompt we gave direction through the use of role-playing, in that case
emulating the style of Steve Jobs, who was famous for iconically naming products. If
you change this aspect of the prompt to someone else who is famous in the training
data (as well as matching the examples to the right style), you’ll get dramatically
different results.
Input:
Brainstorm a list of product names for a shoe that fits any
foot size, in the style of Elon Musk.
Return the results as a comma-separated list, in this format:
Product description: A shoe that fits any foot size
Product names: [list of 3 product names]
## Examples
Product description: A refrigerator that dispenses beer
Product names: Teslacool, FridgeX, Neuradrink
Product description: A watch that can tell accurate time in
space
Product names: WatchX, iSpace, The Watch Company
Product description: A home milkshake maker
Product names: ShakeX, The Milkshake Company, HyperShake
Output:
ShoeX, MarsFit, The Shoe Company
There are also some rules or best practices you would do well to follow, which could
be included in the prompt as context to guide the AI toward a name that works.
This technique is sometimes referred to as prewarming or internal retrieval, and it
is simple but effective ( Liu et al., 2021 ). Starting the conversation asking for best
practice advice, then asking it to follow its own advice, can help a lot. In effect, you
are using it to generate its own direction.
1. Give Direction | 9Input:
Please give me 5 tips for naming products based on expert
industry advice.
Output:
Certainly! Naming a product is crucial as it often gives the
first impression to potential customers. Here are five expert
tips to guide you in naming products:
1. **Simplicity & Memorability**: Opt for names that are
short, easy to spell, and memorable. Long, complicated names
can be difficult for customers to recall. Remember, the
easier the name is to remember, the more likely it is that
consumers will think of your product when making a purchase
decision.
...
5. **Positive Connotations**: Names that evoke positive
emotions or imagery can create a favorable perception of
your product. Avoid names that have negative or
controversial associations in any of the primary languages
or cultures where you intend to do business. It's often
worthwhile to conduct linguistic checks and market testing
to ensure there aren't unintended meanings.
Lastly, once you've shortlisted a few names, it's a good
idea to run them by diverse focus groups or conduct surveys
to gauge reactions. Also, ensure that the name you choose is
available for trademarking and doesn't infringe on existing
trademarks.
Then within the same chat window, where the model has the context of the past
advice it gave, you ask your initial prompt for the task you wanted to complete.
Input:
Using this advice, can I have a list of product names for a
pair of shoes that can fit any foot size?
Output:
Certainly! Based on the criteria given:
1. **Simplicity & Memorability**
2. **Relevance & Descriptiveness**
3. **Differentiation & Uniqueness**
4. **Adaptable & Scalable**
5. **Positive Connotations**
Here's a list of potential product names for shoes that can
fit any foot size:
10 | Chapter 1: The Five Principles of Prompting1. **FitAll Walkers**: A simple and memorable name that
instantly suggests a universal fit.
...
10. **TrueFit Universals**: Emphasizing genuine comfort and
fit for all.
Before finalizing any name, remember to check for trademarks
to ensure there's no conflict with existing products or
brands.
Another fruitful strategy in our experience is to take the best advice out there for the
task you want to accomplish and insert that context into the prompt. For example,
you could take Brandwatch’s 5 Golden Rules for naming a product or another trusted
external resource you find, and insert that as context into the prompt. This will
increase the length of the prompt significantly, which costs more money (when using
the API as a developer), but may be worth the trade-off if the quality of the response
improves.
Input:
Can I have a list of product names for a pair of shoes that
can fit any foot size? Please follow the five rules below:
1. It should be readable and writable.
If your product name is hard to pronounce, people won’t talk about it,
and if they can’t write it down (and spell it correctly!)
when they hear it, how do you expect them to Google it?
Keep it simple and don’t go with any wacky spellings just for the sake of it.
...
5. It should evoke an emotion, feeling, or idea.
Your product name should tie back into what your product is, what the
feeling you want people to have when experiencing your product is, and/or
what idea are you trying to get across.
It should be emotive and inspiring.
Output:
1. FlexFit Footwear
2. OmniSize Sneakers
3. AdaptStride Shoes
...
18. OmniComfort Kicks
19. FlexSize Footwear
20. Boundless Soles
1. Give Direction | 11There are other myriad ways of providing direction. In the image generation exam‐
ple, direction was given by specifying that the business meeting is taking place
around a glass-top table. If you change only that detail, you can get a completely
different image, as detailed in Figure 1-5.
Input:
https://s.mj.run/TKAsyhNiKmc stock photo of business meeting
of four people gathered around a campfire outdoors in the
woods, Panasonic, DC-GH5
Figure 1-5 shows the output.
Figure 1-5. Stock photo of business meeting in the woods
Role-playing is also important for image generation, and one of the quite powerful
ways you can give Midjourney direction is to supply the name of an artist or art style
to emulate. One artist that features heavily in the AI art world is Van Gogh, known
for his bold, dramatic brush strokes and vivid use of colors. Watch what happens
when you include his name in the prompt, as shown in Figure 1-6.
12 | Chapter 1: The Five Principles of PromptingInput:
people in a business meeting, by Van Gogh
Figure 1-6 shows the output.
Figure 1-6. People in a business meeting, by Van Gogh
To get that last prompt to work, you need to strip back a lot of the other direction.
For example, losing the base image and the words stock photo as well as the camera
Panasonic, DC-GH5 helps bring in Van Gogh’s style. The problem you may run into
is that often with too much direction, the model can quickly get to a conflicting
combination that it can’t resolve. If your prompt is overly specific, there might not be
enough samples in the training data to generate an image that’s consistent with all of
your criteria. In cases like these, you should choose which element is more important
(in this case, Van Gogh) and defer to that.
Direction is one of the most commonly used and broadest principles. It can take the
form of simply using the right descriptive words to clarify your intent, or channeling
the personas of relevant business celebrities. While too much direction can narrow
the creativity of the model, too little direction is the more common problem.
1. Give Direction | 132. Specify Format
AI models are universal translators. Not only does that mean translating from French
to English, or Urdu to Klingon, but also between data structures like JSON to YAML,
or natural language to Python code. These models are capable of returning a response
in almost any format, so an important part of prompt engineering is finding ways to
specify what format you want the response to be in.
Every now and again you’ll find that the same prompt will return a different format,
for example, a numbered list instead of comma separated. This isn’t a big deal most of
the time, because most prompts are one-offs and typed into ChatGPT or Midjourney.
However, when you’re incorporating AI tools into production software, occasional
flips in format can cause all kinds of errors.
Just like when working with a human, you can avoid wasted effort by specifying up
front the format you expect the response to be in. For text generation models, it can
often be helpful to output JSON instead of a simple ordered list because that’s the
universal format for API responses, which can make it simpler to parse and spot
errors, as well as to use to render the front-end HTML of an application. YAML is
also another popular choice because it enforces a parseable structure while still being
simple and human-readable.
In the original prompt you gave direction through both the examples provided, and
the colon at the end of the prompt indicated it should complete the list inline. To
swap the format to JSON, you need to update both and leave the JSON uncompleted,
so GPT-4 knows to complete it.
Input:
Return a comma-separated list of product names in JSON for
"A pair of shoes that can fit any foot size.".
Return only JSON.
Examples:
[{
"Product description": "A home milkshake maker.",
"Product names": ["HomeShaker", "Fit Shaker",
"QuickShake", "Shake Maker"]
},
{
"Product description": "A watch that can tell
accurate time in space.",
"Product names": ["AstroTime", "SpaceGuard",
"Orbit-Accurate", "EliptoTime"]}
]
14 | Chapter 1: The Five Principles of PromptingOutput:
[
{
"Product description": "A pair of shoes that can \
fit any foot size.",
"Product names": ["FlexFit Footwear", "OneSize Step",
"Adapt-a-Shoe", "Universal Walker"]
}
]
The output we get back is the completed JSON containing the product names. This
can then be parsed and used programmatically, in an application or local script.
It’s also easy from this point to check if there’s an error in the formatting using a
JSON parser like Python’s standard json library, because broken JSON will result in
a parsing error, which can act as a trigger to retry the prompt or investigate before
continuing. If you’re still not getting the right format back, it can help to specify at the
beginning or end of the prompt, or in the system message if using a chat model: You
are a helpful assistant that only responds in JSON , or specify JSON output
in the model parameters where available (this is called grammars with Llama models.
To get up to speed on JSON if you’re unfamiliar, W3Schools has a
good introduction.
For image generation models, format is very important, because the opportunities for
modifying an image are near endless. They range from obvious formats like stock
photo, illustration, and oil painting, to more unusual formats like dashcam
footage, ice sculpture, or in Minecraft (see Figure 1-7).
Input:
business meeting of four people watching on MacBook on top of
table, in Minecraft
Figure 1-7 shows the output.
2. Specify Format | 15Figure 1-7. Business meeting in Minecraft
When setting a format, it is often necessary to remove other aspects of the prompt
that might clash with the specified format. For example, if you supply a base image
of a stock photo, the result is some combination of stock photo and the format you
wanted. To some degree, image generation models can generalize to new scenarios
and combinations they haven’t seen before in their training set, but in our experience,
the more layers of unrelated elements, the more likely you are to get an unsuitable
image.
There is often some overlap between the first and second principles, Give Direction
and Specify Format. The latter is about defining what type of output you want, for
example JSON format, or the format of a stock photo. The former is about the style of
response you want, independent from the format, for example product names in the
style of Steve Jobs, or an image of a business meeting in the style of Van Gogh. When
there are clashes between style and format, it’s often best to resolve them by dropping
whichever element is less important to your final result.
16 | Chapter 1: The Five Principles of Prompting3. Provide Examples
The original prompt didn’t give the AI any examples of what you think good names
look like. Therefore, the response is approximate to an average of the internet, and
you can do better than that. Researchers would call a prompt with no examples
zero-shot, and it’s always a pleasant surprise when AI can even do a task zero shot:
it’s a sign of a powerful model. If you’re providing zero examples, you’re asking for
a lot without giving much in return. Even providing one example ( one-shot) helps
considerably, and it’s the norm among researchers to test how models perform with
multiple examples (few-shot). One such piece of research is the famous GPT-3 paper
“Language Models are Few-Shot Learners” , the results of which are illustrated in
Figure 1-8, showing adding one example along with a prompt can improve accuracy
in some tasks from 10% to near 50%!
Figure 1-8. Number of examples in context
When briefing a colleague or training a junior employee on a new task, it’s only
natural that you’ d include examples of times that task had previously been done well.
Working with AI is the same, and the strength of a prompt often comes down to the
examples used. Providing examples can sometimes be easier than trying to explain
exactly what it is about those examples you like, so this technique is most effective
when you are not a domain expert in the subject area of the task you are attempting
to complete. The amount of text you can fit in a prompt is limited (at the time of
writing around 6,000 characters on Midjourney and approximately 32,000 characters
for the free version of ChatGPT), so a lot of the work of prompt engineering involves
selecting and inserting diverse and instructive examples.
3. Provide Examples | 17There’s a trade-off between reliability and creativity: go past three to five examples
and your results will become more reliable, while sacrificing creativity. The more
examples you provide, and the lesser the diversity between them, the more con‐
strained the response will be to match your examples. If you change all of the
examples to animal names in the previous prompt, you’ll have a strong effect on the
response, which will reliably return only names including animals.
Input:
Brainstorm a list of product names for a shoe that fits any
foot size.
Return the results as a comma-separated list, in this format:
Product description: A shoe that fits any foot size
Product names: [list of 3 product names]
## Examples:
Product description: A home milkshake maker.
Product names: Fast Panda, Healthy Bear, Compact Koala
Product description: A watch that can tell accurate time in
space.
Product names: AstroLamb, Space Bear, Eagle Orbit
Product description: A refrigerator that dispenses beer
Product names: BearFridge, Cool Cat, PenguinBox
Output:
Product description: A shoe that fits any foot size
Product names: FlexiFox, ChameleonStep, PandaPaws
Of course this runs the risk of missing out on returning a much better name that
doesn’t fit the limited space left for the AI to play in. Lack of diversity and variation
in examples is also a problem in handling edge cases, or uncommon scenarios.
Including one to three examples is easy and almost always has a positive effect, but
above that number it becomes essential to experiment with the number of examples
you include, as well as the similarity between them. There is some evidence ( Hsieh
et al., 2023 ) that direction works better than providing examples, and it typically
isn’t straightforward to collect good examples, so it’s usually prudent to attempt the
principle of Give Direction first.
In the image generation space, providing examples usually comes in the form of
providing a base image in the prompt, called img2img in the open source Stable
Diffusion community. Depending on the image generation model being used, these
images can be used as a starting point for the model to generate from, which greatly
affects the results. Y ou can keep everything about the prompt the same but swap out
the provided base image for a radically different effect, as in Figure 1-9.
18 | Chapter 1: The Five Principles of PromptingInput:
stock photo of business meeting of 4 people watching on
white MacBook on top of glass-top table, Panasonic, DC-GH5
Figure 1-9 shows the output.
Figure 1-9. Stock photo of business meeting of four people
In this case, by substituting for the image shown in Figure 1-10, also from Unsplash,
you can see how the model was pulled in a different direction and incorporates
whiteboards and sticky notes now.
These examples demonstrate the capabilities of image generation
models, but we would exercise caution when uploading base
images for use in prompts. Check the licensing of the image you
plan to upload and use in your prompt as the base image, and
avoid using clearly copyrighted images. Doing so can land you in
legal trouble and is against the terms of service for all the major
image generation model providers.
3. Provide Examples | 19Figure 1-10. Photo by Jason Goodman on Unsplash
4. Evaluate Quality
As of yet, there has been no feedback loop to judge the quality of your responses,
other than the basic trial and error of running the prompt and seeing the results,
referred to as blind prompting. This is fine when your prompts are used temporarily
for a single task and rarely revisited. However, when you’re reusing the same prompt
multiple times or building a production application that relies on a prompt, you need
to be more rigorous with measuring results.
There are a number of ways performance can be evaluated, and it depends largely
on what tasks you’re hoping to accomplish. When a new AI model is released, the
focus tends to be on how well the model did on evals (evaluations), a standardized set
of questions with predefined answers or grading criteria that are used to test perfor‐
mance across models. Different models perform differently across different types of
tasks, and there is no guarantee a prompt that worked previously will translate well to
a new model. OpenAI has made its evals framework  for benchmarking performance
of LLMs open source and encourages others to contribute additional eval templates.
In addition to the standard academic evals, there are also more headline-worthy tests
like GPT-4 passing the bar exam. Evaluation is difficult for more subjective tasks, and
can be time-consuming or prohibitively costly for smaller teams. In some instances
researchers have turned to using more advanced models like GPT-4 to evaluate
responses from less sophisticated models, as was done with the release of Vicuna-13B,
a fine-tuned model based on Meta’s Llama open source model (see Figure 1-11).
20 | Chapter 1: The Five Principles of PromptingFigure 1-11. Vicuna GPT-4 Evals
More rigorous evaluation techniques are necessary when writing scientific papers or
grading a new foundation model release, but often you will only need to go just one
step above basic trial and error. Y ou may find that a simple thumbs-up/thumbs-down
rating system implemented in a Jupyter Notebook can be enough to add some rigor
to prompt optimization, without adding too much overhead. One common test is
to see whether providing examples is worth the additional cost in terms of prompt
length, or whether you can get away with providing no examples in the prompt. The
first step is getting responses for multiple runs of each prompt and storing them in a
spreadsheet, which we will do after setting up our environment.
Y ou can install the OpenAI Python package with pip install openai . If you’re
running into compatability issues with this package, create a virtual environment and
install our requirements.txt (instructions in the preface).
To utilize the API, you’ll need to create an OpenAI account  and then navigate here
for your API key.
Hardcoding API keys in scripts is not recommended due to secu‐
rity reasons. Instead, utilize environment variables or configuration
files to manage your keys.
Once you have an API key, it’s crucial to assign it as an environment variable by
executing the following command, replacing api_key with your actual API key value:
export OPENAI_API_KEY="api_key"
Or on Windows:
set OPENAI_API_KEY=api_key
4. Evaluate Quality | 21Alternatively, if you’ d prefer not to preset an API key, then you can manually set the
key while initializing the model, or load it from an .env file using python-dotenv. First,
install the library with pip install python-dotenv, and then load the environment
variables with the following code at the top of your script or notebook:
from dotenv import load_dotenv
load_dotenv()  # take environment variables from .env.
The first step is getting responses for multiple runs of each prompt and storing them
in a spreadsheet.
Input:
# Define two variants of the prompt to test zero-shot
# vs few-shot
prompt_A = """Product description: A pair of shoes that can
fit any foot size.
Seed words: adaptable, fit, omni-fit.
Product names:"""
prompt_B = """Product description: A home milkshake maker.
Seed words: fast, healthy, compact.
Product names: HomeShaker, Fit Shaker, QuickShake, Shake
Maker
Product description: A watch that can tell accurate time in
space.
Seed words: astronaut, space-hardened, eliptical orbit
Product names: AstroTime, SpaceGuard, Orbit-Accurate,
EliptoTime.
Product description: A pair of shoes that can fit any foot
size.
Seed words: adaptable, fit, omni-fit.
Product names:"""
test_prompts = [prompt_A, prompt_B]
import pandas as pd
from openai import OpenAI
import os
# Set your OpenAI key as an environment variable
# https://platform.openai.com/api-keys
client = OpenAI(
  api_key=os.environ['OPENAI_API_KEY'],  # Default
)
def get_response(prompt):
    response = client.chat.completions.create(
        model="gpt-3.5-turbo",
22 | Chapter 1: The Five Principles of Prompting        messages=[
            {
                "role": "system",
                "content": "You are a helpful assistant."
            },
            {
                "role": "user",
                "content": prompt
            }
        ]
    )
    return response.choices[0].message.content
# Iterate through the prompts and get responses
responses = []
num_tests = 5
for idx, prompt in enumerate(test_prompts):
    # prompt number as a letter
    var_name = chr(ord('A') + idx)
    for i in range(num_tests):
        # Get a response from the model
        response = get_response(prompt)
        data = {
            "variant": var_name,
            "prompt": prompt,
            "response": response
            }
        responses.append(data)
# Convert responses into a dataframe
df = pd.DataFrame(responses)
# Save the dataframe as a CSV file
df.to_csv("responses.csv", index=False)
print(df)
Output:
  variant                                             prompt
  \
0       A  Product description: A pair of shoes that can ...
1       A  Product description: A pair of shoes that can ...
2       A  Product description: A pair of shoes that can ...
3       A  Product description: A pair of shoes that can ...
4       A  Product description: A pair of shoes that can ...
5       B  Product description: A home milkshake maker.\n...
6       B  Product description: A home milkshake maker.\n...
7       B  Product description: A home milkshake maker.\n...
8       B  Product description: A home milkshake maker.\n...
4. Evaluate Quality | 239       B  Product description: A home milkshake maker.\n...
                                            response
0  1. Adapt-a-Fit Shoes \n2. Omni-Fit Footwear \n...
1  1. OmniFit Shoes\n2. Adapt-a-Sneaks \n3. OneFi...
2  1. Adapt-a-fit\n2. Flexi-fit shoes\n3. Omni-fe...
3  1. Adapt-A-Sole\n2. FitFlex\n3. Omni-FitX\n4. ...
4  1. Omni-Fit Shoes\n2. Adapt-a-Fit Shoes\n3. An...
5  Adapt-a-Fit, Perfect Fit Shoes, OmniShoe, OneS...
6       FitAll, OmniFit Shoes, SizeLess, AdaptaShoes
7       AdaptaFit, OmniShoe, PerfectFit, AllSizeFit.
8  FitMaster, AdaptoShoe, OmniFit, AnySize Footwe...
9        Adapt-a-Shoe, PerfectFit, OmniSize, FitForm
Here we’re using the OpenAI API to generate model responses to a set of prompts
and storing the results in a dataframe, which is saved to a CSV file. Here’s how it
works:
1. Two prompt variants are defined, and each variant consists of a product descrip‐1.
tion, seed words, and potential product names, but prompt_B provides two
examples.
2. Import statements are called for the Pandas library, OpenAI library, and os2.
library.
3. The get_response function takes a prompt as input and returns a response from3.
the gpt-3.5-turbo model. The prompt is passed as a user message to the model,
along with a system message to set the model’s behavior.
4. Two prompt variants are stored in the test_prompts list.4.
5. An empty list responses is created to store the generated responses, and the5.
variable num_tests is set to 5.
6. A nested loop is used to generate responses. The outer loop iterates over each6.
prompt, and the inner loop generates num_tests (five in this case) number of
responses per prompt.
a. The enumerate function is used to get the index and value of each prompta.
in test_prompts. This index is then converted to a corresponding uppercase
letter (e.g., 0 becomes A, 1 becomes B) to be used as a variant name.
b. For each iteration, the get_response function is called with the currentb.
prompt to generate a response from the model.
c. A dictionary is created with the variant name, the prompt, and the model’sc.
response, and this dictionary is appended to the responses list.
7. Once all responses have been generated, the responses list (which is now a list of7.
dictionaries) is converted into a Pandas DataFrame.
24 | Chapter 1: The Five Principles of Prompting8. This dataframe is then saved to a CSV file with the Pandas built-in to_csv8.
function, making the file responses.csv with index=False so as to not write row
indices.
9. Finally, the dataframe is printed to the console.9.
Having these responses in a spreadsheet is already useful, because you can see right
away even in the printed response that prompt_A (zero-shot) in the first five rows is
giving us a numbered list, whereas prompt_B (few-shot) in the last five rows tends to
output the desired format of a comma-separated inline list. The next step is to give
a rating on each of the responses, which is best done blind and randomized to avoid
favoring one prompt over another.
Input:
import ipywidgets as widgets
from IPython.display import display
import pandas as pd
# load the responses.csv file
df = pd.read_csv("responses.csv")
# Shuffle the dataframe
df = df.sample(frac=1).reset_index(drop=True)
# df is your dataframe and 'response' is the column with the
# text you want to test
response_index = 0
# add a new column to store feedback
df['feedback'] = pd.Series(dtype='str')
def on_button_clicked(b):
    global response_index
    #  convert thumbs up / down to 1 / 0
    user_feedback = 1 if b.description == "\U0001F44D" else 0
    # update the feedback column
    df.at[response_index, 'feedback'] = user_feedback
    response_index += 1
    if response_index < len(df):
        update_response()
    else:
        # save the feedback to a CSV file
        df.to_csv("results.csv", index=False)
        print("A/B testing completed. Here's the results:")
        # Calculate score and num rows for each variant
        summary_df = df.groupby('variant').agg(
            count=('feedback', 'count'),
            score=('feedback', 'mean')).reset_index()
4. Evaluate Quality | 25        print(summary_df)
def update_response():
    new_response = df.iloc[response_index]['response']
    if pd.notna(new_response):
        new_response = "<p>" + new_response + "</p>"
    else:
        new_response = "<p>No response</p>"
    response.value = new_response
    count_label.value = f"Response: {response_index + 1}"
    count_label.value += f"/{len(df)}"
response = widgets.HTML()
count_label = widgets.Label()
update_response()
thumbs_up_button = widgets.Button(description='\U0001F44D')
thumbs_up_button.on_click(on_button_clicked)
thumbs_down_button = widgets.Button(
    description='\U0001F44E')
thumbs_down_button.on_click(on_button_clicked)
button_box = widgets.HBox([thumbs_down_button,
thumbs_up_button])
display(response, button_box, count_label)
The output is shown in Figure 1-12:
Figure 1-12. Thumbs-up/thumbs-down rating system
If you run this in a Jupyter Notebook, a widget displays each AI response, with a
thumbs-up or thumbs-down button (see Figure 1-12) This provides a simple inter‐
face for quickly labeling responses, with minimal overhead. If you wish to do this
outside of a Jupyter Notebook, you could change the thumbs-up and thumbs-down
emojis for Y and N, and implement a loop using the built-in input() function, as a
text-only replacement for iPyWidgets.
Once you’ve finished labeling the responses, you get the output, which shows you
how each prompt performs.
26 | Chapter 1: The Five Principles of PromptingOutput:
A/B testing completed. Here's the results:
  variant  count  score
0       A      5    0.2
1       B      5    0.6
The dataframe was shuffled at random, and each response was labeled blind (without
seeing the prompt), so you get an accurate picture of how often each prompt per‐
formed. Here is the step-by-step explanation:
1. Three modules are imported: ipywidgets, IPython.display, and pandas. ipy1.
widgets contains interactive HTML widgets for Jupyter Notebooks and the
IPython kernel. IPython.display provides classes for displaying various types
of output like images, sound, displaying HTML, etc. Pandas is a powerful data
manipulation library.
2. The pandas library is used to read in the CSV file responses.csv, which contains2.
the responses you want to test. This creates a Pandas DataFrame called df.
3. df is shuffled using the sample() function with frac=1, which means it uses3.
all the rows. The reset_index(drop=True) is used to reset the indices to the
standard 0, 1, 2, …, n index.
4. The script defines response_index as 0. This is used to track which response4.
from the dataframe the user is currently viewing.
5. A new column feedback is added to the dataframe df with the data type as str5.
or string.
6. Next, the script defines a function on_button_clicked(b), which will execute6.
whenever one of the two buttons in the interface is clicked.
a. The function first checks the description of the button clicked was thea.
thumbs-up button (\U0001F44D; 
 ), and sets user_feedback as 1, or if it was
the thumbs-down button (\U0001F44E 
 ), it sets user_feedback as 0.
b. Then it updates the feedback column of the dataframe at the currentb.
response_index with user_feedback.
c. After that, it increments response_index to move to the next response.c.
d. If response_index is still less than the total number of responses (i.e., thed.
length of the dataframe), it calls the function update_response().
e. If there are no more responses, it saves the dataframe to a new CSV filee.
results.csv, then prints a message, and also prints a summary of the results by
variant, showing the count of feedback received and the average score (mean)
for each variant.
4. Evaluate Quality | 277. The function update_response() fetches the next response from the dataframe,7.
wraps it in paragraph HTML tags (if it’s not null), updates the response widget
to display the new response, and updates the count_label widget to reflect the
current response number and total number of responses.
8. Two widgets, response (an HTML widget) and count_label (a Label widget),8.
are instantiated. The update_response() function is then called to initialize
these widgets with the first response and the appropriate label.
9. Two more widgets, thumbs_up_button and thumbs_down_button (both Button9.
widgets), are created with thumbs-up and thumbs-down emoji as their descrip‐
tions, respectively. Both buttons are configured to call the on_button_clicked()
function when clicked.
10. The two buttons are grouped into a horizontal box ( button_box) using the HBox10.
function.
11. Finally, the response, button_box, and count_label widgets are displayed to the11.
user using the display() function from the IPython.display module.
A simple rating system such as this one can be useful in judging prompt quality
and encountering edge cases. Usually in less than 10 test runs of a prompt you
uncover a deviation, which you otherwise wouldn’t have caught until you started
using it in production. The downside is that it can get tedious rating lots of responses
manually, and your ratings might not represent the preferences of your intended
audience. However, even small numbers of tests can reveal large differences between
two prompting strategies and reveal nonobvious issues before reaching production.
Iterating on and testing prompts can lead to radical decreases in the length of
the prompt and therefore the cost and latency of your system. If you can find
another prompt that performs equally as well (or better) but uses a shorter prompt,
you can afford to scale up your operation considerably. Often you’ll find in this
process that many elements of a complex prompt are completely superfluous, or even
counterproductive.
The thumbs-up or other manually labeled indicators of quality don’t have to be the
only judging criteria. Human evaluation is generally considered to be the most accu‐
rate form of feedback. However, it can be tedious and costly to rate many samples
manually. In many cases, as in math or classification use cases, it may be possible to
establish ground truth (reference answers to test cases) to programmatically rate the
results, allowing you to scale up considerably your testing and monitoring efforts.
The following is not an exhaustive list because there are many motivations for
evaluating your prompt programmatically:
28 | Chapter 1: The Five Principles of PromptingCost
Prompts that use a lot of tokens, or work only with more expensive models,
might be impractical for production use.
Latency
Equally the more tokens there are, or the larger the model required, the longer it
takes to complete a task, which can harm user experience.
Calls
Many AI systems require multiple calls in a loop to complete a task, which can
seriously slow down the process.
Performance
Implement some form of external feedback system, for example a physics engine
or other model for predicting real-world results.
Classification
Determine how often a prompt correctly labels given text, using another AI
model or rules-based labeling.
Reasoning
Work out which instances the AI fails to apply logical reasoning or gets the math
wrong versus reference cases.
Hallucinations
See how frequently you encouner hallucinations, as measured by invention of
new terms not included in the prompt’s context.
Safety
Flag any scenarios where the system might return unsafe or undesirable results
using a safety filter or detection system.
Refusals
Find out how often the system incorrectly refuses to fulfill a reasonable user
request by flagging known refusal language.
Adversarial
Make the prompt robust against known prompt injection attacks that can get the
model to run undesirable prompts instead of what you programmed.
Similarity
Use shared words and phrases ( BLEU or ROGUE ) or vector distance (explained
in Chapter 5) to measure similarity between generated and reference text.
4. Evaluate Quality | 29Once you start rating which examples were good, you can more easily update the
examples used in your prompt as a way to continuously make your system smarter
over time. The data from this feedback can also feed into examples for fine-tuning,
which starts to beat prompt engineering once you can supply a few thousand exam‐
ples, as shown in Figure 1-13.
Figure 1-13. How many data points is a prompt worth?
Graduating from thumbs-up or thumbs-down, you can implement a 3-, 5-, or 10-
point rating system to get more fine-grained feedback on the quality of your prompts.
It’s also possible to determine aggregate relative performance through comparing
responses side by side, rather than looking at responses one at a time. From this you
can construct a fair across-model comparison using an Elo rating, as is popular in
chess and used in the Chatbot Arena by lmsys.org.
For image generation, evaluation usually takes the form of permutation prompting,
where you input multiple directions or formats and generate an image for each
30 | Chapter 1: The Five Principles of Promptingcombination. Images can than be scanned or later arranged in a grid to show the
effect that different elements of the prompt can have on the final image.
Input:
{stock photo, oil painting, illustration} of business
meeting of {four, eight} people watching on white MacBook on
top of glass-top table
In Midjourney this would be compiled into six different prompts, one for every
combination of the three formats (stock photo, oil painting, illustration) and two
numbers of people (four, eight).
Input:
1. stock photo of business meeting of four people watching
on white MacBook on top of glass-top table
2. stock photo of business meeting of eight people watching
on white MacBook on top of glass-top table
3. oil painting of business meeting of four people watching
on white MacBook on top of glass-top table
4. oil painting of business meeting of eight people watching
on white MacBook on top of glass-top table
5. illustration of business meeting of four people watching
on white MacBook on top of glass-top table
6. illustration of business meeting of eight people watching
on white MacBook on top of glass-top table
Each prompt generates its own four images as usual, which makes the output a little
harder to see. We have selected one from each prompt to upscale and then put them
together in a grid, shown as Figure 1-14. Y ou’ll notice that the model doesn’t always
get the correct number of people (generative AI models are surprisingly bad at math),
but it has correctly inferred the general intention by adding more people to the
photos on the right than the left.
Figure 1-14 shows the output.
4. Evaluate Quality | 31Figure 1-14. Prompt permutations grid
With models that have APIs like Stable Diffusion, you can more easily manipulate the
photos and display them in a grid format for easy scanning. Y ou can also manipulate
the random seed of the image to fix a style in place for maximum reproducibility.
With image classifiers it may also be possible to programmatically rate images based
on their safe content, or if they contain certain elements associated with success or
failure.
5. Divide Labor
As you build out your prompt, you start to get to the point where you’re asking a lot
in a single call to the AI. When prompts get longer and more convoluted, you may
find the responses get less deterministic, and hallucinations or anomalies increase.
Even if you manage to arrive at a reliable prompt for your task, that task is likely just
one of a number of interrelated tasks you need to do your job. It’s natural to start
exploring how many other of these tasks could be done by AI and how you might
string them together.
32 | Chapter 1: The Five Principles of PromptingOne of the core principles of engineering is to use task decomposition to break prob‐
lems down into their component parts, so you can more easily solve each individual
problem and then reaggregate the results. Breaking your AI work into multiple calls
that are chained together can help you accomplish more complex tasks, as well as
provide more visibility into what part of the chain is failing.
There are lots of factors that go into product naming, and an important task is naively
outsourced to the AI with no visibility into how it’s weighing the importance of these
factors (if at all). The way our current system works, we’re getting a list of names, but
all names are displayed with equal importance with no further context for helping us
decide. Fortunately AI tools are capable of self-evaluation; if we add a second step to
our task, we can automatically check for nondesirable outputs.
Input:
Rate this list of product names for a pair of shoes that can
fit any foot size. The rating should be out of 10, inline
next to the product name:
UniFit SoleStrides
FlexiSize All-Fit Shoes
OmniFit FootFlex
...
AnyStep AdaptiWalkers
FlexiRange UniversalSteps
PerfectGlide MultiFit Shoes
Output:
UniFit SoleStrides: 7/10
FlexiSize All-Fit Shoes: 8/10
OmniFit FootFlex: 7/10
...
AnyStep AdaptiWalkers: 7/10
FlexiRange UniversalSteps: 7.5/10
PerfectGlide MultiFit Shoes: 9/10
This ranking is based on clarity, memorability, and how well
the name communicates the unique selling point of the
product (that they can fit any foot size). Remember, these
ratings are subjective and the ultimate success of a product
name also depends on the target market, branding, and
marketing efforts.
In running this multiple times, it consistently rates the name “OneSize Glovewalkers”
as the worst, providing context (if you ask) that the concept might be confusing in
a shoe context. Y ou may be wondering why, if the model knows this is a bad name,
does it suggest it in the first place? LLMs work by predicting the next token in a
sequence and therefore struggle to know what the overall response will be when
5. Divide Labor | 33finished. However, when it has all the tokens from a previous response to review, it
can more easily predict whether this would be labeled as a good or bad response.
We can continue to chain multiple calls together to improve the results of our task.
For example, we could split this into three separate ratings: clarity, memorability, and
how well the name communicates the unique selling point of the product. These
ratings could then be given to a human as additional context on the final decision, or
even calculated together to select the final name programmatically.
The real unlock in learning to work professionally with AI versus just playing around
with prompting is realizing that every part of the system can be broken down into a
series of iterative steps. Even with a single prompt this principles applies, as simply
appending Let's think step by step to the prompt can lead to demonstrable gains
in reasoning and proficiency, as well as provide an audit trail for quality assurance
and debugging. When taking the time and tokens to reason, the ratings change and
are more consistent with the scoring criteria.
Input:
Let's think step by step. Rate this list of product names
for a pair of shoes that can fit any foot size. The rating
should be out of 10, inline next to the product name:
UniFit SoleStrides
FlexiSize All-Fit Shoes
OmniFit FootFlex
...
AnyStep AdaptiWalkers
FlexiRange UniversalSteps
PerfectGlide MultiFit Shoes
Output:
Here's a rating out of 10 for each product name, considering
factors like descriptiveness, catchiness, and overall
appeal:
1. **UniFit SoleStrides**: 7/10 - Sounds unifying but may
not immediately convey the "fit any size" idea.
2. **FlexiSize All-Fit Shoes**: 8.5/10 - Combines
flexibility and the notion that it fits all sizes.
3. **OmniFit FootFlex**: 8/10 - Omnipresence (fitting all)
and flexibility nicely conveyed.
...
18. **AnyStep AdaptiWalkers**: 7.5/10 - "AnyStep" suggests
versatility, but the overall name might be less direct.
19. **FlexiRange UniversalSteps**: 8/10 - Combines
flexibility and universal fit.
20. **PerfectGlide MultiFit Shoes**: 8.5/10 - "MultiFit" is
clear, and "PerfectGlide" suggests a comfortable experience.
34 | Chapter 1: The Five Principles of PromptingRemember, the best name can also be influenced by your
target audience, branding strategy, and the overall message
you want to convey.
OpenAI calls this  “giving the model time to think, ” and it is a key tenet of prompt
engineering. In effect, chain of thought  techniques like this, where the model is
encouraged to list out its steps, are like dividing a task within the same prompt. Once
we’ve automated product naming given a product idea, we can call ChatGPT again
to describe each product, which in turn can be fed into Midjourney to generate an
image of each product. Using an AI model to generate a prompt for an AI model
is meta prompting, and it works because LLMs are human-level prompt engineers
(Zhou, 2022).
Input:
Describe the new product "OneFit UltraStride Shoes," a shoe
that can fit any foot size, in great detail:
Output:
Introducing the revolutionary "OneFit UltraStride Shoes,"
the footwear innovation designed to fit any foot size with
unparalleled comfort and style. These shoes are a result of
cutting-edge technology and meticulous craftsmanship,
ensuring a truly personalized and enjoyable walking or
running experience for everyone.
Design and Materials:
The OneFit UltraStride Shoes boast an elegant and modern
design that appeals to both fashion-forward individuals and
sports enthusiasts. They feature a sleek and streamlined
silhouette, crafted using premium materials for durability
and aesthetic appeal. The shoe's upper is constructed from a
combination of breathable mesh and flexible synthetic
materials, allowing for excellent airflow and a lightweight
feel.
...
Color and Customization:
OneFit UltraStride Shoes are available in a variety of
vibrant colors and stylish patterns, catering to diverse
personal preferences. You can choose from classic
monochromatic options for a timeless look or opt for bold
and vibrant hues to make a fashion statement. The ability to
customize the shoe's appearance allows individuals to
express their unique style while enjoying the benefits of
the OneFit technology.
Conclusion:
The OneFit UltraStride Shoes represent a breakthrough in
footwear technology, eliminating the hassle of finding the
5. Divide Labor | 35perfect fit for your feet. With their adaptive design,
superior comfort, and stylish aesthetics, these shoes offer
a truly personalized experience for individuals of all foot
sizes. Stay comfortable, confident, and on-trend with the
OneFit UltraStride Shoes, the ultimate choice for those
seeking unmatched versatility and a perfect fit.
DALL-E is well-known by GPT-4, and therefore you can invoke its name within
ChatGPT and it does a reasonable job at crafting a prompt for an image generation
tool. If you were planning on using this prompt in production, you may consider
applying the prompting principle of providing examples, but it does a good enough
job for our purposes without examples.
Input:
Convert this description into a short prompt for an AI image
generation model such as DALL-E:
Output:
Create an eye-catching image that visually represents the
"OneFit UltraStride Shoes," a revolutionary footwear product
that adapts to any foot size. Showcase a sleek and modern
shoe design incorporating a stretchable upper, an adjustable
lacing system, and a cushioned insole for optimal comfort.
Emphasize the shoe's versatility by displaying it in action
on various surfaces, highlighting its excellent traction.
Incorporate vibrant colors and patterns to convey the
customizable nature of the product. The image should capture
the essence of innovation, style, and personalized fit that
the OneFit UltraStride Shoes offer.
The output of this prompt can now be plugged into image generation tools like
DALL-E or Midjourney as a prompt, which can give you a good starting point for
visualizing what the product might look like. Although this might not be the final
design you go with, seeing an image is more evocative and helps people form an
opinion faster. It’s easier cognitively to criticize or compliment an existing image than
it is to imagine a new image from a blank page or section of text.
Figure 1-15 shows the output.
36 | Chapter 1: The Five Principles of PromptingFigure 1-15. OneFit UltraStride shoes
It’s common practice when working with AI professionally to chain multiple calls
to AI together, and even multiple models, to accomplish more complex goals. Even
single-prompt applications are often built dynamically, based on outside context
queried from various databases or other calls to an AI model. The library LangChain
has developed tooling for chaining multiple prompt templates and queries together,
making this process more observable and well structured. A foundational example is
progressive summarization, where text that is too large to fit into a context window
can be split into multiple chunks of text, with each being summarized, before finally
summarizing the summaries. If you talk to builders of early AI products, you’ll find
they’re all under the hood chaining multiple prompts together, called AI chaining, to
accomplish better results in the final output.
The Reason and Act (ReAct)  framework was one of the first popular attempts at
AI agents, including the open source projects BabyAGI, AgentGPT and Microsoft
AutoGen. In effect, these agents are the result of chaining multiple AI calls together
in order to plan, observe, act, and then evaluate the results of the action. Autonomous
agents will be covered in Chapter 6 but are still not widely used in production at the
5. Divide Labor | 37time of writing. This practice of self-reasoning agents is still early and prone to errors,
but there are promising signs this approach can be useful in achieving complex tasks,
and is likely to be part of the next stage in evolution for AI systems.
There is an AI battle occurring between large tech firms like Microsoft and Google,
as well as a wide array of open source projects on Hugging Face, and venture-funded
start-ups like OpenAI and Anthropic. As new models continue to proliferate, they’re
diversifying in order to compete for different segments of the growing market. For
example, Anthropic’s Claude 2 had an 100,000-token context window , compared
to GPT-4’s standard 8,192 tokens . OpenAI soon responded with a 128,000-token
window version of GPT-4 , and Google touts a 1 million token context length with
Gemini 1.5. For comparison, one of the Harry Potter books would be around 185,000
tokens, so it may become common for an entire book to fit inside a single prompt,
though processing millions of tokens with each API call may be cost prohibitive for
most use cases.
This book focuses on GPT-4 for text generation techniques, as well as Midjourney v6
and Stable Diffusion XL for image generation techniques, but within months these
models may no longer be state of the art. This means it will become increasingly
important to be able to select the right model for the job and chain multiple AI
systems together. Prompt templates are rarely comparable when transferring to a new
model, but the effect of the Five Prompting Principles will consistently improve any
prompt you use, for any model, getting you more reliable results.
Summary
In this chapter, you learned about the importance of prompt engineering in the
context of generative AI. We defined prompt engineering as the process of develop‐
ing effective prompts that yield desired results when interacting with AI models.
Y ou discovered that providing clear direction, formatting the output, incorporating
examples, establishing an evaluation system, and dividing complex tasks into smaller
prompts are key principles of prompt engineering. By applying these principles and
using common prompting techniques, you can improve the quality and reliability of
AI-generated outputs.
Y ou also explored the role of prompt engineering in generating product names and
images. Y ou saw how specifying the desired format and providing instructive exam‐
ples can greatly influence the AI’s output. Additionally, you learned about the concept
of role-playing, where you can ask the AI to generate outputs as if it were a famous
person like Steve Jobs. The chapter emphasized the need for clear direction and
context to achieve desired outcomes when using generative AI models. Furthermore,
you discovered the importance of evaluating the performance of AI models and the
various methods used for measuring results, as well as the trade-offs between quality
and token usage, cost, and latency.
38 | Chapter 1: The Five Principles of PromptingIn the next chapter, you will be introduced to text generation models. Y ou will learn
about the different types of foundation models and their capabilities, as well as their
limitations. The chapter will also review the standard OpenAI offerings, as well as
competitors and open source alternatives. By the end of the chapter, you will have
a solid understanding of the history of text generation models and their relative
strengths and weaknesses. This book will return to image generation prompting in
Chapters 7, 8, and 9, so you should feel free to skip ahead if that is your immediate
need. Get ready to dive deeper into the discipline of prompt engineering and expand
your comfort working with AI.
Summary | 39About the Authors
Mike Taylor cofounded a 50-person growth marketing agency called Ladder with
offices in the USA, UK, and EU. More than 400,000 people have taken his marketing
and AI courses on LinkedIn Learning, Udemy, and Vexpower.
James Phoenix builds reliable data pipelines for marketing teams, automating thou‐
sands of recurring tasks. He has taught 60+ Data Science bootcamps for General
Assembly and partnered with Mike on the Udemy course, and Vexpower.
Both authors been experimenting with prompt engineering since the GPT-3 beta
in 2020. They slowly automated every part of their jobs with AI and now work as
prompt engineers on various projects.
Colophon
The animal on the cover of Prompt Engineering for Generative AI is a screaming hairy
armadillo (Chaetophractus vellerosus). This species of armadillo gets it name due to its
habit of squealing, or screaming, when it is handled or threatened.
The screaming hairy armadillo resides in arid areas, specifically in regions in Argen‐
tina, Bolivia, and Paraguay. This animal prefers subtropical or tropical regions such
as dry forests, scrubland, grassland, and deserts. White and light brown hair cover
the animal’s limbs and belly. A caparace, a thick armor made of keratin, covers the
animal’s body, a shield covers its head, and a small band exists between its ears. The
animal typically reaches 12 to 22 inches in length, including its tail, and weighs less
than 2 pounds, with male armadillos generally being larger than females.
The screaming hairy armadillo is an omnivore, eating small vertebrates such as frogs,
toads, lizards, birds, and rodents, as well as fruits and vegetation. It can go long
periods of time without drinking water.
Although the IUCN Red List designates the screaming hairy armadillo as Least
Concern, it is heavily hunted in parts of Bolivia for its meat and carapace. Many
of the animals on O’Reilly covers are endangered; all of them are important to the
world.
The cover illustration is by Karen Montgomery, based on an antique line engraving
from Beeton’s Dictionary. The series design is by Edie Freedman, Ellie Volckhausen,
and Karen Montgomery. The cover fonts are Gilroy Semibold and Guardian Sans.
The text font is Adobe Minion Pro; the heading font is Adobe Myriad Condensed;
and the code font is Dalton Maag’s Ubuntu Mono.d the need for tests of long-context capability which
go beyond retrieval (Goldman et al., 2024; Karpinska et al., 2024; Levy et al., 2024). These results
point out that the ability of a model to retrieve one or more facts does not necessarily imply that a
model is able to synthesize information from the full context. Existing benchmarks which attempt to
measure a model’s capacity for reasoning over a long context (Arora et al., 2023; Li et al., 2024) may
have a few suboptimal properties. Some examples include:
• relatively small context lengths;
• high artificiality with no natural language or code setting;
• the requirement of significant amounts of human labor to extend to longer context lengths;
• a construction that clearly avoids scenarios where the tasks require the usage of the full context,
either because the information required to answer questions may be present in pretraining data
or because it is possible to “short-circuit” the context length and answer the question with more
local information.
Other approaches have focused on measuring a model’s capability for performing many-shot learning
2Michelangelo: Long Context Evaluations Beyond Haystacks via Latent Structure Queries
(Agarwal et al., 2024; Bohnet et al., 2024a) and summarization (Chang et al., 2024; Kim et al.,
2024), neither of which necessarily require or measure a model’s capacity to reason over the context.
Additionally, many existing long-context evaluation benchmarks construct their evaluation by utilizing
pre-existing (and likely leaked-in-training-data) evaluations, making it difficult to ascertain the extent
to which performance on these benchmarks may be due to leaked information (Agarwal et al., 2024;
Bai et al., 2023; Bohnet et al., 2024a; Hsieh et al., 2024; Lee et al., 2024; Li et al., 2024; Zhang
et al., 2024). In this work, we focus on developing Michelangelo, a minimal, synthetic, and un-leaked
long-context reasoning evaluation which is also extremely easy to automatically score.
Minimality: Michelangelo constitutes a minimal set of the simplest canonical tasks which require
understanding of the context beyond retrieval, and yet are still challenging for present-day models to
solve. Minimal benchmarks, in both size and complexity, which capture a wide range of behavior are
desirable for many reasons: they are easy to interpret, easy to use for tracking progress, and inform
the simplest cases where a model is likely to fail.
Canonical Primitives:Michelangelo constitutes three intuitive and simple long-context synthesis
taskprimitiveswhichrequirethemodeltosynthesizemultiplepiecesofinformationstrewnthroughout
the context in order to produce an answer, and which measure different aspects of a model’s synthesis
capabilities to provide a more holistic understanding of long-context model behavior. Each of the
evaluations we present is positioned in a natural language or code-based setting, and is less synthetic
compared to existing benchmarks. We observe that it is already challenging to synthesize two pieces
of information across a context. In particular, Michelangelo measures the ability of a model to
• reproduce a given unique portion of the context while understanding ordering;
• understand a sequence of modifications to a list (which captures a very wide range of preexisting
tasks);
• determine whether the answer to a query is contained within the context.
Arbitrary Context Length:Michelangelo tasks are arbitrarily extendable in context length, while
maintaining a fixed complexity (as measured by the amount of relevant information in the context
that must be synthesized). This desiderata is fulfilled by the synthetic nature of the evaluation, and
by the approach we take to ensure that both the relevant information and the irrelevant information
are synthetically generated, (generally) share similar distributions, and do not result in logical
contradictions or short-circuits.
Leakage Prevention:Since it is possible to automatically re-generate Michelangelo task instances,
it is easy to avoid future leakage issues. The methodology we use to ensure the evaluations are both
automatically extendable to arbitrary context lengths and based on natural language does not rely
on utilizing existing evaluation sets or internet data (which are most likely leaked in training data),
and thus constitutes an un-leaked automatic, synthetic, and somewhat more realistic long-context
reasoning evaluation.
For an example of frontier model performance on one of the Michelangelo tasks, see Figure 1.
We demonstrate that this MRCR task may reveal interesting connections between existing models
in Figure 2. Notably, MRCR is evaluated with a simple metric and uses a fixed prompt across many
model families with no degradation in signal.
Our contributions are as follows:
• We present Michelangelo, a minimal benchmark for long-context reasoning and synthesis
primitives that measures foundation model performance across arbitrarily large context lengths.
3Michelangelo: Long Context Evaluations Beyond Haystacks via Latent Structure Queries
2K8K 32K 128K
Number of tokens in context
0.7
0.8
0.9
1.0Cumulative Average Score
Multi-Round Coreference Resolution (MRCR) 
GPT 4o
GPT 4T-0409
2K8K 32K 128K
Number of tokens in context
0.6
0.8
1.0Cumulative Average Score
Multi-Round Coreference Resolution (MRCR) 
Claude 3 Haiku
Claude 3 Sonnet
Claude 3 Opus
Claude 3.5 Sonnet
2K8K 32K 128K
Number of tokens in context
0.7
0.8
0.9Cumulative Average Score
Multi-Round Coreference Resolution (MRCR) 
Gemini 1.5 Flash (08/27)
Gemini 1.5 Pro (08/27)
Figure 2|Models from the same families (Gemini, GPT, Claude) tend to have parallel MRCR curves.
Claude-3.5 Sonnet and Claude-3 Opus in particular have strikingly parallel MRCR curves.
• We design the Latent Structure Queries framework for long-context reasoning evaluations,
which encompasses and generalizes existing work on long-context evaluation. Michelangelo
consists of three simple instances of Latent Structure Queries which are measurably diverse
across the capability being measured and the data distribution it is instantiated upon.
• We analyze evaluations up to 1M context on several leading models with significantly higher
resolution over the context length, and analyze model performance. We demonstrate that GPT
and Claude models have non-trivial performance up to 128K context, while Gemini models
have non-trivial generalization capabilities all the way to 1M context. However, the difficulty of
the reasoning tasks induces an initial sharp drop off in performance for all frontier models we
evaluated, indicating that while significant progress has been made on generalizing reasoning
behavior up to 1M context, multiple simple long context reasoning primitives remain unsolved.
2. Michelangelo Evaluation Tasks
In this section, we present descriptions of the specific evaluations comprising Michelangelo and defer
discussion of the overarching framework used to produce these evaluations to Section 3.
2.1. Latent List
We consider a short Python list, and present a sequence of Python operations which modify that list
(append, insert, pop, remove, sort, reverse). Given this sequence of operations, the model is
required to output a view of the resulting latent list. This view can be toprint a full slice of the
list, asum, min, ormax of a slice of the list, or the length of the list. It is important to note that
the size of the resulting list is not dependent on the total context length of the instance – instead,
it is dependent on the number of relevant operations, the size of which indexes the task instance’s
complexity. To fill the context, we uniformly adopt three strategies which do not affect the list’s latent
state: 1) we insertprint(“Do nothing.”) statements, 2) we insert even numbers ofreverse
operations, 3) we insert blocks of operations which all cancel themselves out, locally. We consider
three complexity levels:1 relevant operation,5 relevant operations, and20 relevant operations,
also represented uniformly in the task set (note we do not count the first operation of defining the
4Michelangelo: Long Context Evaluations Beyond Haystacks via Latent Structure Queries
initial list in this metric). The operations use numbers uniformly drawn between[−4000,4000]. The
relevant operations are spread uniformly throughout the input context.
We score model performance with an approximate accuracy metric, which requires exact string
match onprintview operations, and which reports an approximate error normalized to be within
[0,1]for numerical outputs (the rest of the view operations).
We use the following approximate metric to score the Latent List task. We use an approximate
metric to make the task easier for existing models, and we observe a wider dynamic range of signal
when using the approximate version of the task. The following code describes the exact method for
computing this score:
1 def latent_list_metric(
2 model_answer: str,
3 true_target: str,
4 view_op: str,
5 ) -> float:
6 """Computes approximate match for Latent List ."""
7 exact_score = float(int(str(model_answer) == true_target))
8
9 if view_op != 'print':
10 # Since the metric is fuzzy, model_answer is a number.
11 # Compute a normalized absolute error.
12 if not isinstance(model_answer, int):
13 err = 1.0
14 else:
15 try:
16 # Bounded in [0, 1]. Large incorrect answer should have high err.
17 norm = np.abs(float(true_target))
18 err = min(
19 1.0,
20 np.abs(float(true_target) - model_answer) / (1e-10 + norm),
21 )
22 except TypeError:
23 err = 1.0
24 return 1.0 - err
25
26 return exact_score
Since we have a very natural stratification of task instances into a notion of complexity (the
number of list operations that actually impact the answer to the view operation query), we can further
analyze performance on this evaluation as sliced by task complexity.
5Michelangelo: Long Context Evaluations Beyond Haystacks via Latent Structure Queries
Latent List Task Example: Complexity 2
Task Instructions
You will be presented with a sequence of list operations which
modify a Python list...
Relevant Operation
» a = [1, 2, 3, 4, 5, 6]
» a.remove(3)
Irrelevant Operations
» print(“Do nothing.”)
...
» print(“Do nothing.”)
Relevant Operation
» a.insert(2, 325)
Irrelevant Operations
» print(“Do nothing.”)
...
» print(“Do nothing.”)
View Operation
» min(a[2:4])
2.2. Multi-Round Co-reference Resolution (MRCR)
In the Multi-Round Co-reference Resolution (MRCR) task (previously introduced and described in
Google et al. (2024)), the model sees a long conversation between a user and a model, in which
the user requests writing (e.g. poems, riddles, essays) on different topics proceeded by the model
responses. To produce these model responses, we prompted the PaLM 2 model (Anil et al., 2023)
for many outputs corresponding to each request and topic. In each conversation, user requests
containing topics and writing formats distinct from the rest of the conversation are randomly placed
in the context. Given the conversation as context, the model must reproduce the output from the
conversation (the needle) resulting from one of the requests (the key). Either the formats, the
topics, or both, overlap in order to create a key that is adversarially similar to the query key. For
instance, the request “Reproduce the poem about penguins.” requires the model to distinguish
the poem about penguins from the poem about flamingos, and “Reproduce thefirst poem about
penguins.” requires the model to reason about ordering. We score MRCR via a string-similarity
measure between the model output and the correct response. Given a model output, we post-process
the output by checking whether the special output string has been produced. If it has been produced,
we take the text after the special output string and compute a string-similarity measure between the
6Michelangelo: Long Context Evaluations Beyond Haystacks via Latent Structure Queries
model output and the correct response – specifically, the SequenceMatcher ratio as implemented in
https://docs.python.org/3/library/difflib.html. This value is between0 and 1.
MRCR Task Example: Complexity 2
Task Instructions
Here are some examples of conversations...
Relevant Operation
User: Write a poem about penguins.
Model: Ode to Penguins...
Irrelevant Operations
User: Write an essay about complexity theory.
Model: Complexity theory is a challenging subject...
...
User: Write an email about basketball.
Model: Hello! Hope this email finds you well. Basketball is...
Relevant Operation
User: Write a poem about penguins.
Model: O Fowl of the Frozen Seas! Warm and cuddly...
Irrelevant Operations
User: Write a riddle about ducks.
Model: What waddles on two feet and quacks?...
...
User: Write a play about Star Wars.
Model: [Enter stage right.] Narrator: Long, long ago...
View Operation
User: Add the string “AKJSs89sal” to the 2𝑛𝑑 poem about penguins.
2.3. IDK
In the IDK task, a model is presented with a large amount of text and asked a question which does not
have an objective answer given the large pretraining corpus. For example, there may be an invented
story about a woman and her dog, where the dog’s name and age are detailed, but not its color.
Then the view operation would be to simply ask: “What is the color of the woman’s dog?”. For each
instance of this task, four multiple choice answers are presented, one of which is always “(D) I don’t
know”. Each of the other answer choices presented is a relatively plausible response. We score based
on whether or not the model output has the correct answer. If the model does not output any of the
multiple choice options, but instead indicates via text that the model cannot answer the question
because the information required is not present in the context, we also mark that as correct if the
7Michelangelo: Long Context Evaluations Beyond Haystacks via Latent Structure Queries
correct answer is “I don’t know.” Since this metric is just accuracy, it is already between0 and 1. We
set 70% of the task instances in the evaluation to correspond to the true answer being “I don’t know”,
and 30% of the instances to correspond to the case where the answer is findable in the context and
constitutes a simple retrieval task.
For this evaluation, the irrelevant filler text corresponds to a random string of letters from the
English alphabet. Since for cases where the answer is IDK, we say there is “no relevant information”
and the task complexity is0. For the instances which correspond to retrieval tasks, the task complexity
is 1.
IDK Task Example: Complexity 0
Irrelevant Operations
Today, John took his bulldog for a walk at the park where the most
remarkable dog that caught his eye was barking at a tree. The park
was filled with many remarkable breeds, including Dalmations and
Corgies.
Irrelevant Operations
W F D N C T L N I A M P Z N I ...
View Operation
Question: What type of dog breed did John find most remarkable at
the park?
Choices:
(A) Bulldog
(B) Dalmatian
(C) Siberian Husky
(D) I don’t know
3. Latent Structure Queries : A Novel Long-Context Evaluation Framework
In this section, we present the simple Latent Structure Queries (LSQ) framework for creating the
synthetic long-context evaluations we defined in Section 2. Long-context evaluations should adhere
to the following principles:
• generically extendable to arbitrary context lengths;
• complexity indexed by the number of relevant pieces of information;
• context length difficulty should be decoupled from the complexity corresponding to the variant
of the task with no irrelevant information;
• coverage across natural language text and code (two basic domains);
• un-leaked contexts in the task instances;
• test understanding of implicit information conveyed in the context;
• the number of evaluations should be minimal and test orthogonal dimensions of long-context
synthesis capabilities.
The LSQ framework ensures all of these properties hold. We view the context presented to the
model as a stream of information which constitutes updates to a latent structure. One can imagine that
8Michelangelo: Long Context Evaluations Beyond Haystacks via Latent Structure Queries
the full context length is like a block of marble with many irrelevant pieces of information. If a sculptor
chisels away the irrelevant pieces of information, they may reveal a statue inside – corresponding
to the latent structure revealed within the large context. Ultimately, we wish to query this latent
structure – the fact that the context has a complex relationship with this latent structure ensures that
the task associated with the data structure is “beyond retrieval”, with standard single-needle and
multiple-needle retrieval tasks like needle-in-a-haystack (Kamradt, 2023) corresponding to a data
structure with either one key and value or multiple independent keys and values, where one wishes
to query that key or set of keys. So, contextual understanding beyond retrieval can be measured by
understanding the extent to which the model has internalized the final version of this latent structure.
As a simple running example, one can imagine reading a book about a family which goes through
life - parents might divorce, children grow up and marry, elders pass away. Over the course of the
book, the latent structure corresponding to the family tree changes and is updated (and much of
the information in the book may not affect the family tree at all). Thus, the context of the full book
describes the latent structure of the family tree, and one simple approach to checking the model’s
understanding of the latent structure would be to query the model for a description of the full data
structure. However, this task may be a) too difficult and b) relatively intractable in natural language
settings where the structure itself is quite complex. Thus, we instead adopt an approach by which we
query slices of the structure. With many such queries of different slices (analogous to the parable of
the Elephant in the Dark (Wikipedia, 2024)), the model can demonstrate partial understanding of
the whole.
A significant advantage of this framework is the fact that we can control the complexity of the
task by predetermining the number of relevant updates to the latent structure. A relevant update
constitutes an update to the structure that changes the final output of a given query. By keeping the
number of relevant updates fixed, we can decompose the difficulty of the task into two orthogonal
components – the inherent complexity of the task without long distracting information, and the
difficulty of the task due to the context length. This decomposition is useful so that we can attempt
to isolate the effect of the only the context length on model performance (see the ablations which
decouple complexity and context length for Latent List in Section 5.5).
The number of irrelevant updates, on the other hand, controls the context length. As long as a task
has a method for generating irrelevant updates to the latent data structure, the task can be extended
to arbitrary context lengths. In particular, we can make use of large language models to generate
natural text which is completely irrelevant to the final task, while maintaining a closer resemblance to
the existing distribution of natural text, making a task more realistic while still retaining the desirable
synthetic properties.
This scheme of determining relevant updates and irrelevant fillera prioriand using generated
filler that is guaranteed to not impact the final answer for a given task instance allows us to ensure
that a) there is no leakage from pretraining data into the task (in contrast, the approach of simply
hot-swapping entity names in Zhang et al. (2024) of a novel to mitigate leakage may not sufficiently
control for this behavior, since it is still possible to use abundant circumstantial knowledge of the
novel in order to determine relevant sections of the text), and b) ensuring that in the creation of
our task, there are no short cuts the model can take without utilizing the full context – avoiding the
“short-circuit” problem from both ends.
Using this framework, we are able to develop relatively orthogonal tasks that measure different
components of long-context understanding beyond retrieval by varying both the implicit latent
structure corresponding to the task, as well as the task complexity as parameterized by the number of
relevant updates. Notably, we present one code-focused task and two natural-language-focused tasks
– our latent queries framework is broadly applicable regardless of the domain. We will proceed by
9Michelangelo: Long Context Evaluations Beyond Haystacks via Latent Structure Queries
presenting three tasks utilizing this framework. Notably, each of these tasks is not intended to be
used for training prior to evaluation, for the reasons mentioned in Section 6. The code-related task is
not intended be run on a model which has access to a code editor - the goal is to test the implicit
reasoning behavior within the model circuits, as a proxy for even harder reasoning tasks which may
not be so easy to write code to solve.
3.1. Contextualizing Evaluations in the Latent Structure Queries Framework
3.1.1. Latent List
Latent List is a very direct implementation of the LSQ framework: the Python list is the latent
object, we query different components of the latent list, and it is updated with both operations that
meaningfully modify the list and operations which do not.
3.1.2. MRCR
The LSQ framework trivially encompasses the existing retrieval evaluations like Needle-in-a-Haystack
(Kamradt, 2023; Li et al., 2024) - they correspond exactly to retrieving a value given a key in a
dictionary. However, one major deficiency of many existing retrieval evaluations is how distinct the
needle is from the rest of the context. This property makes it significantly easier for large language
models to extract the relevant piece of information.
MRCR can be viewed as an extension of the Needle-in-a-Haystack task to the beyond-retrieval
setting which requires the model to use information about the ordering of multiple needles placed in
the haystack to answer the query. This setup has the advantage of creating highly similar needles to
be retrieved, and which requires the model to use information in two places in the context in order to
determine the correct answer.
This evaluation fits directly into the LSQ framework, where the latent data structure is a nested
dictionary indexed by topic and writing format. For each pair of topic and writing format, the latent
data structure stores a list of model outputs, in the order that they were presented in the context.
Then, the view operations are merely to output a model output value given two keys from the nested
dictionary, and in the case where the keys correspond to a list of model outputs of size greater than
one, the entry with the desired index. We also note that where in our presented version of the task
there are only two confounding outputs, it is easy to extend this evaluation to the setting where there
are multiple confounding outputs (and we may ask for the5𝑡ℎ poem about penguins, for instance,
rather than only the1𝑠𝑡 or 2𝑛𝑑). It is also easy to extend this task to the setting where we allow for
more deeply nested dictionaries, beyond depth2, by introducing more constraints on the output style.
For example, we could consider adding keys like emotion (“happy” or “sad”), style (“in the style of an
Italian mob boss”), and so on. We restrict our considerations to the minimal case where we observe
significant degradation as a function of context length, where we have a dictionary of depth2 and
where there is at most one additional doubly confounding (same topic and same format) instance in
the context.
3.1.3. IDK
IDK is another extremely simple and natural instance of the LSQ framework. In particular, it is the case
where the latent data structure does not contain an answer given the view operation’s parameters. A
simple realization of this case is a query to a dictionary with a key that does not have a corresponding
value – the correct answer is that the key is not present in the dictionary. Notably, to determine this
fact, the model must be able to rule out that any information present in the entire context contains
10Michelangelo: Long Context Evaluations Beyond Haystacks via Latent Structure Queries
the answer to the query.
3.2. Context Stratification
When developing long-context models, it often makes sense to apply a staggered approach as a
function of context length – first ensure performance works up to 32K context, then 128K context,
and then finally 1M context. As it is more expensive to run longer context evaluations, this approach
allows the model developer to iterate more quickly.
Withthisdevelopmentprocessinmind, weprovidethreesubsetsforeachevaluation: Adistribution
of samples up to 32K context, a distribution of samples up to 128K context, and a distribution of
samples up to 1M context. All of these individual subsets measure cumulative performance, and can
be stacked together and normalized by bucket (e.g., we can consider only 32K subset, the union of
32K and 128K subsets, or the union of all three subsets 32K, 128K, and 1M, weighted equally) to
decrease variance. To normalize, we divide by the number of repetitions of each bucket in order
to keep a similar histogram profile of the context lengths. For instance, when considering all three
subsets (32K, 128K, and 1M), we divide the 32K bucket by 3 since it shows up three times, and the
128K bucket by 2 since it shows up twice.
3.3. Prompting Approach
For all tasks, we first present the model with a description of the task, followed by a few-shot approach
in which we introduce demonstrations of short-context examples of the task. The full prompts are
in Appendix A. The prompt for the MRCR task is additionally noteworthy. In MRCR, the model is
required to reproduce a chunk of text up to512 tokens, and we have found it helpful to require
the model to prefix a random string to the output. This prefix helps test that the model is following
instructions and helps post-process the longer output of the model, which should be only the content
output following the random string. This tweak to the prompt is particularly important for MRCR,
since we score the model output based on an approximate edit distance score, which is in contrast
with the majority of existing long-context evaluations (and which has the nice property of resulting
in a very smooth context-performance curve that degrades as a function of context).
3.3.1. Pre-training and Post-training Evaluation
While in this report we only describe results on post-trained models, we note that we have successfully
used the Latent List and MRCR tasks as pretraining evaluations as well. For pretraining evaluations,
the few-shot nature of the prompts is critical to ensuring a good signal. For post-training evaluations
on all models we tried, MRCR worked out of the box with no tweaks. On the other hand, Latent List
and IDK both required additional post-processing in order to ensure the signal was captured due to
variations in model output styles.
4. Experimental Results
We evaluate ten frontier models of varying sizes on our long beyond-retrieval evaluation tasks: Gemini
1.5 Flash (05-14 and 08-27) and Pro (05-14 and 08-27) (Google et al., 2024), GPT-4 Turbo (04-09)
and GPT-4o (OpenAI, 2023, 2024), Claude 3 Haiku, Sonnet, and Opus (Anthropic, 2024a), and
Claude 3.5 Sonnet (Anthropic, 2024b). In our experiments, we observe that the rank orderings differ
across each of our evaluations – further highlighting the fact that we are capturing a broad spectrum
of long-context reasoning behavior with a minimal set of evaluations.
11Michelangelo: Long Context Evaluations Beyond Haystacks via Latent Structure Queries
2K 8K 32K 128K
Number of tokens in context
0.65
0.70
0.75
0.80
0.85
0.90
0.95
1.00Cumulative Average Score
Multi-Round Coreference Resolution (MRCR) 
GPT 4o
Claude 3 Opus
Gemini 1.5 Flash (08/27)
Gemini 1.5 Pro (08/27)
Figure 3|Gemini models perform the best on MRCR at 128K context, and notably have a significantly
different slope profile compared to GPT and Claude.
For each task, we present plots which set average context length in tokens against the cumulative
average score for the task at 128K and 1M context. In all our plots, higher score is better and is
bounded between[0,1]. For 1M context, we only plot the Gemini 1.5 models, while the others are
all plotted up to 128K context length. For each plot, we select the best performing models from each
family to be plotted. Note that for the Latent List tasks, Claude-3 Haiku and Sonnet and Claude-3.5
Sonnet performance was very low due to model refusal rates. We exclude these models from the
plots.
4.1. Initial Degradation up to 128K
We now consider a closer look up to 128K context for each of the evaluations, and identify the winning
model families. See Figures 3, 4, and 5.
4.2. Gemini’s Non-Decreasing Performance from 128K to 1M
In this section, we demonstrate that on these evaluations, Gemini performance does not decrease
from 128K to 1M context. See Figures 6, 7, and 8.
12Michelangelo: Long Context Evaluations Beyond Haystacks via Latent Structure Queries
512 8K 32K 128K
Number of tokens in context
0.2
0.4
0.6
0.8
1.0Cumulative Average Score
Latent List 
GPT 4o
Claude 3 Opus
Gemini 1.5 Flash (08/27)
Gemini 1.5 Pro (08/27)
Figure 4|GPT models perform the best on Latent List at 128K context.
512 8K 32K 128K
Number of tokens in context
0.60
0.65
0.70
0.75
0.80
0.85
0.90
0.95
1.00Cumulative Average Score
IDK (Multi-letter Variant) 
Gemini 1.5 Flash (05/14)
Gemini 1.5 Pro (05/14)
GPT 4o
Claude 3.5 Sonnet
Figure 5|Claude-3.5 Sonnet performs the best on IDK, with Gemini 1.5 Pro (05/14) following closely
behind. GPT models perform poorly on this task, with early performance dropping off rapidly, though
performance stabilizes shortly thereafter.
13Michelangelo: Long Context Evaluations Beyond Haystacks via Latent Structure Queries
2K 128K 512K 1M
Number of tokens in context
0.60
0.65
0.70
0.75
0.80
0.85
0.90
0.95
1.00Cumulative Average Score
Multi-Round Coreference Resolution (MRCR) 
GPT 4o
Claude 3 Opus
Gemini 1.5 Flash (08/27)
Gemini 1.5 Pro (08/27)
Figure 6|On MRCR, Gemini models have non-degrading performance up to 1M context after the
initial drop in performance, while GPT and Claude models have a more negative slope at 128K context.
Gemini 1.5 Pro has the highest performance on this task.
512 128K 512K 1M
Number of tokens in context
0.0
0.2
0.4
0.6
0.8
1.0Cumulative Average Score
Latent List 
GPT 4o
GPT 4T-0409
Claude 3 Opus
Gemini 1.5 Flash (08/27)
Gemini 1.5 Pro (08/27)
Figure 7|On Latent List, Gemini models have non-degrading performance up to 1M context after
the initial drop in performance, while GPT and Claude models have a more negative slope at 128K
context. GPT-4o has the highest performance on this task at 128K context, though it is unclear how it
fairs at 1M context.
14Michelangelo: Long Context Evaluations Beyond Haystacks via Latent Structure Queries
512 128K 512K 1M
Number of tokens in context
0.60
0.65
0.70
0.75
0.80
0.85
0.90
0.95
1.00Cumulative Average Score
IDK (Multi-letter Variant) 
Gemini 1.5 Flash (05/14)
Gemini 1.5 Pro (05/14)
GPT 4o
Claude 3 Opus
Claude 3.5 Sonnet
Figure 8|On IDK, Gemini models have non-degrading performance up to 1M context after the initial
drop in performance. All models appear to have a flat trend after the initial degradation. Claude-3.5
Sonnet has the highest performance at 128K context.
5. Discussion
5.1. Measuring Different Aspects of Long Context Understanding
First, we establish that Michelangelo has subtasks which measure different components of long
context utilization. MRCR measures the model’s ability to understanding ordering in natural text, to
distinguish between similar drafts of writing, and to reproduce a specified piece of previous context
subject to adversarially difficult queries. Latent List measures the ability of a model to track a latent
data structure’s properties over the course of a stream of code instructions. IDK measures the model’s
ability to understand whether it knows what it doesn’t know based on the presented context. Each
of these long context synthesis primitives is significantly more difficult than both needle retrieval
and multi-needle retrieval tasks, and this result is reflected in the context-vs-average-cumulative-
performance curves.
Notably, the model families we evaluate each perform significantly differently on these core
primitives. Gemini models perform the best on MRCR, GPT models outperform others on Latent List,
and Claude-3.5 Sonnet performs the best on IDK (while intriguingly, GPT performs the worst on
IDK!).
Figure 9 records the cross-correlations between these evaluations, measured across ten different
models and calculated using Spearman rank correlation. Thus we also have quantitative evidence
that Michelangelo evaluations measure diverse aspects of long context understanding.
Based on these results, we can conclude that each evaluation measures something rather different
in terms of long context understanding, and there are possibly even trade-offs in performance to be
made across these three evaluations. Of the frontier models we have measured, none is the clear
winner on all three Michelangelo tasks.
15Michelangelo: Long Context Evaluations Beyond Haystacks via Latent Structure Queries
MRCR Latent List IDK
MRCRLatent ListIDK
1 0.64 0.043
0.64 1 -0.25
0.043 -0.25 1
Spearman Cross-Rank Correlation for Michelangelo
0.2
0.0
0.2
0.4
0.6
0.8
1.0
Figure 9|Spearman Rank Correlation between Michelangelo evaluations at 128K context over all ten
models. We observe that Latent List and IDK are anti-correlated, while MRCR and Latent List have
some rank correlation, it is fairly far from1. These rank correlations were computed only over the
ten models in this report.
5.2. Degradation before 32K Context
As we can observe from the context versus performance plots (Figures 3, 4, and 5), the models often
degrade in score quite early in the context (at 32K), allowing us to more efficiently detect degradation
in long-context performance when using Michelangelo . This property is in contrast with existing
long evaluation benchmarks like Hsieh et al. (2024), where one must look as far as 128K context to
see any degradation in performance.
In particular, we see a very common trend across all of the Michelangelo evaluations: There is
one initial sharp super-linear drop in performance in short-context (note that the task complexity
is kept fixed across the context, so this drop is entirely due to the model’s long-context processing
capabilities), after which performance often either flattens out or continues to degrade at a roughly
linear rate. In the cases where performance flattens out, this behavior often lasts until very large
context lengths. We can interpret this behavior as suggesting that the model has sub-capabilities
which are sufficiently good to achieve a certain level of performance on a given task, and that those
sub-capabilities length-generalize all the way to very large context lengths.
5.3. Cross-Over Behaviors with Increasing Context Length
We note that different models’ performance drops at different rates as a function of context length,
leading to scenarios where model performance on a given evaluation crosses over – at long enough
context lengths, one model which was previously worse on a task may become better than another
model which was previously better. We observe this phenomenon in Figure 6, 7, 8, and 13. Generally,
Gemini models often outstrip the performance of GPT and Claude models on each of the various tasks
at long enough context lengths, though not always. It is particularly interesting to note that Gemini
1.5 Flash outperforms GPT and Claude models on MRCR and IDK at long enough context lengths,
and that advantage is retained up to 1M tokens of context.
This behavior highlights a potential tradeoff between short-context performance and long-context
performancethatthesemodelsmaymake. OnMRCR,wenotethatGPTandClaudemodelsoutperform
Gemini models on context smaller than 8K, but have a correspondingly higher rate of decay in length
generalization performance, indicating that short and long context performance may be difficult to
balance.
16Michelangelo: Long Context Evaluations Beyond Haystacks via Latent Structure Queries
5.4. Parallel Model Family Curves on MRCR
In Figure 2, we pointed out that various model families have parallel curves on MRCR, an intriguing
phenomenon. We highlight this property as both evidence for the fact that the evaluation bears high
signal, as well as a pointer to paths for future investigation. Since the curves between certain model
pairs so closely parallel each other, we suspect that there were uniquely similar aspects of the model
training process in these models (even though there may be an absolute disparity in performance).
Future work should investigate evaluations which reveal implicit information about model training.
We also note that we found MRCR to be a particularly robust measure of performance. MRCR
behavior across models required no changes to the prompt in order to ensure the model gave a
verifiable output with low variance.
5.5. Stratifying Latent List by Complexity
In this section we validate the claim that higher complexity tasks are more difficult as measured by
score (Figures 10, 11, 12). We also note that the highest complexity task has a significantly steeper
slope as a function of context length, especially for the highest performing model, GPT-4o, suggesting
that as the task complexity increases, the more rapid the performance fall-off as a function of context
length. Notably, on the highest task complexity, performance of GPT-4o is on par with GPT-4T and
the Gemini 1.5 models (Figure 12).
512 8K 32K 128K
Number of tokens in context
0.2
0.4
0.6
0.8
1.0Cumulative Average Score
Latent List (Complexity Level: [1])
GPT 4o
GPT 4T-0409
Claude 3 Opus
Gemini 1.5 Flash (08/27)
Gemini 1.5 Pro (08/27)
Figure 10|Latent List: Complexity Level1
5.6. Slicing IDK Performance When the Answer is “I Don’t Know”
In Figure 13 we similarly observe a steeper fall-off curve in performance when we restrict IDK to
task instances where the answer is only “I don’t know”. This task is harder because it requires the
model to assess the full context for information which may not be present, rather than identify a
single location where an answer may be located. Notably, Gemini 1.5 Pro performs slightly better
than Claude 3.5 Sonnet on long-enough contexts on this subtask.
17Michelangelo: Long Context Evaluations Beyond Haystacks via Latent Structure Queries
512 8K 32K 128K
Number of tokens in context
0.0
0.2
0.4
0.6
0.8
1.0Cumulative Average Score
Latent List (Complexity Level: [5])
GPT 4o
GPT 4T-0409
Claude 3 Opus
Gemini 1.5 Flash (08/27)
Gemini 1.5 Pro (08/27)
Figure 11|Latent List: Complexity Level5. At 128K, Gemini and GPT-4T have similar performance.
5.7. An Interesting Failure Mode: GPT Models Hallucinate Puzzles in IDK
We previously noted that the GPT-4 family performs significantly worse than the other model families
on IDK in Figure 13 – this is due to an interesting quirk in that model family’s behavior. When
presented with a string of random letters, both GPT-4 models sometimes assume that there is a hidden
riddle in the text, and attempt to “solve" the riddle by hallucinating the presence of one of the answer
choices. This leads to a significantly worse performance. We provide examples of this behavior in
Figure 14.
Note that the OpenAI API does not allow inputs with simpler context (like for instance, “X X
X··· ”), presumably due to repetition attack schemes (Nasr et al., 2023) which cause the model to
reproduce its training data.
18Michelangelo: Long Context Evaluations Beyond Haystacks via Latent Structure Queries
512 8K 32K 128K
Number of tokens in context
0.0
0.2
0.4
0.6
0.8
1.0Cumulative Average Score
Latent List (Complexity Level: [20])
GPT 4o
GPT 4T-0409
Claude 3 Opus
Gemini 1.5 Flash (08/27)
Gemini 1.5 Pro (08/27)
Figure 12|Latent List: Complexity Level20. At 128K, Gemini and both GPT models have similar
performance.
512 8K 32K 128K
Number of tokens in context
0.60
0.65
0.70
0.75
0.80
0.85
0.90
0.95
1.00Cumulative Average Score
IDK (Multi-letter Variant) (Unknown Answer)
Gemini 1.5 Flash (05/14)
Gemini 1.5 Pro (05/14)
GPT 4o
Claude 3.5 Sonnet
Figure 13|IDK: The Answer is Always Unknown. At around 8K context, Gemini 1.5 Pro overtakes
Claude 3.5 Sonnet as the top performer on this subtask.
19Michelangelo: Long Context Evaluations Beyond Haystacks via Latent Structure Queries
Failure Modes for GPT-4o on IDK
IDK Example 1
A man is walking down the street when he sees a friend. The friend
is wearing a Stetson hat and Oakley sunglasses. W F D O F J F J U Q
C M Z J U A G O C E W W H R S L E E U W I L ···
...
Question: What is the friends name?
Choices:
(A) John Oakley
(B) Jim Stetson
(C) Bob Field
(D) I don’t know
Answer:
GPT-4o’s Incorrect Response
The friend’s name can be deduced from the description given. The
friend is wearing a Stetson hat and Oakley sunglasses. The choices
provided are: (A) John Oakley (B) Jim Stetson (C) Bob Field (D)
I don’t know Given the clues: - Stetson hat (suggesting the
name Stetson) - Oakley sunglasses (suggesting the name Oakley)
The correct answer is likely to be a combination of these clues.
Therefore, the friend’s name is: (B) Jim Stetson
IDK Example 2
I am searching for jobs in the computer technology industry. X F R
L L O G W J Z D V F X E G Z W G E A L C R W N T E D A F Y ···
...
Question: What company did I not apply too?
Choices:
(A) Google
(B) Meta
(C) Apple
(D) I don’t know
Answer:
GPT-4o’s Incorrect Response
Based on the provided string, which appears to be a word search
puzzle, the task is to identify which of the companies listed in
the choices (Google, Meta, Apple) does not appear in the puzzle.
Let’s look for each company name in the string: 1. **Google**:
- Searching for "GOOGLE" in the string, we find "GOOGLE" at the
position starting from the 67th character: ‘...G O O G L E...‘ 2.
**Meta**: - Searching for "META" in the string, we find "META"
at the position starting from the 626th character: ‘...M E T
A...‘ 3. **Apple**: - Searching for "APPLE" in the string, we
do not find the sequence "APPLE" anywhere in the string. Given
this information, the answer is: (C) Apple
Figure 14|Examples for which GPT-4o fails on IDK tasks due to hallucinating puzzles.
20Michelangelo: Long Context Evaluations Beyond Haystacks via Latent Structure Queries
6. Related Work
6.1. Broader Work on Long Context Evaluation
For the purposes of this paper, we consider “long-context” to mean at least 32K context examples
(ideally at least 128K context). There have been many recent long-context evaluation benchmarks,
which tend to focus on retrieval (Bohnet et al., 2024b; Google et al., 2024; Hsieh et al., 2024; Kamradt,
2023; Kočisk`y et al., 2018; Li et al., 2024; Zhang et al., 2024) and in-context many-shot learning
(Agarwal et al., 2024; Bohnet et al., 2024a) rather than reasoning. There is also existing work on
long-context summarization (Chang et al., 2024; Kim et al., 2024). In this paper we focus exclusively
on evaluations for reasoning over a long-context. In particular, we assume that there is a sparse
set of relevant information embedded in the context, not all of which is immediately identifiable
independent of other portions of the context. The relevant information must be synthesized rather
than merely retrieved.
Another popular approach for claiming long-context capability is to create plots of context length
vs. average perplexity on a designated set of long documents (Anthropic, 2023; Google et al., 2024).
It is worth noting that the decreasing perplexity for a fixed model (the difference in magnitude
between the average perplexities at the longest context and the shortest context) is anti-correlated
with the difference in error between the longest context and the shortest context on the long reasoning
evaluations we introduce in this report. Note that the anti-correlation is immediate from the fact
that the perplexity plots are approximately monotone decreasing as a function of context length,
while the error plots for the evaluations we present are approximately monotone increasing as a
function of context lengths (accuracy is approximately monotone decreasing). Thus, examining the
context-vs-perplexity plot on a single model may not be a proxy for understanding model performance
as measured by accuracy on complex long reasoning tasks, though it is possible that it can be a proxy
for capabilities like many-shot in-context learning for translation tasks (see for instance some of the
examples in Google et al. (2024)).
Another noteworthy point is that the majority of long-context evaluations use multiple-choice
formats (sometimes only with two possible responses) – this paradigm is considerably limiting. While
some other methods use ROUGE-style scores (especially in summarization) or human evaluations,
both of these approaches can be additionally problematic in both noisiness (see for instance the
discussions in Cohan and Goharian (2016) and Akter et al. (2022)) and cost.
6.2. Challenges with Developing Reliable and Minimal Long Reasoning Evaluations
There are a few pre-existing evaluations which study arbitrarily long reasoning tasks, with evaluations
sometimes going up to 1M context, including Hsieh et al. (2024); Kuratov et al. (2024); Lee et al.
(2024); Li et al. (2024); Zhang et al. (2024). These existing evaluations tend to suffer from at least
one of a few deficiencies:
• Short-circuiting: When an evaluation task does not account for information in either training
data or later in the context which the model can use to avoid requiring the full context to answer
a query, we say the model can “short-circuit” the long-context and perform well on the task
while not actually representing a utilization of the context. As one example, the tasks in Kuratov
et al. (2024) may all suffer from this problem, both because the evaluation from which the
expanded tasks are derived is heavily leaked and because the tasks themselves are significantly
flawed (Kaushik and Lipton, 2018). Many similarly constructed evaluations (perhaps using
different base evaluations) have the same issues.
• Secret retrieval tasks:Some recently popular evaluation benchmarks (for instance, Hsieh
21Michelangelo: Long Context Evaluations Beyond Haystacks via Latent Structure Queries
et al. (2024); Li et al. (2024)) describe a subset of their tasks as testing long-context reasoning
capabilities. However, due to their construction, they often do not represent a task which
requires anything more than single-needle or multi-needle retrieval. As one example of such a
task, we consider the Variable Tracing (VT) task from RULER (Hsieh et al., 2024). On the surface
level, the task contains a context filled with many variable assignments (e.g.𝑋 = 3; 𝑌 = 𝑋; 𝑍 = 𝑌
and so on, interspersed with distractor context). The model is then required to enumerate
all variables which have the value3. However, in the default RULER implementation of this
task, every variable which has been introduced in the context actually indeed has the value3 -
there are no spurious variables present. Thus this setting of the task is ultimately reduced to
a multi-needle retrieval task, where the needles correspond to the mentioned variable names,
and is why we see exceedingly high performance on this task from many models.
• Out-of-distribution distractor context:Many existing tasks proceed by inserting dramatically
out-of-distribution context in order to form the irrelevant information component of the evalua-
tion, such as Paul Graham essays or repeated phrases (Hsieh et al., 2024; Kamradt, 2023; Li
et al., 2024). This setup makes the problem significantly easier, since it implicitly brings any
long reasoning task closer to a retrieval task – if the relevant information isa prioriidentifiable
without understanding anything about the interrelated nature of the relevant information, then
the task becomes effectively a multi-needle retrieval task.
• Training on toy synthetic tasks:Some evaluation methods (often older evaluation sets from
the pre-modern LLM era, like the Long Range Arena (LRA) (Tay et al., 2020)) require the model
to train on the task to see good performance on the task. These evaluations test the ability of a
given architecture tolearn a specific task, which while potentially interesting, is not the key
point of interest in our setting, as we hope that via next-token prediction we will learn far more
interesting long reasoning circuits that apply more generally over language and multimodal
elements.
See Appendix D for a more detailed breakdown of these issues and how they apply to additional
existing proposed long reasoning evaluations.
Another relevant long-reasoning evaluation is the SPIDER task in the LOFT benchmark (Lee et al.
(2024)), which tests multi-hop reasoning for SQL queries. SPIDER is quite focused on a particular
application (SQL), and we include some orthogonal tests of long reasoning in our benchmark.
Michelangelo avoids the above pitfalls by ensuring that the irrelevant information in the context
is indeed completely unrelated to the information required to answer the query, and by ensuring that
the relevant information in the query is completely unique and/or there is no information present in
training data which would be helpful for answering the query, since it is too generic. For most of the
tasks, we also ensure that the irrelevant information is not significantly out-of-distribution compared
to the relevant information, thus creating a more realistic setting where the relevant information is
not clearly different from surrounding context.
7. Conclusion
We introduced the Michelangelo evaluations, a suite of long-context synthesis and reasoning eval-
uations built using the Latent Structure Queries framework for developing long-context reasoning
evaluations which can be arbitrarily extended in length and set to arbitrary complexity levels while
avoiding leaked context from pre-existing evaluations. We propose three minimal simple tasks in this
framework – Latent List, MRCR, and IDK – which highlight simple cases of tasks beyond retrieval
which current frontier long-context models do not solve, even up to 32K context. We analyze ten
frontier models with long-context capabilities on these tasks up to 128K context, and we further run
22Michelangelo: Long Context Evaluations Beyond Haystacks via Latent Structure Queries
these evaluations on the Gemini 1.5 family up to 1M context, and observe that though there is a
significant initial degradation in behavior on these evaluations, after a certain point, many (but not
all) frontier models experience a non-trivial flattening of the context-vs-performance curve, suggesting
that while some long-context capabilities are present in these models (e.g. the ability to retrieve),
there is still a significant gap in capability compared to the goal of reasoning over long contexts.
One important part of the contributions of this work is to validate the robustness and smoothness
of MRCR across multiple frontier models. MRCR benefits from using a very simple metric and is more
robust to prompting choices as compared to Needle-in-a-Haystack. In internal testing, we have also
discovered that this evaluation has high signal for smaller models as well as models which have not
been post-trained. Thus, we also make a recommendation – given that MRCR minimally tests a simple
primitive of beyond-retrieval understanding of long contexts while also capturing harder distractor
contexts for retrieval, MRCR and its natural extensions are a suitable default replacement for the
popular Needle-in-a-Haystack evaluation, in which the needle is often quite apparently different from
the rest of the text in a manner that stands out and which fails to capture nuance in measuring model
capabilities over long-contexts.
8. Contributions
Corresponding Author: Kiran Vodrahalli
Core Contributors: Santiago Ontañón, Nilesh Tripuraneni, Kelvin Xu, Sanil Jain, Rakesh Shivanna,
Jeffrey Hui, Nishanth Dikkala, Mehran Kazemi, Bahare Fatemi
Contributors: Rohan Anil, Ethan Dyer, Siamak Shakeri, Roopali Vij, Harsh Mehta, Vinay Ramasesh,
Quoc Le, Ed Chi, Yifeng Lu, Orhan Firat, Angeliki Lazaridou, Jean-Baptiste Lespiau, Nithya Attaluri,
Kate Olszewska
The roles are defined as below:
• Corresponding Author: Led the project, created many of the evaluations and the overall frame-
work.
• Core Contributor: Individual who had significant impact throughout the project.
• Contributor: Individual that had partial contributions to the project and was partially involved
with the effort.
Within each category, the authors are presented in no particular order.
References
R. Agarwal, A. Singh, L. M. Zhang, B. Bohnet, L. Rosias, S. Chan, B. Zhang, A. Anand, Z. Abbas,
A. Nova, J. D. Co-Reyes, E. Chu, F. Behbahani, A. Faust, and H. Larochelle. Many-shot in-context
learning, 2024. URLhttps://arxiv.org/abs/2404.11018.
M. Akter, N. Bansal, and S. K. Karmaker. Revisiting automatic evaluation of extractive summarization
task: Can we do better than ROUGE? In S. Muresan, P. Nakov, and A. Villavicencio, editors,Findings
of the Association for Computational Linguistics: ACL 2022, pages 1547–1560, Dublin, Ireland, May
2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.findings-acl.122. URL
https://aclanthology.org/2022.findings-acl.122.
R. Anil, A. M. Dai, O. Firat, M. Johnson, D. Lepikhin, A. Passos, S. Shakeri, E. Taropa, P. Bailey,
Z. Chen, E. Chu, J. H. Clark, L. E. Shafey, Y. Huang, K. Meier-Hellstern, G. Mishra, E. Moreira,
23Michelangelo: Long Context Evaluations Beyond Haystacks via Latent Structure Queries
M. Omernick, K. Robinson, S. Ruder, Y. Tay, K. Xiao, Y. Xu, Y. Zhang, G. H. Abrego, J. Ahn, J. Austin,
P. Barham, J. Botha, J. Bradbury, S. Brahma, K. Brooks, M. Catasta, Y. Cheng, C. Cherry, C. A.
Choquette-Choo, A. Chowdhery, C. Crepy, S. Dave, M. Dehghani, S. Dev, J. Devlin, M. Díaz, N. Du,
E. Dyer, V. Feinberg, F. Feng, V. Fienber, M. Freitag, X. Garcia, S. Gehrmann, L. Gonzalez, G. Gur-Ari,
S. Hand, H. Hashemi, L. Hou, J. Howland, A. Hu, J. Hui, J. Hurwitz, M. Isard, A. Ittycheriah,
M. Jagielski, W. Jia, K. Kenealy, M. Krikun, S. Kudugunta, C. Lan, K. Lee, B. Lee, E. Li, M. Li,
W. Li, Y. Li, J. Li, H. Lim, H. Lin, Z. Liu, F. Liu, M. Maggioni, A. Mahendru, J. Maynez, V. Misra,
M. Moussalem, Z. Nado, J. Nham, E. Ni, A. Nystrom, A. Parrish, M. Pellat, M. Polacek, A. Polozov,
R. Pope, S. Qiao, E. Reif, B. Richter, P. Riley, A. C. Ros, A. Roy, B. Saeta, R. Samuel, R. Shelby,
A. Slone, D. Smilkov, D. R. So, D. Sohn, S. Tokumine, D. Valter, V. Vasudevan, K. Vodrahalli, X. Wang,
P. Wang, Z. Wang, T. Wang, J. Wieting, Y. Wu, K. Xu, Y. Xu, L. Xue, P. Yin, J. Yu, Q. Zhang, S. Zheng,
C. Zheng, W. Zhou, D. Zhou, S. Petrov, and Y. Wu. Palm 2 technical report, 2023.
Anthropic. Model Card and Evaluations for Claude Models, 2023.
Anthropic. The Claude 3 Model Family: Opus, Sonnet, Haiku, 2024a.
Anthropic. Claude 3.5 Sonnet Model Card Addendum, 2024b.
S. Arora, S. Eyuboglu, A. Timalsina, I. Johnson, M. Poli, J. Zou, A. Rudra, and C. Ré. Zoology:
Measuring and improving recall in efficient language models, 2023. URLhttps://arxiv.org/
abs/2312.04927.
Y. Bai, X. Lv, J. Zhang, H. Lyu, J. Tang, Z. Huang, Z. Du, X. Liu, A. Zeng, L. Hou, et al. Longbench: A
bilingual, multitask benchmark for long context understanding.arXiv preprint arXiv:2308.14508,
2023.
B. Bohnet, A. Nova, A. T. Parisi, K. Swersky, K. Goshvadi, H. Dai, D. Schuurmans, N. Fiedel, and
H. Sedghi. Exploring and benchmarking the planning capabilities of large language models, 2024a.
URL https://arxiv.org/abs/2406.13094.
B. Bohnet, K. Swersky, R. Liu, P. Awasthi, A. Nova, J. Snaider, H. Sedghi, A. T. Parisi, M. Collins,
A. Lazaridou, O. Firat, and N. Fiedel. Long-span question-answering: Automatic question generation
and qa-system ranking via side-by-side evaluation, 2024b. URLhttps://arxiv.org/abs/2406.
00179.
Y. Chang, K. Lo, T. Goyal, and M. Iyyer. Booookscore: A systematic exploration of book-length sum-
marization in the era of LLMs. InThe Twelfth International Conference on Learning Representations,
2024. URLhttps://openreview.net/forum?id=7Ttk3RzDeu.
A. Cohan and N. Goharian. Revisiting summarization evaluation for scientific articles. In N. Calzolari,
K. Choukri, T. Declerck, S. Goggi, M. Grobelnik, B. Maegaard, J. Mariani, H. Mazo, A. Moreno,
J. Odijk, and S. Piperidis, editors,Proceedings of the Tenth International Conference on Language
Resources and Evaluation (LREC’16), pages 806–813, Portorož, Slovenia, May 2016. European
Language Resources Association (ELRA). URLhttps://aclanthology.org/L16-1130.
O. Goldman, A. Jacovi, A. Slobodkin, A. Maimon, I. Dagan, and R. Tsarfaty. Is it really long context
if all you need is retrieval? towards genuinely difficult long context nlp, 2024. URLhttps:
//arxiv.org/abs/2407.00402.
Google, P. Georgiev, V. I. Lei, R. Burnell, L. Bai, A. Gulati, G. Tanzer, D. Vincent, Z. Pan, S. Wang,
S. Mariooryad, Y. Ding, X. Geng, F. Alcober, R. Frostig, M. Omernick, L. Walker, C. Paduraru,
C. Sorokin, A. Tacchetti, C. Gaffney, S. Daruki, O. Sercinoglu, Z. Gleicher, J. Love, P. Voigtlaender,
24Michelangelo: Long Context Evaluations Beyond Haystacks via Latent Structure Queries
R. Jain, G. Surita, K. Mohamed, R. Blevins, J. Ahn, T. Zhu, K. Kawintiranon, O. Firat, Y. Gu,
Y. Zhang, M. Rahtz, M. Faruqui, N. Clay, J. Gilmer, J. Co-Reyes, I. Penchev, R. Zhu, N. Morioka,
K. Hui, K. Haridasan, V. Campos, M. Mahdieh, M. Guo, S. Hassan, K. Kilgour, A. Vezer, H.-T. Cheng,
R. de Liedekerke, S. Goyal, P. Barham, D. Strouse, S. Noury, J. Adler, M. Sundararajan, S. Vikram,
D. Lepikhin, M. Paganini, X. Garcia, F. Yang, D. Valter, M. Trebacz, K. Vodrahalli, C. Asawaroengchai,
R. Ring, N. Kalb, L. B. Soares, S. Brahma, D. Steiner, T. Yu, F. Mentzer, A. He, L. Gonzalez, B. Xu, R. L.
Kaufman, L. E. Shafey, J. Oh, T. Hennigan, G. van den Driessche, S. Odoom, M. Lucic, B. Roelofs,
S. Lall, A. Marathe, B. Chan, S. Ontanon, L. He, D. Teplyashin, J. Lai, P. Crone, B. Damoc, L. Ho,
S. Riedel, K. Lenc, C.-K. Yeh, A. Chowdhery, Y. Xu, M. Kazemi, E. Amid, A. Petrushkina, K. Swersky,
A. Khodaei, G. Chen, C. Larkin, M. Pinto, G. Yan, A. P. Badia, P. Patil, S. Hansen, D. Orr, S. M. R.
Arnold, J. Grimstad, A. Dai, S. Douglas, R. Sinha, V. Yadav, X. Chen, E. Gribovskaya, J. Austin,
J. Zhao, K. Patel, P. Komarek, S. Austin, S. Borgeaud, L. Friso, A. Goyal, B. Caine, K. Cao, D.-W.
Chung, M. Lamm, G. Barth-Maron, T. Kagohara, K. Olszewska, M. Chen, K. Shivakumar, R. Agarwal,
H. Godhia, R. Rajwar, J. Snaider, X. Dotiwalla, Y. Liu, A. Barua, V. Ungureanu, Y. Zhang, B.-O.
Batsaikhan, M. Wirth, J. Qin, I. Danihelka, T. Doshi, M. Chadwick, J. Chen, S. Jain, Q. Le, A. Kar,
M. Gurumurthy, C. Li, R. Sang, F. Liu, L. Lamprou, R. Munoz, N. Lintz, H. Mehta, H. Howard,
M. Reynolds, L. Aroyo, Q. Wang, L. Blanco, A. Cassirer, J. Griffith, D. Das, S. Lee, J. Sygnowski,
Z. Fisher, J. Besley, R. Powell, Z. Ahmed, D. Paulus, D. Reitter, Z. Borsos, R. Joshi, A. Pope, S. Hand,
V. Selo, V. Jain, N. Sethi, M. Goel, T. Makino, R. May, Z. Yang, J. Schalkwyk, C. Butterfield, A. Hauth,
A. Goldin, W. Hawkins, E. Senter, S. Brin, O. Woodman, M. Ritter, E. Noland, M. Giang, V. Bolina,
L. Lee, T. Blyth, I. Mackinnon, M. Reid, O. Sarvana, D. Silver, A. Chen, L. Wang, L. Maggiore,
O. Chang, N. Attaluri, G. Thornton, C.-C. Chiu, O. Bunyan, N. Levine, T. Chung, E. Eltyshev, X. Si,
T. Lillicrap, D. Brady, V. Aggarwal, B. Wu, Y. Xu, R. McIlroy, K. Badola, P. Sandhu, E. Moreira,
W. Stokowiec, R. Hemsley, D. Li, A. Tudor, P. Shyam, E. Rahimtoroghi, S. Haykal, P. Sprechmann,
X. Zhou, D. Mincu, Y. Li, R. Addanki, K. Krishna, X. Wu, A. Frechette, M. Eyal, A. Dafoe, D. Lacey,
J. Whang, T. Avrahami, Y. Zhang, E. Taropa, H. Lin, D. Toyama, E. Rutherford, James Phoenix  
& Mike Taylor
Prompt 
Engineering for 
Generative AI
Future-Proof Inputs for Reliable AI Outputs
Free 
ChapterThis excerpt contains Chapter 1. The complete book is
available on the O’Reilly Online Learning Platform and
through other retailers.
James Phoenix and Mike Taylor
Prompt Engineering for
Generative AI
Future-Proof Inputs for Reliable
AI Outputs at Scale
Boston Farnham Sebastopol TokyoBeijing Boston Farnham Sebastopol TokyoBeijing978-1-098-15343-4
[LSI]
Prompt Engineering for Generative AI
by James Phoenix and Mike Taylor
Copyright © 2024 Saxifrage, LLC and Just Understanding Data LTD. All rights reserved.
Printed in the United States of America.
Published by O’Reilly Media, Inc., 1005 Gravenstein Highway North, Sebastopol, CA 95472.
O’Reilly books may be purchased for educational, business, or sales promotional use. Online editions are
also available for most titles (http://oreilly.com). For more information, contact our corporate/institutional
sales department: 800-998-9938 or corporate@oreilly.com.
Acquisitions Editor: Nicole Butterfield
Development Editor: Corbin Collins
Copyeditor: Piper Editorial Consulting, LLC
Proofreader: Kim Wimpsett
Indexer: nSight, Inc.
Interior Designer: David Futato
Cover Designer: Karen Montgomery
Illustrator: Kate Dullea
May 2024:  First Edition
Revision History for the First Edition
2024-05-15: First Release
See http://oreilly.com/catalog/errata.csp?isbn=9781098153434 for release details.
The O’Reilly logo is a registered trademark of O’Reilly Media, Inc. Prompt Engineering for Generative AI,
the cover image, and related trade dress are trademarks of O’Reilly Media, Inc.
The views expressed in this work are those of the authors and do not represent the publisher’s views.
While the publisher and the authors have used good faith efforts to ensure that the information and
instructions contained in this work are accurate, the publisher and the authors disclaim all responsibility
for errors or omissions, including without limitation responsibility for damages resulting from the use
of or reliance on this work. Use of the information and instructions contained in this work is at your
own risk. If any code samples or other technology this work contains or describes is subject to open
source licenses or the intellectual property rights of others, it is your responsibility to ensure that your use
thereof complies with such licenses and/or rights.Table of Contents
1. The Five Principles of Prompting. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  1
Overview of the Five Principles of Prompting                                                              4
1. Give Direction                                                                                                                8
2. Specify Format                                                                                                              14
3. Provide Examples                                                                                                        17
4. Evaluate Quality                                                                                                           20
5. Divide Labor                                                                                                                 32
Summary                                                                                                                           38
iiiCHAPTER 1
The Five Principles of Prompting
Prompt engineering is the process of discovering prompts that reliably yield useful or
desired results.
A prompt is the input you provide, typically text, when interfacing with an AI model
like ChatGPT or Midjourney. The prompt serves as a set of instructions the model
uses to predict the desired response: text from large language models  (LLMs) like
ChatGPT, or images from diffusion models like Midjourney.
Here is a simple example of a prompt input for a product name generator (inspired
by one of OpenAI’s examples), and the resulting output from ChatGPT.
Input:
Can I have a list of product names for a pair of shoes that
can fit any foot size?
Output:
Certainly! Here's a list of product names for a pair of shoes
that can fit any foot size:
UniFit SoleStrides
FlexiSize All-Fit Shoes
OmniFit FootFlex
...
AnyStep AdaptiWalkers
FlexiRange UniversalSteps
PerfectGlide MultiFit Shoes
1This is already a remarkable response for a naive prompt, which feels like magic
because we got here with very little effort. As the state-of-the-art models improve,
the likelihood you will get good enough  results on your first try goes up. For any
throwaway interactions with an AI, where you don’t plan to do the same task again,
the naive approach is all you need.
However, if you planned to put this prompt into production, you’ d benefit from
investing more work into getting it right. Mistakes cost you money in terms of the
fees OpenAI charges based on the length of the prompt and response, as well as
the time spent fixing mistakes. If you were building a product name generator with
thousands of users, there are some obvious issues you’ d want attempt to fix:
Vague direction
Y ou’re not briefing the AI on what style of name you want, or what attributes it
should have. Do you want a single word or a concatenation? Can the words be
made up, or is it important that they’re in real English? Do you want the AI to
emulate somebody you admire who is famous for great product names?
Unformatted output
Y ou’re getting back a list of separated names line by line, of unspecified length.
When you run this prompt multiple times, you’ll see sometimes it comes back
with a numbered list, and often it has text at the beginning, which makes it hard
to parse programmatically.
Missing examples
Y ou haven’t given the AI any examples of what good names look like. It’s auto‐
completing using an average of its training data, i.e., the entire internet (with all
its inherent bias), but is that what you want? Ideally you’ d feed it examples of
successful names, common names in an industry, or even just other names you
like.
Limited evaluation
Y ou have no consistent or scalable way to define which names are good or bad, so
you have to manually review each response. If you can institute a rating system
or other form of measurement, you can optimize the prompt to get better results
and identify how many times it fails.
No task division
Y ou’re asking a lot of a single prompt here: there are lots of factors that go into
product naming, and this important task is being naively outsourced to the AI all
in one go, with no task specialization or visibility into how it’s handling this task
for you.
2 | Chapter 1: The Five Principles of PromptingAddressing these problems is the basis for the core principles we use throughout
this book. There are many different ways to ask an AI model to do the same task,
and even slight changes can make a big difference. LLMs work by continuously
predicting the next token (approximately three-fourths of a word), starting from what
was in your prompt. Each new token is selected based on its probability of appearing
next, with an element of randomness (controlled by the temperature parameter). As
demonstrated in Figure 1-1, the word shoes had a lower probability of coming after
the start of the name AnyFit (0.88%), where a more predictable response would be
Athletic (72.35%).
Figure 1-1. How the response breaks down into tokens
LLMs are trained on essentially the entire text of the internet, and are then further
fine-tuned to give helpful responses. Average prompts will return average responses,
leading some to be underwhelmed when their results don’t live up to the hype.
What you put in your prompt changes the probability of every word generated, so
it matters a great deal to the results you’ll get. These models have seen the best and
worst of what humans have produced and are capable of emulating almost anything
if you know the right way to ask. OpenAI charges based on the number of tokens
The Five Principles of Prompting | 3used in the prompt and the response, so prompt engineers need to make these tokens
count by optimizing prompts for cost, quality, and reliability.
Here’s the same example with the application of several prompt engineering tech‐
niques. We ask for names in the style of Steve Jobs, state that we want a comma-
separated list, and supply examples of the task done well.
Input:
Brainstorm a list of product names for a shoe that fits any
foot size, in the style of Steve Jobs.
Return the results as a comma-separated list, in this format:
Product description: A shoe that fits any foot size
Product names: [list of 3 product names]
## Examples
Product description: A refrigerator that dispenses beer
Product names: iBarFridge, iFridgeBeer, iDrinkBeerFridge
Product description: A watch that can tell accurate time in
space
Product names: iNaut, iSpace, iTime
Product description: A home milkshake maker
Product names: iShake, iSmoothie, iShake Mini
Output:
Product description: A shoe that fits any foot size
Product names: iFitFoot, iPerfectFit, iShoeSize
While no prompt is ever perfect, this prompt is optimized to reliably deliver solid
product names in the right format. The user of your product name generator can
choose somebody other than Steve Jobs to get the types of names they like, they can
change the response format if needed, and the output of this prompt can become the
input of another. Finally, you could periodically update the examples you use in the
prompt based on user feedback, making your system smarter over time.
Overview of the Five Principles of Prompting
The process for optimizing this prompt follows the Five Principles of Prompting ,
which we will dissect using this example in the remainder of this chapter, and recall
throughout the book. They map exactly to the five issues we raised when discussing
the naive text prompt. Y ou’ll find references back to these principles throughout the
rest of the book to help you connect the dots to how they’re used in practice. The Five
Principles of Prompting are as follows:
4 | Chapter 1: The Five Principles of PromptingGive Direction
Describe the desired style in detail, or reference a relevant persona
Specify Format
Define what rules to follow, and the required structure of the response
Provide Examples
Insert a diverse set of test cases where the task was done correctly
Evaluate Quality
Identify errors and rate responses, testing what drives performance.
Divide Labor
Split tasks into multiple steps, chained together for complex goals
These principles are not short-lived tips or hacks but are generally accepted conven‐
tions that are useful for working with any level of intelligence, biological or artificial.
These principles are model-agnostic and should work to improve your prompt no
matter which generative text or image model you’re using. We first published these
principles in July 2022 in the blog post “Prompt Engineering: From Words to Art
and Copy” , and they have stood the test of time, including mapping quite closely
to OpenAI’s own Prompt Engineering Guide , which came a year later. Anyone who
works closely with generative AI models is likely to converge on a similar set of
strategies for solving common issues, and throughout this book you’ll see hundreds
of demonstrative examples of how they can be useful for improving your prompts.
We have provided downloadable one-pagers for text and image generation you can
use as a checklist when applying these principles. These were created for our popu‐
lar Udemy course The Complete Prompt Engineering for AI Bootcamp  (70,000+
students), which was based on the same principles but with different material to this
book.
• Text Generation One-Pager•
• Image Generation One-Pager•
To show these principles apply equally well to prompting image models, let’s use
the following example, and explain how to apply each of the Five Principles of
Prompting to this specific scenario. Copy and paste the entire input prompt into the
Midjourney Bot in Discord, including the link to the image at the beginning, after
typing /imagine to trigger the prompt box to appear (requires a free Discord account,
and a paid Midjourney account).
Overview of the Five Principles of Prompting | 5Input:
https://s.mj.run/TKAsyhNiKmc stock photo of business meeting
of 4 people watching on white MacBook on top of glass-top
table, Panasonic, DC-GH5
Figure 1-2 shows the output.
Figure 1-2. Stock photo of business meeting
This prompt takes advantage of Midjourney’s ability to take a base image as an
example by uploading the image to Discord and then copy and pasting the URL into
the prompt ( https://s.mj.run/TKAsyhNiKmc), for which the royalty-free image from
Unsplash is used (Figure 1-3). If you run into an error with the prompt, try uploading
the image yourself and reviewing Midjourney’s documentation for any formatting
changes.
6 | Chapter 1: The Five Principles of PromptingFigure 1-3. Photo by Mimi Thian on Unsplash
Let’s compare this well-engineered prompt to what you get back from Midjourney if
you naively ask for a stock photo in the simplest way possible. Figure 1-4 shows an
example of what you get without prompt engineering, an image with a darker, more
stylistic take on a stock photo than you’ d typically expect.
Input:
people in a business meeting
Figure 1-4 shows the output.
Although less prominent an issue in v5 of Midjourney onwards, community feedback
mechanisms (when users select an image to resize to a higher resolution, that choice
may be used to train the model) have reportedly biased the model toward a fantasy
aesthetic, which is less suitable for the stock photo use case. The early adopters of
Midjourney came from the digital art world and naturally gravitated toward fantasy
and sci-fi styles, which can be reflected in the results from the model even when this
aesthetic is not suitable.
Overview of the Five Principles of Prompting | 7Figure 1-4. People in a business meeting
Throughout this book the examples used will be compatiable with ChatGPT Plus
(GPT-4) as the text model and Midjourney v6 or Stable Diffusion XL as the image
model, though we will specify if it’s important. These foundational models are the
current state of the art and are good at a diverse range of tasks. The principles are
intended to be future-proof as much as is possible, so if you’re reading this book
when GPT-5, Midjourney v7, or Stable Diffusion XXL is out, or if you’re using
another vendor like Google, everything you learn here should still prove useful.
1. Give Direction
One of the issues with the naive text prompt discussed earlier was that it wasn’t
briefing the AI on what types of product names you wanted. To some extent, naming
a product is a subjective endeavor, and without giving the AI an idea of what names
you like, it has a low probability of guessing right.
8 | Chapter 1: The Five Principles of PromptingBy the way, a human would also struggle to complete this task without a good brief,
which is why creative and branding agencies require a detailed briefing on any task
from their clients.
Although it’s not a perfect mapping, it can be helpful to imagine
what context a human might need for this task and try including it
in the prompt.
In the example prompt we gave direction through the use of role-playing, in that case
emulating the style of Steve Jobs, who was famous for iconically naming products. If
you change this aspect of the prompt to someone else who is famous in the training
data (as well as matching the examples to the right style), you’ll get dramatically
different results.
Input:
Brainstorm a list of product names for a shoe that fits any
foot size, in the style of Elon Musk.
Return the results as a comma-separated list, in this format:
Product description: A shoe that fits any foot size
Product names: [list of 3 product names]
## Examples
Product description: A refrigerator that dispenses beer
Product names: Teslacool, FridgeX, Neuradrink
Product description: A watch that can tell accurate time in
space
Product names: WatchX, iSpace, The Watch Company
Product description: A home milkshake maker
Product names: ShakeX, The Milkshake Company, HyperShake
Output:
ShoeX, MarsFit, The Shoe Company
There are also some rules or best practices you would do well to follow, which could
be included in the prompt as context to guide the AI toward a name that works.
This technique is sometimes referred to as prewarming or internal retrieval, and it
is simple but effective ( Liu et al., 2021 ). Starting the conversation asking for best
practice advice, then asking it to follow its own advice, can help a lot. In effect, you
are using it to generate its own direction.
1. Give Direction | 9Input:
Please give me 5 tips for naming products based on expert
industry advice.
Output:
Certainly! Naming a product is crucial as it often gives the
first impression to potential customers. Here are five expert
tips to guide you in naming products:
1. **Simplicity & Memorability**: Opt for names that are
short, easy to spell, and memorable. Long, complicated names
can be difficult for customers to recall. Remember, the
easier the name is to remember, the more likely it is that
consumers will think of your product when making a purchase
decision.
...
5. **Positive Connotations**: Names that evoke positive
emotions or imagery can create a favorable perception of
your product. Avoid names that have negative or
controversial associations in any of the primary languages
or cultures where you intend to do business. It's often
worthwhile to conduct linguistic checks and market testing
to ensure there aren't unintended meanings.
Lastly, once you've shortlisted a few names, it's a good
idea to run them by diverse focus groups or conduct surveys
to gauge reactions. Also, ensure that the name you choose is
available for trademarking and doesn't infringe on existing
trademarks.
Then within the same chat window, where the model has the context of the past
advice it gave, you ask your initial prompt for the task you wanted to complete.
Input:
Using this advice, can I have a list of product names for a
pair of shoes that can fit any foot size?
Output:
Certainly! Based on the criteria given:
1. **Simplicity & Memorability**
2. **Relevance & Descriptiveness**
3. **Differentiation & Uniqueness**
4. **Adaptable & Scalable**
5. **Positive Connotations**
Here's a list of potential product names for shoes that can
fit any foot size:
10 | Chapter 1: The Five Principles of Prompting1. **FitAll Walkers**: A simple and memorable name that
instantly suggests a universal fit.
...
10. **TrueFit Universals**: Emphasizing genuine comfort and
fit for all.
Before finalizing any name, remember to check for trademarks
to ensure there's no conflict with existing products or
brands.
Another fruitful strategy in our experience is to take the best advice out there for the
task you want to accomplish and insert that context into the prompt. For example,
you could take Brandwatch’s 5 Golden Rules for naming a product or another trusted
external resource you find, and insert that as context into the prompt. This will
increase the length of the prompt significantly, which costs more money (when using
the API as a developer), but may be worth the trade-off if the quality of the response
improves.
Input:
Can I have a list of product names for a pair of shoes that
can fit any foot size? Please follow the five rules below:
1. It should be readable and writable.
If your product name is hard to pronounce, people won’t talk about it,
and if they can’t write it down (and spell it correctly!)
when they hear it, how do you expect them to Google it?
Keep it simple and don’t go with any wacky spellings just for the sake of it.
...
5. It should evoke an emotion, feeling, or idea.
Your product name should tie back into what your product is, what the
feeling you want people to have when experiencing your product is, and/or
what idea are you trying to get across.
It should be emotive and inspiring.
Output:
1. FlexFit Footwear
2. OmniSize Sneakers
3. AdaptStride Shoes
...
18. OmniComfort Kicks
19. FlexSize Footwear
20. Boundless Soles
1. Give Direction | 11There are other myriad ways of providing direction. In the image generation exam‐
ple, direction was given by specifying that the business meeting is taking place
around a glass-top table. If you change only that detail, you can get a completely
different image, as detailed in Figure 1-5.
Input:
https://s.mj.run/TKAsyhNiKmc stock photo of business meeting
of four people gathered around a campfire outdoors in the
woods, Panasonic, DC-GH5
Figure 1-5 shows the output.
Figure 1-5. Stock photo of business meeting in the woods
Role-playing is also important for image generation, and one of the quite powerful
ways you can give Midjourney direction is to supply the name of an artist or art style
to emulate. One artist that features heavily in the AI art world is Van Gogh, known
for his bold, dramatic brush strokes and vivid use of colors. Watch what happens
when you include his name in the prompt, as shown in Figure 1-6.
12 | Chapter 1: The Five Principles of PromptingInput:
people in a business meeting, by Van Gogh
Figure 1-6 shows the output.
Figure 1-6. People in a business meeting, by Van Gogh
To get that last prompt to work, you need to strip back a lot of the other direction.
For example, losing the base image and the words stock photo as well as the camera
Panasonic, DC-GH5 helps bring in Van Gogh’s style. The problem you may run into
is that often with too much direction, the model can quickly get to a conflicting
combination that it can’t resolve. If your prompt is overly specific, there might not be
enough samples in the training data to generate an image that’s consistent with all of
your criteria. In cases like these, you should choose which element is more important
(in this case, Van Gogh) and defer to that.
Direction is one of the most commonly used and broadest principles. It can take the
form of simply using the right descriptive words to clarify your intent, or channeling
the personas of relevant business celebrities. While too much direction can narrow
the creativity of the model, too little direction is the more common problem.
1. Give Direction | 132. Specify Format
AI models are universal translators. Not only does that mean translating from French
to English, or Urdu to Klingon, but also between data structures like JSON to YAML,
or natural language to Python code. These models are capable of returning a response
in almost any format, so an important part of prompt engineering is finding ways to
specify what format you want the response to be in.
Every now and again you’ll find that the same prompt will return a different format,
for example, a numbered list instead of comma separated. This isn’t a big deal most of
the time, because most prompts are one-offs and typed into ChatGPT or Midjourney.
However, when you’re incorporating AI tools into production software, occasional
flips in format can cause all kinds of errors.
Just like when working with a human, you can avoid wasted effort by specifying up
front the format you expect the response to be in. For text generation models, it can
often be helpful to output JSON instead of a simple ordered list because that’s the
universal format for API responses, which can make it simpler to parse and spot
errors, as well as to use to render the front-end HTML of an application. YAML is
also another popular choice because it enforces a parseable structure while still being
simple and human-readable.
In the original prompt you gave direction through both the examples provided, and
the colon at the end of the prompt indicated it should complete the list inline. To
swap the format to JSON, you need to update both and leave the JSON uncompleted,
so GPT-4 knows to complete it.
Input:
Return a comma-separated list of product names in JSON for
"A pair of shoes that can fit any foot size.".
Return only JSON.
Examples:
[{
"Product description": "A home milkshake maker.",
"Product names": ["HomeShaker", "Fit Shaker",
"QuickShake", "Shake Maker"]
},
{
"Product description": "A watch that can tell
accurate time in space.",
"Product names": ["AstroTime", "SpaceGuard",
"Orbit-Accurate", "EliptoTime"]}
]
14 | Chapter 1: The Five Principles of PromptingOutput:
[
{
"Product description": "A pair of shoes that can \
fit any foot size.",
"Product names": ["FlexFit Footwear", "OneSize Step",
"Adapt-a-Shoe", "Universal Walker"]
}
]
The output we get back is the completed JSON containing the product names. This
can then be parsed and used programmatically, in an application or local script.
It’s also easy from this point to check if there’s an error in the formatting using a
JSON parser like Python’s standard json library, because broken JSON will result in
a parsing error, which can act as a trigger to retry the prompt or investigate before
continuing. If you’re still not getting the right format back, it can help to specify at the
beginning or end of the prompt, or in the system message if using a chat model: You
are a helpful assistant that only responds in JSON , or specify JSON output
in the model parameters where available (this is called grammars with Llama models.
To get up to speed on JSON if you’re unfamiliar, W3Schools has a
good introduction.
For image generation models, format is very important, because the opportunities for
modifying an image are near endless. They range from obvious formats like stock
photo, illustration, and oil painting, to more unusual formats like dashcam
footage, ice sculpture, or in Minecraft (see Figure 1-7).
Input:
business meeting of four people watching on MacBook on top of
table, in Minecraft
Figure 1-7 shows the output.
2. Specify Format | 15Figure 1-7. Business meeting in Minecraft
When setting a format, it is often necessary to remove other aspects of the prompt
that might clash with the specified format. For example, if you supply a base image
of a stock photo, the result is some combination of stock photo and the format you
wanted. To some degree, image generation models can generalize to new scenarios
and combinations they haven’t seen before in their training set, but in our experience,
the more layers of unrelated elements, the more likely you are to get an unsuitable
image.
There is often some overlap between the first and second principles, Give Direction
and Specify Format. The latter is about defining what type of output you want, for
example JSON format, or the format of a stock photo. The former is about the style of
response you want, independent from the format, for example product names in the
style of Steve Jobs, or an image of a business meeting in the style of Van Gogh. When
there are clashes between style and format, it’s often best to resolve them by dropping
whichever element is less important to your final result.
16 | Chapter 1: The Five Principles of Prompting3. Provide Examples
The original prompt didn’t give the AI any examples of what you think good names
look like. Therefore, the response is approximate to an average of the internet, and
you can do better than that. Researchers would call a prompt with no examples
zero-shot, and it’s always a pleasant surprise when AI can even do a task zero shot:
it’s a sign of a powerful model. If you’re providing zero examples, you’re asking for
a lot without giving much in return. Even providing one example ( one-shot) helps
considerably, and it’s the norm among researchers to test how models perform with
multiple examples (few-shot). One such piece of research is the famous GPT-3 paper
“Language Models are Few-Shot Learners” , the results of which are illustrated in
Figure 1-8, showing adding one example along with a prompt can improve accuracy
in some tasks from 10% to near 50%!
Figure 1-8. Number of examples in context
When briefing a colleague or training a junior employee on a new task, it’s only
natural that you’ d include examples of times that task had previously been done well.
Working with AI is the same, and the strength of a prompt often comes down to the
examples used. Providing examples can sometimes be easier than trying to explain
exactly what it is about those examples you like, so this technique is most effective
when you are not a domain expert in the subject area of the task you are attempting
to complete. The amount of text you can fit in a prompt is limited (at the time of
writing around 6,000 characters on Midjourney and approximately 32,000 characters
for the free version of ChatGPT), so a lot of the work of prompt engineering involves
selecting and inserting diverse and instructive examples.
3. Provide Examples | 17There’s a trade-off between reliability and creativity: go past three to five examples
and your results will become more reliable, while sacrificing creativity. The more
examples you provide, and the lesser the diversity between them, the more con‐
strained the response will be to match your examples. If you change all of the
examples to animal names in the previous prompt, you’ll have a strong effect on the
response, which will reliably return only names including animals.
Input:
Brainstorm a list of product names for a shoe that fits any
foot size.
Return the results as a comma-separated list, in this format:
Product description: A shoe that fits any foot size
Product names: [list of 3 product names]
## Examples:
Product description: A home milkshake maker.
Product names: Fast Panda, Healthy Bear, Compact Koala
Product description: A watch that can tell accurate time in
space.
Product names: AstroLamb, Space Bear, Eagle Orbit
Product description: A refrigerator that dispenses beer
Product names: BearFridge, Cool Cat, PenguinBox
Output:
Product description: A shoe that fits any foot size
Product names: FlexiFox, ChameleonStep, PandaPaws
Of course this runs the risk of missing out on returning a much better name that
doesn’t fit the limited space left for the AI to play in. Lack of diversity and variation
in examples is also a problem in handling edge cases, or uncommon scenarios.
Including one to three examples is easy and almost always has a positive effect, but
above that number it becomes essential to experiment with the number of examples
you include, as well as the similarity between them. There is some evidence ( Hsieh
et al., 2023 ) that direction works better than providing examples, and it typically
isn’t straightforward to collect good examples, so it’s usually prudent to attempt the
principle of Give Direction first.
In the image generation space, providing examples usually comes in the form of
providing a base image in the prompt, called img2img in the open source Stable
Diffusion community. Depending on the image generation model being used, these
images can be used as a starting point for the model to generate from, which greatly
affects the results. Y ou can keep everything about the prompt the same but swap out
the provided base image for a radically different effect, as in Figure 1-9.
18 | Chapter 1: The Five Principles of PromptingInput:
stock photo of business meeting of 4 people watching on
white MacBook on top of glass-top table, Panasonic, DC-GH5
Figure 1-9 shows the output.
Figure 1-9. Stock photo of business meeting of four people
In this case, by substituting for the image shown in Figure 1-10, also from Unsplash,
you can see how the model was pulled in a different direction and incorporates
whiteboards and sticky notes now.
These examples demonstrate the capabilities of image generation
models, but we would exercise caution when uploading base
images for use in prompts. Check the licensing of the image you
plan to upload and use in your prompt as the base image, and
avoid using clearly copyrighted images. Doing so can land you in
legal trouble and is against the terms of service for all the major
image generation model providers.
3. Provide Examples | 19Figure 1-10. Photo by Jason Goodman on Unsplash
4. Evaluate Quality
As of yet, there has been no feedback loop to judge the quality of your responses,
other than the basic trial and error of running the prompt and seeing the results,
referred to as blind prompting. This is fine when your prompts are used temporarily
for a single task and rarely revisited. However, when you’re reusing the same prompt
multiple times or building a production application that relies on a prompt, you need
to be more rigorous with measuring results.
There are a number of ways performance can be evaluated, and it depends largely
on what tasks you’re hoping to accomplish. When a new AI model is released, the
focus tends to be on how well the model did on evals (evaluations), a standardized set
of questions with predefined answers or grading criteria that are used to test perfor‐
mance across models. Different models perform differently across different types of
tasks, and there is no guarantee a prompt that worked previously will translate well to
a new model. OpenAI has made its evals framework  for benchmarking performance
of LLMs open source and encourages others to contribute additional eval templates.
In addition to the standard academic evals, there are also more headline-worthy tests
like GPT-4 passing the bar exam. Evaluation is difficult for more subjective tasks, and
can be time-consuming or prohibitively costly for smaller teams. In some instances
researchers have turned to using more advanced models like GPT-4 to evaluate
responses from less sophisticated models, as was done with the release of Vicuna-13B,
a fine-tuned model based on Meta’s Llama open source model (see Figure 1-11).
20 | Chapter 1: The Five Principles of PromptingFigure 1-11. Vicuna GPT-4 Evals
More rigorous evaluation techniques are necessary when writing scientific papers or
grading a new foundation model release, but often you will only need to go just one
step above basic trial and error. Y ou may find that a simple thumbs-up/thumbs-down
rating system implemented in a Jupyter Notebook can be enough to add some rigor
to prompt optimization, without adding too much overhead. One common test is
to see whether providing examples is worth the additional cost in terms of prompt
length, or whether you can get away with providing no examples in the prompt. The
first step is getting responses for multiple runs of each prompt and storing them in a
spreadsheet, which we will do after setting up our environment.
Y ou can install the OpenAI Python package with pip install openai . If you’re
running into compatability issues with this package, create a virtual environment and
install our requirements.txt (instructions in the preface).
To utilize the API, you’ll need to create an OpenAI account  and then navigate here
for your API key.
Hardcoding API keys in scripts is not recommended due to secu‐
rity reasons. Instead, utilize environment variables or configuration
files to manage your keys.
Once you have an API key, it’s crucial to assign it as an environment variable by
executing the following command, replacing api_key with your actual API key value:
export OPENAI_API_KEY="api_key"
Or on Windows:
set OPENAI_API_KEY=api_key
4. Evaluate Quality | 21Alternatively, if you’ d prefer not to preset an API key, then you can manually set the
key while initializing the model, or load it from an .env file using python-dotenv. First,
install the library with pip install python-dotenv, and then load the environment
variables with the following code at the top of your script or notebook:
from dotenv import load_dotenv
load_dotenv()  # take environment variables from .env.
The first step is getting responses for multiple runs of each prompt and storing them
in a spreadsheet.
Input:
# Define two variants of the prompt to test zero-shot
# vs few-shot
prompt_A = """Product description: A pair of shoes that can
fit any foot size.
Seed words: adaptable, fit, omni-fit.
Product names:"""
prompt_B = """Product description: A home milkshake maker.
Seed words: fast, healthy, compact.
Product names: HomeShaker, Fit Shaker, QuickShake, Shake
Maker
Product description: A watch that can tell accurate time in
space.
Seed words: astronaut, space-hardened, eliptical orbit
Product names: Astro