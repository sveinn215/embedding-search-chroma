James Phoenix  
& Mike Taylor
Prompt 
Engineering for 
Generative AI
Future-Proof Inputs for Reliable AI Outputs
Free 
ChapterThis excerpt contains Chapter 1. The complete book is
available on the O’Reilly Online Learning Platform and
through other retailers.
James Phoenix and Mike Taylor
Prompt Engineering for
Generative AI
Future-Proof Inputs for Reliable
AI Outputs at Scale
Boston Farnham Sebastopol TokyoBeijing Boston Farnham Sebastopol TokyoBeijing978-1-098-15343-4
[LSI]
Prompt Engineering for Generative AI
by James Phoenix and Mike Taylor
Copyright © 2024 Saxifrage, LLC and Just Understanding Data LTD. All rights reserved.
Printed in the United States of America.
Published by O’Reilly Media, Inc., 1005 Gravenstein Highway North, Sebastopol, CA 95472.
O’Reilly books may be purchased for educational, business, or sales promotional use. Online editions are
also available for most titles (http://oreilly.com). For more information, contact our corporate/institutional
sales department: 800-998-9938 or corporate@oreilly.com.
Acquisitions Editor: Nicole Butterfield
Development Editor: Corbin Collins
Copyeditor: Piper Editorial Consulting, LLC
Proofreader: Kim Wimpsett
Indexer: nSight, Inc.
Interior Designer: David Futato
Cover Designer: Karen Montgomery
Illustrator: Kate Dullea
May 2024:  First Edition
Revision History for the First Edition
2024-05-15: First Release
See http://oreilly.com/catalog/errata.csp?isbn=9781098153434 for release details.
The O’Reilly logo is a registered trademark of O’Reilly Media, Inc. Prompt Engineering for Generative AI,
the cover image, and related trade dress are trademarks of O’Reilly Media, Inc.
The views expressed in this work are those of the authors and do not represent the publisher’s views.
While the publisher and the authors have used good faith efforts to ensure that the information and
instructions contained in this work are accurate, the publisher and the authors disclaim all responsibility
for errors or omissions, including without limitation responsibility for damages resulting from the use
of or reliance on this work. Use of the information and instructions contained in this work is at your
own risk. If any code samples or other technology this work contains or describes is subject to open
source licenses or the intellectual property rights of others, it is your responsibility to ensure that your use
thereof complies with such licenses and/or rights.Table of Contents
1. The Five Principles of Prompting. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  1
Overview of the Five Principles of Prompting                                                              4
1. Give Direction                                                                                                                8
2. Specify Format                                                                                                              14
3. Provide Examples                                                                                                        17
4. Evaluate Quality                                                                                                           20
5. Divide Labor                                                                                                                 32
Summary                                                                                                                           38
iiiCHAPTER 1
The Five Principles of Prompting
Prompt engineering is the process of discovering prompts that reliably yield useful or
desired results.
A prompt is the input you provide, typically text, when interfacing with an AI model
like ChatGPT or Midjourney. The prompt serves as a set of instructions the model
uses to predict the desired response: text from large language models  (LLMs) like
ChatGPT, or images from diffusion models like Midjourney.
Here is a simple example of a prompt input for a product name generator (inspired
by one of OpenAI’s examples), and the resulting output from ChatGPT.
Input:
Can I have a list of product names for a pair of shoes that
can fit any foot size?
Output:
Certainly! Here's a list of product names for a pair of shoes
that can fit any foot size:
UniFit SoleStrides
FlexiSize All-Fit Shoes
OmniFit FootFlex
...
AnyStep AdaptiWalkers
FlexiRange UniversalSteps
PerfectGlide MultiFit Shoes
1This is already a remarkable response for a naive prompt, which feels like magic
because we got here with very little effort. As the state-of-the-art models improve,
the likelihood you will get good enough  results on your first try goes up. For any
throwaway interactions with an AI, where you don’t plan to do the same task again,
the naive approach is all you need.
However, if you planned to put this prompt into production, you’ d benefit from
investing more work into getting it right. Mistakes cost you money in terms of the
fees OpenAI charges based on the length of the prompt and response, as well as
the time spent fixing mistakes. If you were building a product name generator with
thousands of users, there are some obvious issues you’ d want attempt to fix:
Vague direction
Y ou’re not briefing the AI on what style of name you want, or what attributes it
should have. Do you want a single word or a concatenation? Can the words be
made up, or is it important that they’re in real English? Do you want the AI to
emulate somebody you admire who is famous for great product names?
Unformatted output
Y ou’re getting back a list of separated names line by line, of unspecified length.
When you run this prompt multiple times, you’ll see sometimes it comes back
with a numbered list, and often it has text at the beginning, which makes it hard
to parse programmatically.
Missing examples
Y ou haven’t given the AI any examples of what good names look like. It’s auto‐
completing using an average of its training data, i.e., the entire internet (with all
its inherent bias), but is that what you want? Ideally you’ d feed it examples of
successful names, common names in an industry, or even just other names you
like.
Limited evaluation
Y ou have no consistent or scalable way to define which names are good or bad, so
you have to manually review each response. If you can institute a rating system
or other form of measurement, you can optimize the prompt to get better results
and identify how many times it fails.
No task division
Y ou’re asking a lot of a single prompt here: there are lots of factors that go into
product naming, and this important task is being naively outsourced to the AI all
in one go, with no task specialization or visibility into how it’s handling this task
for you.
2 | Chapter 1: The Five Principles of PromptingAddressing these problems is the basis for the core principles we use throughout
this book. There are many different ways to ask an AI model to do the same task,
and even slight changes can make a big difference. LLMs work by continuously
predicting the next token (approximately three-fourths of a word), starting from what
was in your prompt. Each new token is selected based on its probability of appearing
next, with an element of randomness (controlled by the temperature parameter). As
demonstrated in Figure 1-1, the word shoes had a lower probability of coming after
the start of the name AnyFit (0.88%), where a more predictable response would be
Athletic (72.35%).
Figure 1-1. How the response breaks down into tokens
LLMs are trained on essentially the entire text of the internet, and are then further
fine-tuned to give helpful responses. Average prompts will return average responses,
leading some to be underwhelmed when their results don’t live up to the hype.
What you put in your prompt changes the probability of every word generated, so
it matters a great deal to the results you’ll get. These models have seen the best and
worst of what humans have produced and are capable of emulating almost anything
if you know the right way to ask. OpenAI charges based on the number of tokens
The Five Principles of Prompting | 3used in the prompt and the response, so prompt engineers need to make these tokens
count by optimizing prompts for cost, quality, and reliability.
Here’s the same example with the application of several prompt engineering tech‐
niques. We ask for names in the style of Steve Jobs, state that we want a comma-
separated list, and supply examples of the task done well.
Input:
Brainstorm a list of product names for a shoe that fits any
foot size, in the style of Steve Jobs.
Return the results as a comma-separated list, in this format:
Product description: A shoe that fits any foot size
Product names: [list of 3 product names]
## Examples
Product description: A refrigerator that dispenses beer
Product names: iBarFridge, iFridgeBeer, iDrinkBeerFridge
Product description: A watch that can tell accurate time in
space
Product names: iNaut, iSpace, iTime
Product description: A home milkshake maker
Product names: iShake, iSmoothie, iShake Mini
Output:
Product description: A shoe that fits any foot size
Product names: iFitFoot, iPerfectFit, iShoeSize
While no prompt is ever perfect, this prompt is optimized to reliably deliver solid
product names in the right format. The user of your product name generator can
choose somebody other than Steve Jobs to get the types of names they like, they can
change the response format if needed, and the output of this prompt can become the
input of another. Finally, you could periodically update the examples you use in the
prompt based on user feedback, making your system smarter over time.
Overview of the Five Principles of Prompting
The process for optimizing this prompt follows the Five Principles of Prompting ,
which we will dissect using this example in the remainder of this chapter, and recall
throughout the book. They map exactly to the five issues we raised when discussing
the naive text prompt. Y ou’ll find references back to these principles throughout the
rest of the book to help you connect the dots to how they’re used in practice. The Five
Principles of Prompting are as follows:
4 | Chapter 1: The Five Principles of PromptingGive Direction
Describe the desired style in detail, or reference a relevant persona
Specify Format
Define what rules to follow, and the required structure of the response
Provide Examples
Insert a diverse set of test cases where the task was done correctly
Evaluate Quality
Identify errors and rate responses, testing what drives performance.
Divide Labor
Split tasks into multiple steps, chained together for complex goals
These principles are not short-lived tips or hacks but are generally accepted conven‐
tions that are useful for working with any level of intelligence, biological or artificial.
These principles are model-agnostic and should work to improve your prompt no
matter which generative text or image model you’re using. We first published these
principles in July 2022 in the blog post “Prompt Engineering: From Words to Art
and Copy” , and they have stood the test of time, including mapping quite closely
to OpenAI’s own Prompt Engineering Guide , which came a year later. Anyone who
works closely with generative AI models is likely to converge on a similar set of
strategies for solving common issues, and throughout this book you’ll see hundreds
of demonstrative examples of how they can be useful for improving your prompts.
We have provided downloadable one-pagers for text and image generation you can
use as a checklist when applying these principles. These were created for our popu‐
lar Udemy course The Complete Prompt Engineering for AI Bootcamp  (70,000+
students), which was based on the same principles but with different material to this
book.
• Text Generation One-Pager•
• Image Generation One-Pager•
To show these principles apply equally well to prompting image models, let’s use
the following example, and explain how to apply each of the Five Principles of
Prompting to this specific scenario. Copy and paste the entire input prompt into the
Midjourney Bot in Discord, including the link to the image at the beginning, after
typing /imagine to trigger the prompt box to appear (requires a free Discord account,
and a paid Midjourney account).
Overview of the Five Principles of Prompting | 5Input:
https://s.mj.run/TKAsyhNiKmc stock photo of business meeting
of 4 people watching on white MacBook on top of glass-top
table, Panasonic, DC-GH5
Figure 1-2 shows the output.
Figure 1-2. Stock photo of business meeting
This prompt takes advantage of Midjourney’s ability to take a base image as an
example by uploading the image to Discord and then copy and pasting the URL into
the prompt ( https://s.mj.run/TKAsyhNiKmc), for which the royalty-free image from
Unsplash is used (Figure 1-3). If you run into an error with the prompt, try uploading
the image yourself and reviewing Midjourney’s documentation for any formatting
changes.
6 | Chapter 1: The Five Principles of PromptingFigure 1-3. Photo by Mimi Thian on Unsplash
Let’s compare this well-engineered prompt to what you get back from Midjourney if
you naively ask for a stock photo in the simplest way possible. Figure 1-4 shows an
example of what you get without prompt engineering, an image with a darker, more
stylistic take on a stock photo than you’ d typically expect.
Input:
people in a business meeting
Figure 1-4 shows the output.
Although less prominent an issue in v5 of Midjourney onwards, community feedback
mechanisms (when users select an image to resize to a higher resolution, that choice
may be used to train the model) have reportedly biased the model toward a fantasy
aesthetic, which is less suitable for the stock photo use case. The early adopters of
Midjourney came from the digital art world and naturally gravitated toward fantasy
and sci-fi styles, which can be reflected in the results from the model even when this
aesthetic is not suitable.
Overview of the Five Principles of Prompting | 7Figure 1-4. People in a business meeting
Throughout this book the examples used will be compatiable with ChatGPT Plus
(GPT-4) as the text model and Midjourney v6 or Stable Diffusion XL as the image
model, though we will specify if it’s important. These foundational models are the
current state of the art and are good at a diverse range of tasks. The principles are
intended to be future-proof as much as is possible, so if you’re reading this book
when GPT-5, Midjourney v7, or Stable Diffusion XXL is out, or if you’re using
another vendor like Google, everything you learn here should still prove useful.
1. Give Direction
One of the issues with the naive text prompt discussed earlier was that it wasn’t
briefing the AI on what types of product names you wanted. To some extent, naming
a product is a subjective endeavor, and without giving the AI an idea of what names
you like, it has a low probability of guessing right.
8 | Chapter 1: The Five Principles of PromptingBy the way, a human would also struggle to complete this task without a good brief,
which is why creative and branding agencies require a detailed briefing on any task
from their clients.
Although it’s not a perfect mapping, it can be helpful to imagine
what context a human might need for this task and try including it
in the prompt.
In the example prompt we gave direction through the use of role-playing, in that case
emulating the style of Steve Jobs, who was famous for iconically naming products. If
you change this aspect of the prompt to someone else who is famous in the training
data (as well as matching the examples to the right style), you’ll get dramatically
different results.
Input:
Brainstorm a list of product names for a shoe that fits any
foot size, in the style of Elon Musk.
Return the results as a comma-separated list, in this format:
Product description: A shoe that fits any foot size
Product names: [list of 3 product names]
## Examples
Product description: A refrigerator that dispenses beer
Product names: Teslacool, FridgeX, Neuradrink
Product description: A watch that can tell accurate time in
space
Product names: WatchX, iSpace, The Watch Company
Product description: A home milkshake maker
Product names: ShakeX, The Milkshake Company, HyperShake
Output:
ShoeX, MarsFit, The Shoe Company
There are also some rules or best practices you would do well to follow, which could
be included in the prompt as context to guide the AI toward a name that works.
This technique is sometimes referred to as prewarming or internal retrieval, and it
is simple but effective ( Liu et al., 2021 ). Starting the conversation asking for best
practice advice, then asking it to follow its own advice, can help a lot. In effect, you
are using it to generate its own direction.
1. Give Direction | 9Input:
Please give me 5 tips for naming products based on expert
industry advice.
Output:
Certainly! Naming a product is crucial as it often gives the
first impression to potential customers. Here are five expert
tips to guide you in naming products:
1. **Simplicity & Memorability**: Opt for names that are
short, easy to spell, and memorable. Long, complicated names
can be difficult for customers to recall. Remember, the
easier the name is to remember, the more likely it is that
consumers will think of your product when making a purchase
decision.
...
5. **Positive Connotations**: Names that evoke positive
emotions or imagery can create a favorable perception of
your product. Avoid names that have negative or
controversial associations in any of the primary languages
or cultures where you intend to do business. It's often
worthwhile to conduct linguistic checks and market testing
to ensure there aren't unintended meanings.
Lastly, once you've shortlisted a few names, it's a good
idea to run them by diverse focus groups or conduct surveys
to gauge reactions. Also, ensure that the name you choose is
available for trademarking and doesn't infringe on existing
trademarks.
Then within the same chat window, where the model has the context of the past
advice it gave, you ask your initial prompt for the task you wanted to complete.
Input:
Using this advice, can I have a list of product names for a
pair of shoes that can fit any foot size?
Output:
Certainly! Based on the criteria given:
1. **Simplicity & Memorability**
2. **Relevance & Descriptiveness**
3. **Differentiation & Uniqueness**
4. **Adaptable & Scalable**
5. **Positive Connotations**
Here's a list of potential product names for shoes that can
fit any foot size:
10 | Chapter 1: The Five Principles of Prompting1. **FitAll Walkers**: A simple and memorable name that
instantly suggests a universal fit.
...
10. **TrueFit Universals**: Emphasizing genuine comfort and
fit for all.
Before finalizing any name, remember to check for trademarks
to ensure there's no conflict with existing products or
brands.
Another fruitful strategy in our experience is to take the best advice out there for the
task you want to accomplish and insert that context into the prompt. For example,
you could take Brandwatch’s 5 Golden Rules for naming a product or another trusted
external resource you find, and insert that as context into the prompt. This will
increase the length of the prompt significantly, which costs more money (when using
the API as a developer), but may be worth the trade-off if the quality of the response
improves.
Input:
Can I have a list of product names for a pair of shoes that
can fit any foot size? Please follow the five rules below:
1. It should be readable and writable.
If your product name is hard to pronounce, people won’t talk about it,
and if they can’t write it down (and spell it correctly!)
when they hear it, how do you expect them to Google it?
Keep it simple and don’t go with any wacky spellings just for the sake of it.
...
5. It should evoke an emotion, feeling, or idea.
Your product name should tie back into what your product is, what the
feeling you want people to have when experiencing your product is, and/or
what idea are you trying to get across.
It should be emotive and inspiring.
Output:
1. FlexFit Footwear
2. OmniSize Sneakers
3. AdaptStride Shoes
...
18. OmniComfort Kicks
19. FlexSize Footwear
20. Boundless Soles
1. Give Direction | 11There are other myriad ways of providing direction. In the image generation exam‐
ple, direction was given by specifying that the business meeting is taking place
around a glass-top table. If you change only that detail, you can get a completely
different image, as detailed in Figure 1-5.
Input:
https://s.mj.run/TKAsyhNiKmc stock photo of business meeting
of four people gathered around a campfire outdoors in the
woods, Panasonic, DC-GH5
Figure 1-5 shows the output.
Figure 1-5. Stock photo of business meeting in the woods
Role-playing is also important for image generation, and one of the quite powerful
ways you can give Midjourney direction is to supply the name of an artist or art style
to emulate. One artist that features heavily in the AI art world is Van Gogh, known
for his bold, dramatic brush strokes and vivid use of colors. Watch what happens
when you include his name in the prompt, as shown in Figure 1-6.
12 | Chapter 1: The Five Principles of PromptingInput:
people in a business meeting, by Van Gogh
Figure 1-6 shows the output.
Figure 1-6. People in a business meeting, by Van Gogh
To get that last prompt to work, you need to strip back a lot of the other direction.
For example, losing the base image and the words stock photo as well as the camera
Panasonic, DC-GH5 helps bring in Van Gogh’s style. The problem you may run into
is that often with too much direction, the model can quickly get to a conflicting
combination that it can’t resolve. If your prompt is overly specific, there might not be
enough samples in the training data to generate an image that’s consistent with all of
your criteria. In cases like these, you should choose which element is more important
(in this case, Van Gogh) and defer to that.
Direction is one of the most commonly used and broadest principles. It can take the
form of simply using the right descriptive words to clarify your intent, or channeling
the personas of relevant business celebrities. While too much direction can narrow
the creativity of the model, too little direction is the more common problem.
1. Give Direction | 132. Specify Format
AI models are universal translators. Not only does that mean translating from French
to English, or Urdu to Klingon, but also between data structures like JSON to YAML,
or natural language to Python code. These models are capable of returning a response
in almost any format, so an important part of prompt engineering is finding ways to
specify what format you want the response to be in.
Every now and again you’ll find that the same prompt will return a different format,
for example, a numbered list instead of comma separated. This isn’t a big deal most of
the time, because most prompts are one-offs and typed into ChatGPT or Midjourney.
However, when you’re incorporating AI tools into production software, occasional
flips in format can cause all kinds of errors.
Just like when working with a human, you can avoid wasted effort by specifying up
front the format you expect the response to be in. For text generation models, it can
often be helpful to output JSON instead of a simple ordered list because that’s the
universal format for API responses, which can make it simpler to parse and spot
errors, as well as to use to render the front-end HTML of an application. YAML is
also another popular choice because it enforces a parseable structure while still being
simple and human-readable.
In the original prompt you gave direction through both the examples provided, and
the colon at the end of the prompt indicated it should complete the list inline. To
swap the format to JSON, you need to update both and leave the JSON uncompleted,
so GPT-4 knows to complete it.
Input:
Return a comma-separated list of product names in JSON for
"A pair of shoes that can fit any foot size.".
Return only JSON.
Examples:
[{
"Product description": "A home milkshake maker.",
"Product names": ["HomeShaker", "Fit Shaker",
"QuickShake", "Shake Maker"]
},
{
"Product description": "A watch that can tell
accurate time in space.",
"Product names": ["AstroTime", "SpaceGuard",
"Orbit-Accurate", "EliptoTime"]}
]
14 | Chapter 1: The Five Principles of PromptingOutput:
[
{
"Product description": "A pair of shoes that can \
fit any foot size.",
"Product names": ["FlexFit Footwear", "OneSize Step",
"Adapt-a-Shoe", "Universal Walker"]
}
]
The output we get back is the completed JSON containing the product names. This
can then be parsed and used programmatically, in an application or local script.
It’s also easy from this point to check if there’s an error in the formatting using a
JSON parser like Python’s standard json library, because broken JSON will result in
a parsing error, which can act as a trigger to retry the prompt or investigate before
continuing. If you’re still not getting the right format back, it can help to specify at the
beginning or end of the prompt, or in the system message if using a chat model: You
are a helpful assistant that only responds in JSON , or specify JSON output
in the model parameters where available (this is called grammars with Llama models.
To get up to speed on JSON if you’re unfamiliar, W3Schools has a
good introduction.
For image generation models, format is very important, because the opportunities for
modifying an image are near endless. They range from obvious formats like stock
photo, illustration, and oil painting, to more unusual formats like dashcam
footage, ice sculpture, or in Minecraft (see Figure 1-7).
Input:
business meeting of four people watching on MacBook on top of
table, in Minecraft
Figure 1-7 shows the output.
2. Specify Format | 15Figure 1-7. Business meeting in Minecraft
When setting a format, it is often necessary to remove other aspects of the prompt
that might clash with the specified format. For example, if you supply a base image
of a stock photo, the result is some combination of stock photo and the format you
wanted. To some degree, image generation models can generalize to new scenarios
and combinations they haven’t seen before in their training set, but in our experience,
the more layers of unrelated elements, the more likely you are to get an unsuitable
image.
There is often some overlap between the first and second principles, Give Direction
and Specify Format. The latter is about defining what type of output you want, for
example JSON format, or the format of a stock photo. The former is about the style of
response you want, independent from the format, for example product names in the
style of Steve Jobs, or an image of a business meeting in the style of Van Gogh. When
there are clashes between style and format, it’s often best to resolve them by dropping
whichever element is less important to your final result.
16 | Chapter 1: The Five Principles of Prompting3. Provide Examples
The original prompt didn’t give the AI any examples of what you think good names
look like. Therefore, the response is approximate to an average of the internet, and
you can do better than that. Researchers would call a prompt with no examples
zero-shot, and it’s always a pleasant surprise when AI can even do a task zero shot:
it’s a sign of a powerful model. If you’re providing zero examples, you’re asking for
a lot without giving much in return. Even providing one example ( one-shot) helps
considerably, and it’s the norm among researchers to test how models perform with
multiple examples (few-shot). One such piece of research is the famous GPT-3 paper
“Language Models are Few-Shot Learners” , the results of which are illustrated in
Figure 1-8, showing adding one example along with a prompt can improve accuracy
in some tasks from 10% to near 50%!
Figure 1-8. Number of examples in context
When briefing a colleague or training a junior employee on a new task, it’s only
natural that you’ d include examples of times that task had previously been done well.
Working with AI is the same, and the strength of a prompt often comes down to the
examples used. Providing examples can sometimes be easier than trying to explain
exactly what it is about those examples you like, so this technique is most effective
when you are not a domain expert in the subject area of the task you are attempting
to complete. The amount of text you can fit in a prompt is limited (at the time of
writing around 6,000 characters on Midjourney and approximately 32,000 characters
for the free version of ChatGPT), so a lot of the work of prompt engineering involves
selecting and inserting diverse and instructive examples.
3. Provide Examples | 17There’s a trade-off between reliability and creativity: go past three to five examples
and your results will become more reliable, while sacrificing creativity. The more
examples you provide, and the lesser the diversity between them, the more con‐
strained the response will be to match your examples. If you change all of the
examples to animal names in the previous prompt, you’ll have a strong effect on the
response, which will reliably return only names including animals.
Input:
Brainstorm a list of product names for a shoe that fits any
foot size.
Return the results as a comma-separated list, in this format:
Product description: A shoe that fits any foot size
Product names: [list of 3 product names]
## Examples:
Product description: A home milkshake maker.
Product names: Fast Panda, Healthy Bear, Compact Koala
Product description: A watch that can tell accurate time in
space.
Product names: AstroLamb, Space Bear, Eagle Orbit
Product description: A refrigerator that dispenses beer
Product names: BearFridge, Cool Cat, PenguinBox
Output:
Product description: A shoe that fits any foot size
Product names: FlexiFox, ChameleonStep, PandaPaws
Of course this runs the risk of missing out on returning a much better name that
doesn’t fit the limited space left for the AI to play in. Lack of diversity and variation
in examples is also a problem in handling edge cases, or uncommon scenarios.
Including one to three examples is easy and almost always has a positive effect, but
above that number it becomes essential to experiment with the number of examples
you include, as well as the similarity between them. There is some evidence ( Hsieh
et al., 2023 ) that direction works better than providing examples, and it typically
isn’t straightforward to collect good examples, so it’s usually prudent to attempt the
principle of Give Direction first.
In the image generation space, providing examples usually comes in the form of
providing a base image in the prompt, called img2img in the open source Stable
Diffusion community. Depending on the image generation model being used, these
images can be used as a starting point for the model to generate from, which greatly
affects the results. Y ou can keep everything about the prompt the same but swap out
the provided base image for a radically different effect, as in Figure 1-9.
18 | Chapter 1: The Five Principles of PromptingInput:
stock photo of business meeting of 4 people watching on
white MacBook on top of glass-top table, Panasonic, DC-GH5
Figure 1-9 shows the output.
Figure 1-9. Stock photo of business meeting of four people
In this case, by substituting for the image shown in Figure 1-10, also from Unsplash,
you can see how the model was pulled in a different direction and incorporates
whiteboards and sticky notes now.
These examples demonstrate the capabilities of image generation
models, but we would exercise caution when uploading base
images for use in prompts. Check the licensing of the image you
plan to upload and use in your prompt as the base image, and
avoid using clearly copyrighted images. Doing so can land you in
legal trouble and is against the terms of service for all the major
image generation model providers.
3. Provide Examples | 19Figure 1-10. Photo by Jason Goodman on Unsplash
4. Evaluate Quality
As of yet, there has been no feedback loop to judge the quality of your responses,
other than the basic trial and error of running the prompt and seeing the results,
referred to as blind prompting. This is fine when your prompts are used temporarily
for a single task and rarely revisited. However, when you’re reusing the same prompt
multiple times or building a production application that relies on a prompt, you need
to be more rigorous with measuring results.
There are a number of ways performance can be evaluated, and it depends largely
on what tasks you’re hoping to accomplish. When a new AI model is released, the
focus tends to be on how well the model did on evals (evaluations), a standardized set
of questions with predefined answers or grading criteria that are used to test perfor‐
mance across models. Different models perform differently across different types of
tasks, and there is no guarantee a prompt that worked previously will translate well to
a new model. OpenAI has made its evals framework  for benchmarking performance
of LLMs open source and encourages others to contribute additional eval templates.
In addition to the standard academic evals, there are also more headline-worthy tests
like GPT-4 passing the bar exam. Evaluation is difficult for more subjective tasks, and
can be time-consuming or prohibitively costly for smaller teams. In some instances
researchers have turned to using more advanced models like GPT-4 to evaluate
responses from less sophisticated models, as was done with the release of Vicuna-13B,
a fine-tuned model based on Meta’s Llama open source model (see Figure 1-11).
20 | Chapter 1: The Five Principles of PromptingFigure 1-11. Vicuna GPT-4 Evals
More rigorous evaluation techniques are necessary when writing scientific papers or
grading a new foundation model release, but often you will only need to go just one
step above basic trial and error. Y ou may find that a simple thumbs-up/thumbs-down
rating system implemented in a Jupyter Notebook can be enough to add some rigor
to prompt optimization, without adding too much overhead. One common test is
to see whether providing examples is worth the additional cost in terms of prompt
length, or whether you can get away with providing no examples in the prompt. The
first step is getting responses for multiple runs of each prompt and storing them in a
spreadsheet, which we will do after setting up our environment.
Y ou can install the OpenAI Python package with pip install openai . If you’re
running into compatability issues with this package, create a virtual environment and
install our requirements.txt (instructions in the preface).
To utilize the API, you’ll need to create an OpenAI account  and then navigate here
for your API key.
Hardcoding API keys in scripts is not recommended due to secu‐
rity reasons. Instead, utilize environment variables or configuration
files to manage your keys.
Once you have an API key, it’s crucial to assign it as an environment variable by
executing the following command, replacing api_key with your actual API key value:
export OPENAI_API_KEY="api_key"
Or on Windows:
set OPENAI_API_KEY=api_key
4. Evaluate Quality | 21Alternatively, if you’ d prefer not to preset an API key, then you can manually set the
key while initializing the model, or load it from an .env file using python-dotenv. First,
install the library with pip install python-dotenv, and then load the environment
variables with the following code at the top of your script or notebook:
from dotenv import load_dotenv
load_dotenv()  # take environment variables from .env.
The first step is getting responses for multiple runs of each prompt and storing them
in a spreadsheet.
Input:
# Define two variants of the prompt to test zero-shot
# vs few-shot
prompt_A = """Product description: A pair of shoes that can
fit any foot size.
Seed words: adaptable, fit, omni-fit.
Product names:"""
prompt_B = """Product description: A home milkshake maker.
Seed words: fast, healthy, compact.
Product names: HomeShaker, Fit Shaker, QuickShake, Shake
Maker
Product description: A watch that can tell accurate time in
space.
Seed words: astronaut, space-hardened, eliptical orbit
Product names: AstroTime, SpaceGuard, Orbit-Accurate,
EliptoTime.
Product description: A pair of shoes that can fit any foot
size.
Seed words: adaptable, fit, omni-fit.
Product names:"""
test_prompts = [prompt_A, prompt_B]
import pandas as pd
from openai import OpenAI
import os
# Set your OpenAI key as an environment variable
# https://platform.openai.com/api-keys
client = OpenAI(
  api_key=os.environ['OPENAI_API_KEY'],  # Default
)
def get_response(prompt):
    response = client.chat.completions.create(
        model="gpt-3.5-turbo",
22 | Chapter 1: The Five Principles of Prompting        messages=[
            {
                "role": "system",
                "content": "You are a helpful assistant."
            },
            {
                "role": "user",
                "content": prompt
            }
        ]
    )
    return response.choices[0].message.content
# Iterate through the prompts and get responses
responses = []
num_tests = 5
for idx, prompt in enumerate(test_prompts):
    # prompt number as a letter
    var_name = chr(ord('A') + idx)
    for i in range(num_tests):
        # Get a response from the model
        response = get_response(prompt)
        data = {
            "variant": var_name,
            "prompt": prompt,
            "response": response
            }
        responses.append(data)
# Convert responses into a dataframe
df = pd.DataFrame(responses)
# Save the dataframe as a CSV file
df.to_csv("responses.csv", index=False)
print(df)
Output:
  variant                                             prompt
  \
0       A  Product description: A pair of shoes that can ...
1       A  Product description: A pair of shoes that can ...
2       A  Product description: A pair of shoes that can ...
3       A  Product description: A pair of shoes that can ...
4       A  Product description: A pair of shoes that can ...
5       B  Product description: A home milkshake maker.\n...
6       B  Product description: A home milkshake maker.\n...
7       B  Product description: A home milkshake maker.\n...
8       B  Product description: A home milkshake maker.\n...
4. Evaluate Quality | 239       B  Product description: A home milkshake maker.\n...
                                            response
0  1. Adapt-a-Fit Shoes \n2. Omni-Fit Footwear \n...
1  1. OmniFit Shoes\n2. Adapt-a-Sneaks \n3. OneFi...
2  1. Adapt-a-fit\n2. Flexi-fit shoes\n3. Omni-fe...
3  1. Adapt-A-Sole\n2. FitFlex\n3. Omni-FitX\n4. ...
4  1. Omni-Fit Shoes\n2. Adapt-a-Fit Shoes\n3. An...
5  Adapt-a-Fit, Perfect Fit Shoes, OmniShoe, OneS...
6       FitAll, OmniFit Shoes, SizeLess, AdaptaShoes
7       AdaptaFit, OmniShoe, PerfectFit, AllSizeFit.
8  FitMaster, AdaptoShoe, OmniFit, AnySize Footwe...
9        Adapt-a-Shoe, PerfectFit, OmniSize, FitForm
Here we’re using the OpenAI API to generate model responses to a set of prompts
and storing the results in a dataframe, which is saved to a CSV file. Here’s how it
works:
1. Two prompt variants are defined, and each variant consists of a product descrip‐1.
tion, seed words, and potential product names, but prompt_B provides two
examples.
2. Import statements are called for the Pandas library, OpenAI library, and os2.
library.
3. The get_response function takes a prompt as input and returns a response from3.
the gpt-3.5-turbo model. The prompt is passed as a user message to the model,
along with a system message to set the model’s behavior.
4. Two prompt variants are stored in the test_prompts list.4.
5. An empty list responses is created to store the generated responses, and the5.
variable num_tests is set to 5.
6. A nested loop is used to generate responses. The outer loop iterates over each6.
prompt, and the inner loop generates num_tests (five in this case) number of
responses per prompt.
a. The enumerate function is used to get the index and value of each prompta.
in test_prompts. This index is then converted to a corresponding uppercase
letter (e.g., 0 becomes A, 1 becomes B) to be used as a variant name.
b. For each iteration, the get_response function is called with the currentb.
prompt to generate a response from the model.
c. A dictionary is created with the variant name, the prompt, and the model’sc.
response, and this dictionary is appended to the responses list.
7. Once all responses have been generated, the responses list (which is now a list of7.
dictionaries) is converted into a Pandas DataFrame.
24 | Chapter 1: The Five Principles of Prompting8. This dataframe is then saved to a CSV file with the Pandas built-in to_csv8.
function, making the file responses.csv with index=False so as to not write row
indices.
9. Finally, the dataframe is printed to the console.9.
Having these responses in a spreadsheet is already useful, because you can see right
away even in the printed response that prompt_A (zero-shot) in the first five rows is
giving us a numbered list, whereas prompt_B (few-shot) in the last five rows tends to
output the desired format of a comma-separated inline list. The next step is to give
a rating on each of the responses, which is best done blind and randomized to avoid
favoring one prompt over another.
Input:
import ipywidgets as widgets
from IPython.display import display
import pandas as pd
# load the responses.csv file
df = pd.read_csv("responses.csv")
# Shuffle the dataframe
df = df.sample(frac=1).reset_index(drop=True)
# df is your dataframe and 'response' is the column with the
# text you want to test
response_index = 0
# add a new column to store feedback
df['feedback'] = pd.Series(dtype='str')
def on_button_clicked(b):
    global response_index
    #  convert thumbs up / down to 1 / 0
    user_feedback = 1 if b.description == "\U0001F44D" else 0
    # update the feedback column
    df.at[response_index, 'feedback'] = user_feedback
    response_index += 1
    if response_index < len(df):
        update_response()
    else:
        # save the feedback to a CSV file
        df.to_csv("results.csv", index=False)
        print("A/B testing completed. Here's the results:")
        # Calculate score and num rows for each variant
        summary_df = df.groupby('variant').agg(
            count=('feedback', 'count'),
            score=('feedback', 'mean')).reset_index()
4. Evaluate Quality | 25        print(summary_df)
def update_response():
    new_response = df.iloc[response_index]['response']
    if pd.notna(new_response):
        new_response = "<p>" + new_response + "</p>"
    else:
        new_response = "<p>No response</p>"
    response.value = new_response
    count_label.value = f"Response: {response_index + 1}"
    count_label.value += f"/{len(df)}"
response = widgets.HTML()
count_label = widgets.Label()
update_response()
thumbs_up_button = widgets.Button(description='\U0001F44D')
thumbs_up_button.on_click(on_button_clicked)
thumbs_down_button = widgets.Button(
    description='\U0001F44E')
thumbs_down_button.on_click(on_button_clicked)
button_box = widgets.HBox([thumbs_down_button,
thumbs_up_button])
display(response, button_box, count_label)
The output is shown in Figure 1-12:
Figure 1-12. Thumbs-up/thumbs-down rating system
If you run this in a Jupyter Notebook, a widget displays each AI response, with a
thumbs-up or thumbs-down button (see Figure 1-12) This provides a simple inter‐
face for quickly labeling responses, with minimal overhead. If you wish to do this
outside of a Jupyter Notebook, you could change the thumbs-up and thumbs-down
emojis for Y and N, and implement a loop using the built-in input() function, as a
text-only replacement for iPyWidgets.
Once you’ve finished labeling the responses, you get the output, which shows you
how each prompt performs.
26 | Chapter 1: The Five Principles of PromptingOutput:
A/B testing completed. Here's the results:
  variant  count  score
0       A      5    0.2
1       B      5    0.6
The dataframe was shuffled at random, and each response was labeled blind (without
seeing the prompt), so you get an accurate picture of how often each prompt per‐
formed. Here is the step-by-step explanation:
1. Three modules are imported: ipywidgets, IPython.display, and pandas. ipy1.
widgets contains interactive HTML widgets for Jupyter Notebooks and the
IPython kernel. IPython.display provides classes for displaying various types
of output like images, sound, displaying HTML, etc. Pandas is a powerful data
manipulation library.
2. The pandas library is used to read in the CSV file responses.csv, which contains2.
the responses you want to test. This creates a Pandas DataFrame called df.
3. df is shuffled using the sample() function with frac=1, which means it uses3.
all the rows. The reset_index(drop=True) is used to reset the indices to the
standard 0, 1, 2, …, n index.
4. The script defines response_index as 0. This is used to track which response4.
from the dataframe the user is currently viewing.
5. A new column feedback is added to the dataframe df with the data type as str5.
or string.
6. Next, the script defines a function on_button_clicked(b), which will execute6.
whenever one of the two buttons in the interface is clicked.
a. The function first checks the description of the button clicked was thea.
thumbs-up button (\U0001F44D; 
 ), and sets user_feedback as 1, or if it was
the thumbs-down button (\U0001F44E 
 ), it sets user_feedback as 0.
b. Then it updates the feedback column of the dataframe at the currentb.
response_index with user_feedback.
c. After that, it increments response_index to move to the next response.c.
d. If response_index is still less than the total number of responses (i.e., thed.
length of the dataframe), it calls the function update_response().
e. If there are no more responses, it saves the dataframe to a new CSV filee.
results.csv, then prints a message, and also prints a summary of the results by
variant, showing the count of feedback received and the average score (mean)
for each variant.
4. Evaluate Quality | 277. The function update_response() fetches the next response from the dataframe,7.
wraps it in paragraph HTML tags (if it’s not null), updates the response widget
to display the new response, and updates the count_label widget to reflect the
current response number and total number of responses.
8. Two widgets, response (an HTML widget) and count_label (a Label widget),8.
are instantiated. The update_response() function is then called to initialize
these widgets with the first response and the appropriate label.
9. Two more widgets, thumbs_up_button and thumbs_down_button (both Button9.
widgets), are created with thumbs-up and thumbs-down emoji as their descrip‐
tions, respectively. Both buttons are configured to call the on_button_clicked()
function when clicked.
10. The two buttons are grouped into a horizontal box ( button_box) using the HBox10.
function.
11. Finally, the response, button_box, and count_label widgets are displayed to the11.
user using the display() function from the IPython.display module.
A simple rating system such as this one can be useful in judging prompt quality
and encountering edge cases. Usually in less than 10 test runs of a prompt you
uncover a deviation, which you otherwise wouldn’t have caught until you started
using it in production. The downside is that it can get tedious rating lots of responses
manually, and your ratings might not represent the preferences of your intended
audience. However, even small numbers of tests can reveal large differences between
two prompting strategies and reveal nonobvious issues before reaching production.
Iterating on and testing prompts can lead to radical decreases in the length of
the prompt and therefore the cost and latency of your system. If you can find
another prompt that performs equally as well (or better) but uses a shorter prompt,
you can afford to scale up your operation considerably. Often you’ll find in this
process that many elements of a complex prompt are completely superfluous, or even
counterproductive.
The thumbs-up or other manually labeled indicators of quality don’t have to be the
only judging criteria. Human evaluation is generally considered to be the most accu‐
rate form of feedback. However, it can be tedious and costly to rate many samples
manually. In many cases, as in math or classification use cases, it may be possible to
establish ground truth (reference answers to test cases) to programmatically rate the
results, allowing you to scale up considerably your testing and monitoring efforts.
The following is not an exhaustive list because there are many motivations for
evaluating your prompt programmatically:
28 | Chapter 1: The Five Principles of PromptingCost
Prompts that use a lot of tokens, or work only with more expensive models,
might be impractical for production use.
Latency
Equally the more tokens there are, or the larger the model required, the longer it
takes to complete a task, which can harm user experience.
Calls
Many AI systems require multiple calls in a loop to complete a task, which can
seriously slow down the process.
Performance
Implement some form of external feedback system, for example a physics engine
or other model for predicting real-world results.
Classification
Determine how often a prompt correctly labels given text, using another AI
model or rules-based labeling.
Reasoning
Work out which instances the AI fails to apply logical reasoning or gets the math
wrong versus reference cases.
Hallucinations
See how frequently you encouner hallucinations, as measured by invention of
new terms not included in the prompt’s context.
Safety
Flag any scenarios where the system might return unsafe or undesirable results
using a safety filter or detection system.
Refusals
Find out how often the system incorrectly refuses to fulfill a reasonable user
request by flagging known refusal language.
Adversarial
Make the prompt robust against known prompt injection attacks that can get the
model to run undesirable prompts instead of what you programmed.
Similarity
Use shared words and phrases ( BLEU or ROGUE ) or vector distance (explained
in Chapter 5) to measure similarity between generated and reference text.
4. Evaluate Quality | 29Once you start rating which examples were good, you can more easily update the
examples used in your prompt as a way to continuously make your system smarter
over time. The data from this feedback can also feed into examples for fine-tuning,
which starts to beat prompt engineering once you can supply a few thousand exam‐
ples, as shown in Figure 1-13.
Figure 1-13. How many data points is a prompt worth?
Graduating from thumbs-up or thumbs-down, you can implement a 3-, 5-, or 10-
point rating system to get more fine-grained feedback on the quality of your prompts.
It’s also possible to determine aggregate relative performance through comparing
responses side by side, rather than looking at responses one at a time. From this you
can construct a fair across-model comparison using an Elo rating, as is popular in
chess and used in the Chatbot Arena by lmsys.org.
For image generation, evaluation usually takes the form of permutation prompting,
where you input multiple directions or formats and generate an image for each
30 | Chapter 1: The Five Principles of Promptingcombination. Images can than be scanned or later arranged in a grid to show the
effect that different elements of the prompt can have on the final image.
Input:
{stock photo, oil painting, illustration} of business
meeting of {four, eight} people watching on white MacBook on
top of glass-top table
In Midjourney this would be compiled into six different prompts, one for every
combination of the three formats (stock photo, oil painting, illustration) and two
numbers of people (four, eight).
Input:
1. stock photo of business meeting of four people watching
on white MacBook on top of glass-top table
2. stock photo of business meeting of eight people watching
on white MacBook on top of glass-top table
3. oil painting of business meeting of four people watching
on white MacBook on top of glass-top table
4. oil painting of business meeting of eight people watching
on white MacBook on top of glass-top table
5. illustration of business meeting of four people watching
on white MacBook on top of glass-top table
6. illustration of business meeting of eight people watching
on white MacBook on top of glass-top table
Each prompt generates its own four images as usual, which makes the output a little
harder to see. We have selected one from each prompt to upscale and then put them
together in a grid, shown as Figure 1-14. Y ou’ll notice that the model doesn’t always
get the correct number of people (generative AI models are surprisingly bad at math),
but it has correctly inferred the general intention by adding more people to the
photos on the right than the left.
Figure 1-14 shows the output.
4. Evaluate Quality | 31Figure 1-14. Prompt permutations grid
With models that have APIs like Stable Diffusion, you can more easily manipulate the
photos and display them in a grid format for easy scanning. Y ou can also manipulate
the random seed of the image to fix a style in place for maximum reproducibility.
With image classifiers it may also be possible to programmatically rate images based
on their safe content, or if they contain certain elements associated with success or
failure.
5. Divide Labor
As you build out your prompt, you start to get to the point where you’re asking a lot
in a single call to the AI. When prompts get longer and more convoluted, you may
find the responses get less deterministic, and hallucinations or anomalies increase.
Even if you manage to arrive at a reliable prompt for your task, that task is likely just
one of a number of interrelated tasks you need to do your job. It’s natural to start
exploring how many other of these tasks could be done by AI and how you might
string them together.
32 | Chapter 1: The Five Principles of PromptingOne of the core principles of engineering is to use task decomposition to break prob‐
lems down into their component parts, so you can more easily solve each individual
problem and then reaggregate the results. Breaking your AI work into multiple calls
that are chained together can help you accomplish more complex tasks, as well as
provide more visibility into what part of the chain is failing.
There are lots of factors that go into product naming, and an important task is naively
outsourced to the AI with no visibility into how it’s weighing the importance of these
factors (if at all). The way our current system works, we’re getting a list of names, but
all names are displayed with equal importance with no further context for helping us
decide. Fortunately AI tools are capable of self-evaluation; if we add a second step to
our task, we can automatically check for nondesirable outputs.
Input:
Rate this list of product names for a pair of shoes that can
fit any foot size. The rating should be out of 10, inline
next to the product name:
UniFit SoleStrides
FlexiSize All-Fit Shoes
OmniFit FootFlex
...
AnyStep AdaptiWalkers
FlexiRange UniversalSteps
PerfectGlide MultiFit Shoes
Output:
UniFit SoleStrides: 7/10
FlexiSize All-Fit Shoes: 8/10
OmniFit FootFlex: 7/10
...
AnyStep AdaptiWalkers: 7/10
FlexiRange UniversalSteps: 7.5/10
PerfectGlide MultiFit Shoes: 9/10
This ranking is based on clarity, memorability, and how well
the name communicates the unique selling point of the
product (that they can fit any foot size). Remember, these
ratings are subjective and the ultimate success of a product
name also depends on the target market, branding, and
marketing efforts.
In running this multiple times, it consistently rates the name “OneSize Glovewalkers”
as the worst, providing context (if you ask) that the concept might be confusing in
a shoe context. Y ou may be wondering why, if the model knows this is a bad name,
does it suggest it in the first place? LLMs work by predicting the next token in a
sequence and therefore struggle to know what the overall response will be when
5. Divide Labor | 33finished. However, when it has all the tokens from a previous response to review, it
can more easily predict whether this would be labeled as a good or bad response.
We can continue to chain multiple calls together to improve the results of our task.
For example, we could split this into three separate ratings: clarity, memorability, and
how well the name communicates the unique selling point of the product. These
ratings could then be given to a human as additional context on the final decision, or
even calculated together to select the final name programmatically.
The real unlock in learning to work professionally with AI versus just playing around
with prompting is realizing that every part of the system can be broken down into a
series of iterative steps. Even with a single prompt this principles applies, as simply
appending Let's think step by step to the prompt can lead to demonstrable gains
in reasoning and proficiency, as well as provide an audit trail for quality assurance
and debugging. When taking the time and tokens to reason, the ratings change and
are more consistent with the scoring criteria.
Input:
Let's think step by step. Rate this list of product names
for a pair of shoes that can fit any foot size. The rating
should be out of 10, inline next to the product name:
UniFit SoleStrides
FlexiSize All-Fit Shoes
OmniFit FootFlex
...
AnyStep AdaptiWalkers
FlexiRange UniversalSteps
PerfectGlide MultiFit Shoes
Output:
Here's a rating out of 10 for each product name, considering
factors like descriptiveness, catchiness, and overall
appeal:
1. **UniFit SoleStrides**: 7/10 - Sounds unifying but may
not immediately convey the "fit any size" idea.
2. **FlexiSize All-Fit Shoes**: 8.5/10 - Combines
flexibility and the notion that it fits all sizes.
3. **OmniFit FootFlex**: 8/10 - Omnipresence (fitting all)
and flexibility nicely conveyed.
...
18. **AnyStep AdaptiWalkers**: 7.5/10 - "AnyStep" suggests
versatility, but the overall name might be less direct.
19. **FlexiRange UniversalSteps**: 8/10 - Combines
flexibility and universal fit.
20. **PerfectGlide MultiFit Shoes**: 8.5/10 - "MultiFit" is
clear, and "PerfectGlide" suggests a comfortable experience.
34 | Chapter 1: The Five Principles of PromptingRemember, the best name can also be influenced by your
target audience, branding strategy, and the overall message
you want to convey.
OpenAI calls this  “giving the model time to think, ” and it is a key tenet of prompt
engineering. In effect, chain of thought  techniques like this, where the model is
encouraged to list out its steps, are like dividing a task within the same prompt. Once
we’ve automated product naming given a product idea, we can call ChatGPT again
to describe each product, which in turn can be fed into Midjourney to generate an
image of each product. Using an AI model to generate a prompt for an AI model
is meta prompting, and it works because LLMs are human-level prompt engineers
(Zhou, 2022).
Input:
Describe the new product "OneFit UltraStride Shoes," a shoe
that can fit any foot size, in great detail:
Output:
Introducing the revolutionary "OneFit UltraStride Shoes,"
the footwear innovation designed to fit any foot size with
unparalleled comfort and style. These shoes are a result of
cutting-edge technology and meticulous craftsmanship,
ensuring a truly personalized and enjoyable walking or
running experience for everyone.
Design and Materials:
The OneFit UltraStride Shoes boast an elegant and modern
design that appeals to both fashion-forward individuals and
sports enthusiasts. They feature a sleek and streamlined
silhouette, crafted using premium materials for durability
and aesthetic appeal. The shoe's upper is constructed from a
combination of breathable mesh and flexible synthetic
materials, allowing for excellent airflow and a lightweight
feel.
...
Color and Customization:
OneFit UltraStride Shoes are available in a variety of
vibrant colors and stylish patterns, catering to diverse
personal preferences. You can choose from classic
monochromatic options for a timeless look or opt for bold
and vibrant hues to make a fashion statement. The ability to
customize the shoe's appearance allows individuals to
express their unique style while enjoying the benefits of
the OneFit technology.
Conclusion:
The OneFit UltraStride Shoes represent a breakthrough in
footwear technology, eliminating the hassle of finding the
5. Divide Labor | 35perfect fit for your feet. With their adaptive design,
superior comfort, and stylish aesthetics, these shoes offer
a truly personalized experience for individuals of all foot
sizes. Stay comfortable, confident, and on-trend with the
OneFit UltraStride Shoes, the ultimate choice for those
seeking unmatched versatility and a perfect fit.
DALL-E is well-known by GPT-4, and therefore you can invoke its name within
ChatGPT and it does a reasonable job at crafting a prompt for an image generation
tool. If you were planning on using this prompt in production, you may consider
applying the prompting principle of providing examples, but it does a good enough
job for our purposes without examples.
Input:
Convert this description into a short prompt for an AI image
generation model such as DALL-E:
Output:
Create an eye-catching image that visually represents the
"OneFit UltraStride Shoes," a revolutionary footwear product
that adapts to any foot size. Showcase a sleek and modern
shoe design incorporating a stretchable upper, an adjustable
lacing system, and a cushioned insole for optimal comfort.
Emphasize the shoe's versatility by displaying it in action
on various surfaces, highlighting its excellent traction.
Incorporate vibrant colors and patterns to convey the
customizable nature of the product. The image should capture
the essence of innovation, style, and personalized fit that
the OneFit UltraStride Shoes offer.
The output of this prompt can now be plugged into image generation tools like
DALL-E or Midjourney as a prompt, which can give you a good starting point for
visualizing what the product might look like. Although this might not be the final
design you go with, seeing an image is more evocative and helps people form an
opinion faster. It’s easier cognitively to criticize or compliment an existing image than
it is to imagine a new image from a blank page or section of text.
Figure 1-15 shows the output.
36 | Chapter 1: The Five Principles of PromptingFigure 1-15. OneFit UltraStride shoes
It’s common practice when working with AI professionally to chain multiple calls
to AI together, and even multiple models, to accomplish more complex goals. Even
single-prompt applications are often built dynamically, based on outside context
queried from various databases or other calls to an AI model. The library LangChain
has developed tooling for chaining multiple prompt templates and queries together,
making this process more observable and well structured. A foundational example is
progressive summarization, where text that is too large to fit into a context window
can be split into multiple chunks of text, with each being summarized, before finally
summarizing the summaries. If you talk to builders of early AI products, you’ll find
they’re all under the hood chaining multiple prompts together, called AI chaining, to
accomplish better results in the final output.
The Reason and Act (ReAct)  framework was one of the first popular attempts at
AI agents, including the open source projects BabyAGI, AgentGPT and Microsoft
AutoGen. In effect, these agents are the result of chaining multiple AI calls together
in order to plan, observe, act, and then evaluate the results of the action. Autonomous
agents will be covered in Chapter 6 but are still not widely used in production at the
5. Divide Labor | 37time of writing. This practice of self-reasoning agents is still early and prone to errors,
but there are promising signs this approach can be useful in achieving complex tasks,
and is likely to be part of the next stage in evolution for AI systems.
There is an AI battle occurring between large tech firms like Microsoft and Google,
as well as a wide array of open source projects on Hugging Face, and venture-funded
start-ups like OpenAI and Anthropic. As new models continue to proliferate, they’re
diversifying in order to compete for different segments of the growing market. For
example, Anthropic’s Claude 2 had an 100,000-token context window , compared
to GPT-4’s standard 8,192 tokens . OpenAI soon responded with a 128,000-token
window version of GPT-4 , and Google touts a 1 million token context length with
Gemini 1.5. For comparison, one of the Harry Potter books would be around 185,000
tokens, so it may become common for an entire book to fit inside a single prompt,
though processing millions of tokens with each API call may be cost prohibitive for
most use cases.
This book focuses on GPT-4 for text generation techniques, as well as Midjourney v6
and Stable Diffusion XL for image generation techniques, but within months these
models may no longer be state of the art. This means it will become increasingly
important to be able to select the right model for the job and chain multiple AI
systems together. Prompt templates are rarely comparable when transferring to a new
model, but the effect of the Five Prompting Principles will consistently improve any
prompt you use, for any model, getting you more reliable results.
Summary
In this chapter, you learned about the importance of prompt engineering in the
context of generative AI. We defined prompt engineering as the process of develop‐
ing effective prompts that yield desired results when interacting with AI models.
Y ou discovered that providing clear direction, formatting the output, incorporating
examples, establishing an evaluation system, and dividing complex tasks into smaller
prompts are key principles of prompt engineering. By applying these principles and
using common prompting techniques, you can improve the quality and reliability of
AI-generated outputs.
Y ou also explored the role of prompt engineering in generating product names and
images. Y ou saw how specifying the desired format and providing instructive exam‐
ples can greatly influence the AI’s output. Additionally, you learned about the concept
of role-playing, where you can ask the AI to generate outputs as if it were a famous
person like Steve Jobs. The chapter emphasized the need for clear direction and
context to achieve desired outcomes when using generative AI models. Furthermore,
you discovered the importance of evaluating the performance of AI models and the
various methods used for measuring results, as well as the trade-offs between quality
and token usage, cost, and latency.
38 | Chapter 1: The Five Principles of PromptingIn the next chapter, you will be introduced to text generation models. Y ou will learn
about the different types of foundation models and their capabilities, as well as their
limitations. The chapter will also review the standard OpenAI offerings, as well as
competitors and open source alternatives. By the end of the chapter, you will have
a solid understanding of the history of text generation models and their relative
strengths and weaknesses. This book will return to image generation prompting in
Chapters 7, 8, and 9, so you should feel free to skip ahead if that is your immediate
need. Get ready to dive deeper into the discipline of prompt engineering and expand
your comfort working with AI.
Summary | 39About the Authors
Mike Taylor cofounded a 50-person growth marketing agency called Ladder with
offices in the USA, UK, and EU. More than 400,000 people have taken his marketing
and AI courses on LinkedIn Learning, Udemy, and Vexpower.
James Phoenix builds reliable data pipelines for marketing teams, automating thou‐
sands of recurring tasks. He has taught 60+ Data Science bootcamps for General
Assembly and partnered with Mike on the Udemy course, and Vexpower.
Both authors been experimenting with prompt engineering since the GPT-3 beta
in 2020. They slowly automated every part of their jobs with AI and now work as
prompt engineers on various projects.
Colophon
The animal on the cover of Prompt Engineering for Generative AI is a screaming hairy
armadillo (Chaetophractus vellerosus). This species of armadillo gets it name due to its
habit of squealing, or screaming, when it is handled or threatened.
The screaming hairy armadillo resides in arid areas, specifically in regions in Argen‐
tina, Bolivia, and Paraguay. This animal prefers subtropical or tropical regions such
as dry forests, scrubland, grassland, and deserts. White and light brown hair cover
the animal’s limbs and belly. A caparace, a thick armor made of keratin, covers the
animal’s body, a shield covers its head, and a small band exists between its ears. The
animal typically reaches 12 to 22 inches in length, including its tail, and weighs less
than 2 pounds, with male armadillos generally being larger than females.
The screaming hairy armadillo is an omnivore, eating small vertebrates such as frogs,
toads, lizards, birds, and rodents, as well as fruits and vegetation. It can go long
periods of time without drinking water.
Although the IUCN Red List designates the screaming hairy armadillo as Least
Concern, it is heavily hunted in parts of Bolivia for its meat and carapace. Many
of the animals on O’Reilly covers are endangered; all of them are important to the
world.
The cover illustration is by Karen Montgomery, based on an antique line engraving
from Beeton’s Dictionary. The series design is by Edie Freedman, Ellie Volckhausen,
and Karen Montgomery. The cover fonts are Gilroy Semibold and Guardian Sans.
The text font is Adobe Minion Pro; the heading font is Adobe Myriad Condensed;
and the code font is Dalton Maag’s Ubuntu Mono. Calculate the determinant of the product
det_product_solution = det_A * det_B
return det_product_solution
def exercise3():
"""
Terrell usually lifts two 20-pound weights 12 times.
If he switches to two 15-pound weights, how many times must he lift
them to match the total weight lifted earlier?
"""
weight_20 = 20
times_20 = 12
# Calculate the total weight Terrell lifts with 20-pound weights
total_weight_20_solution = 2 * weight_20 * times_20
# Calculate the number of times Terrell should lift the 15-pound weights
weight_15 = 15
times_15_solution = total_weight_20_solution / (2 * weight_15)
return times_15_solution
def exercise4():
"""
Given the system of equations:
6x-4y = a
6y-9x = b
Determine the value of \(\frac{a}{b} \).
"""
import sympy as sp
a, b, y = sp.symbols(’a b y’)
x_expr = (a + 4*y) / 6
b_expr = 6*y - 9*x_expr
a_over_b_solution = a / b_expr
return a_over_b_solution.simplify()
12.6.8. Example instructions added to Python prompts
We tried a few wordings of these instructions that are very close to the following template:
You are a math and coding expert and you know how to use attached code snippets. I gave
you a series of demonstrations of math Problems and Python Solutions. When you respond
to the next problem,
first find similar snippets and describe them before writing your solution. Describe
functions, classes and methods that are relevant for the Problem. Then respond with the
Python Solution of the final Problem, thinking step by step. Remember to define all
variables and include all imports.
Finish your code with
answer = ...
114Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context
print(answer)
where answer is a float number, a tuple of float numbers, a sympy expression or a string.
In the second stage, the prompt was divided into two parts:
You are a math and coding expert and you know how to use attached code snippets. I gave
you a series of demonstrations of math Problems and Python Solutions. The next problem
has a solution that generated
an exception.
The above instruction was followed by the proposed program and error traces. The instruction
was ended with
First find similar snippets and describe them before writing your solution. Describe
functions, classes and methods that are relevant for the Problem. Then analyze why the
original solution resulted in an exception
and respond with an improved Python Solution of the final Problem, thinking step by step.
Remember to define all variables and include all imports.
Finish your code with
answer = ...
print(answer)
where answer is a float number, a tuple of float numbers, a sympy expression or a string.
In the third stage we modified the above instruction to account for two solutions present in the
context.
12.6.9. Example problem and a Gemini 1.5 Pro solution
Problem:
Let $a,$ $b,$ and $c$ be nonnegative real numbers such that $a + b + c = 1.$ Find the
maximum value of
\[a(a + b)^2 (b + c)^3 (a + c)^4.\]
Python answer:
## Finding the Maximum Value of a Function with Constraints
This problem involves finding the maximum value of a function with the constraint that
the sum of its variables equals 1. We can approach this using optimization techniques
from SciPy.
### Relevant Functions and Classes
1. **‘scipy.optimize.minimize‘**: This function is used to find the minimum of a given
function. Since we want to find the maximum, we can minimize the negative of the
function.
2. **‘scipy.optimize.Bounds‘**: This class allows us to specify constraints on the
variables, ensuring they remain nonnegative and sum to 1.
### Python Solution
Here’s the Python code to solve the problem:
‘‘‘python
115Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context
import numpy as np
from scipy.optimize import minimize, Bounds
def objective_function(x):
a, b, c = x
return -a * (a + b)**2 * (b + c)**3 * (a + c)**4 # negative for maximization
def constraint(x):
return np.sum(x) - 1 # constraint: a + b + c = 1
# Initial guess (must satisfy the constraint)
x0 = np.array([0.3, 0.3, 0.4])
# Define bounds: 0 <= a, b, c <= 1
bounds = Bounds(np.zeros_like(x0), np.ones_like(x0))
# Minimize the negative of the function with the constraint
result = minimize(objective_function, x0, method=’SLSQP’, bounds=bounds, constraints={’
type’: ’eq’, ’fun’: constraint})
# Extract the solution and compute the maximum value
a, b, c = result.x
answer = a * (a + b)**2 * (b + c)**3 * (a + c)**4
print(answer)
‘‘‘
This code defines the objective function (the negative of the given expression) and the
constraint (a + b + c = 1). It then uses the ‘minimize‘ function with the ’SLSQP’ method,
which is suitable for constrained optimization problems. The ‘Bounds‘ object ensures
that the variables remain within the desired range. Finally, the code extracts the
solution and computes the maximum value of the original expression.
Running this code in Google Colab produces a result of0.015624507088912548, which falls
within the numerical tolerance used for evaluations on the MATH dataset (see Appendix 12.13 for
details), compared to the exact answer of1/64. While alternative solutions using natural language or
SymPy with multiple applications of the AM-GM inequality are possible, the SciPy approach presented
here offers the most straightforward, albeit approximate solution.
12.7. Unstructured Multimodal Data Analytics
Below is the full prompt we used for each model when performing analytics of the images (some
information is anonymized).
Please process the following images, and output the [category], [color], [semantic
attribute] for each image.
- For [category], please choose from [anonymized, 38 classes]
- For [color], please choose from [anonymized, 45 classes]
- For [semantic attribute], please choose from [anonymized, 8 classes]
Each image has its own ID starting from id_0. Please only output the table formatted in
the following order:
image_0,category,color,semantic attribute
image_1,category,color,semantic attribute
...
Below are the images:
image_0: <base64 string of the image>
image_1: <base64 string of the image>
image_2: <base64 string of the image>
<...and more images>
116Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context
image_255: <base64 string of the image>
Please output the table only in the format specified.
And the evaluation results on each of the individual attribute is presented in Figure 29.
8 16 32 64 128 256 512
Batch Size
30
40
50
60
70
80
90Accuracy (%)
Accuracy on category extraction
Gemini 1.5 Pro
GPT 4 T urbo 20240409
Claude 3 Opus
8 16 32 64 128 256 512
Batch Size
20
30
40
50
60Accuracy (%)
Accuracy on color extraction
Gemini 1.5 Pro
GPT 4 T urbo 20240409
Claude 3 Opus
8 16 32 64 128 256 512
Batch Size
45
50
55
60
65
70Accuracy (%)
Accuracy on semantic attribute extraction
Gemini 1.5 Pro
GPT 4 T urbo 20240409
Claude 3 Opus
Figure 29|Performance of models on unstructured data analytics tasks.
12.8. Planning
Bellow is the 1-shot prompt for the BlocksWorld task.
Please solve the problem:
(define (problem BW-rand-4)
(:domain blocksworld-4ops)
(:objects b4 b1 b3 b2)
(:init
(on b3 b1)
(on b1 b4)
(clear b3)
(handempty)
(ontable b2)
(ontable b4)
(clear b2)
)
(:goal (and
(on b2 b4)
(on b3 b1)
))
)
Your plan as plain text without formatting:
(unstack b3 b1)
(put-down b3)
(unstack b1 b4)
(put-down b1)
(pick-up b2)
(stack b2 b4)
(pick-up b3)
(stack b3 b1)
done.
Please solve the problem:
(define (problem BW-rand-6)
(:domain blocksworld-4ops)
(:objects b5 b1 b4 b2 b3 b6)
(:init
117Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context
(on b4 b1)
(handempty)
(ontable b6)
(on b2 b4)
(clear b3)
(ontable b5)
(on b3 b2)
(clear b6)
(on b1 b5)
)
(:goal (and
(on b4 b2)
(on b1 b4)
(on b5 b1)
(on b3 b5)
))
)
Your plan as plain text without formatting:
Bellow is the 1-shot prompt for the Logistics task.
Please solve the problem:
(define (problem logistics-c4-s2-p3-a4)
(:domain logistics-strips)
(:objects
a0 a1 a2 a3
c0 c1 c2 c3
t0 t1 t2 t3
l0-0 l0-1 l1-0 l1-1 l2-0 l2-1 l3-0 l3-1
p0 p1 p2
)
(:init
(AIRPLANE a0) (AIRPLANE a1)(AIRPLANE a2)(AIRPLANE a3)
(CITY c0)(CITY c1)(CITY c2)(CITY c3)
(TRUCK t0)(TRUCK t1)(TRUCK t2)(TRUCK t3)
(LOCATION l0-0)(in-city l0-0 c0)
(LOCATION l0-1)(in-city l0-1 c0)
(LOCATION l1-0)(in-city l1-0 c1)
(LOCATION l1-1)(in-city l1-1 c1)
(LOCATION l2-0)(in-city l2-0 c2)
(LOCATION l2-1)(in-city l2-1 c2)
(LOCATION l3-0)(in-city l3-0 c3)
(LOCATION l3-1)(in-city l3-1 c3)
(AIRPORT l0-0)(AIRPORT l1-0)(AIRPORT l2-0)(AIRPORT l3-0)
(OBJ p0)(OBJ p1)(OBJ p2)
(at t0 l0-0)(at t1 l1-1)(at t2 l2-0)(at t3 l3-0)
(at p0 l1-1)(at p1 l0-1)(at p2 l0-0)
(at a0 l1-0)
(at a1 l1-0)
(at a2 l2-0)
(at a3 l3-0)
)
(:goal
(and
(at p0 l2-0)
(at p1 l2-0)
(at p2 l1-1)
)
)
)
118Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context
Your plan as plain text without formatting:
(load-truck p0 t1 l1-1)
(drive-truck t1 l1-1 l1-0 c1)
(unload-truck p0 t1 l1-0)
(load-airplane p0 a1 l1-0)
(fly-airplane a1 l1-0 l2-0)
(unload-airplane p0 a1 l2-0)
(drive-truck t0 l0-0 l0-1 c0)
(load-truck p1 t0 l0-1)
(drive-truck t0 l0-1 l0-0 c0)
(unload-truck p1 t0 l0-0)
(fly-airplane a3 l3-0 l0-0)
(load-airplane p2 a3 l0-0)
(fly-airplane a3 l0-0 l1-0)
(unload-airplane p2 a3 l1-0)
(load-truck p2 t1 l1-0)
(drive-truck t1 l1-0 l1-1 c1)
(unload-truck p2 t1 l1-1)
(fly-airplane a1 l2-0 l0-0)
(load-airplane p1 a1 l0-0)
(fly-airplane a1 l0-0 l2-0)
(unload-airplane p1 a1 l2-0)
done.
Please solve the problem:
(define (problem logistics-c2-s2-p3-a2)
(:domain logistics-strips)
(:objects
a0 a1
c0 c1
t0 t1
l0-0 l0-1 l1-0 l1-1
p0 p1 p2
)
(:init
(AIRPLANE a0)(AIRPLANE a1)
(CITY c0)(CITY c1)
(TRUCK t0) (TRUCK t1)
(LOCATION l0-0)(in-city l0-0 c0)
(LOCATION l0-1)(in-city l0-1 c0)
(LOCATION l1-0)(in-city l1-0 c1)
(LOCATION l1-1)(in-city l1-1 c1)
(AIRPORT l0-0) (AIRPORT l1-0)
(OBJ p0)(OBJ p1)(OBJ p2)
(at t0 l0-1)(at t1 l1-0)
(at p0 l0-1)(at p1 l1-0)(at p2 l1-1)
(at a0 l0-0)(at a1 l0-0)
)
(:goal
(and
(at p0 l0-1)
(at p1 l1-0)
(at p2 l0-0)
)
)
)
119Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context
Your plan as plain text without formatting:
Bellow is the 1-shot prompt for the Mini-Grid task.
Please solve the problem:
(define (problem grid_2Vroom2)
(:domain grid)
(:objects
p0 p1 p2 p3 p4 p5 p6 p7 p8
shape0
key0
)
(:init
; Object types
(place p0) (place p1) (place p2) (place p3) (place p4) (place p5) (place p6) (place
p7) (place p8)
(shape shape0)
(key key0)
; Open/locked cells
(open p0) (open p1) (open p2) (open p3) (open p5) (open p6) (open p7) (open p8)
(locked p4)
; Connected cells
(conn p0 p1)
(conn p0 p2)
(conn p1 p0)
(conn p1 p3)
(conn p2 p0)
(conn p2 p3)
(conn p2 p4)
(conn p3 p2)
(conn p3 p1)
(conn p4 p2)
(conn p4 p5)
(conn p5 p4)
(conn p5 p6)
(conn p5 p7)
(conn p6 p5)
(conn p6 p8)
(conn p7 p5)
(conn p7 p8)
(conn p8 p7)
(conn p8 p6)
; Lock and key shapes
(lock-shape p4 shape0)
(key-shape key0 shape0)
; Key placement
(at key0 p0)
; Robot placement
(at-robot p3)
(arm-empty)
)
(:goal (at-robot p7))
)
Your plan as plain text without formatting:
(move p3 p2)
(move p2 p0)
(pickup p0 key0)
(move p0 p2)
(unlock p2 p4 key0 shape0)
(move p2 p4)
120Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context
(move p4 p5)
(move p5 p7)
done.
Please solve the problem:
(define (problem grid_3Vroom3)
(:domain grid)
(:objects
p0 p1 p2 p3 p4 p5 p6 p7 p8 p9 p10 p11 p12 p13 p14 p15 p16 p17 p18 p19 p20 p21 p22 p23
p24 p25 p26 p27 p28
shape0
key0
)
(:init
; Object types
(place p0) (place p1) (place p2) (place p3) (place p4) (place p5) (place p6) (place
p7) (place p8) (place p9) (place p10) (place p11) (place p12) (place p13) (place p14)
(place p15) (place p16) (place p17) (place p18) (place p19) (place p20) (place p21)
(place p22) (place p23) (place p24) (place p25) (place p26) (place p27) (place p28)
(shape shape0)
(key key0)
; Open/locked cells
(open p0) (open p1) (open p2) (open p3) (open p4) (open p5) (open p6) (open p7) (open
p8) (open p10) (open p11) (open p12) (open p13) (open p14) (open p15) (open p16) (
open p17) (open p18) (open p20) (open p21) (open p22) (open p23) (open p24) (open p25
) (open p26) (open p27) (open p28)
(locked p9) (locked p19)
; Connected cells
(conn p0 p1)
(conn p0 p3)
(conn p1 p0)
(conn p1 p2)
(conn p1 p4)
(conn p2 p1)
(conn p2 p5)
(conn p3 p0)
(conn p3 p4)
(conn p3 p6)
(conn p4 p3)
(conn p4 p1)
(conn p4 p5)
(conn p4 p7)
(conn p5 p4)
(conn p5 p2)
(conn p5 p8)
(conn p6 p3)
(conn p6 p7)
(conn p6 p9)
(conn p7 p6)
(conn p7 p4)
(conn p7 p8)
(conn p8 p7)
(conn p8 p5)
(conn p9 p6)
(conn p9 p10)
(conn p10 p9)
(conn p10 p11)
(conn p10 p13)
(conn p11 p10)
(conn p11 p12)
(conn p11 p14)
121Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context
(conn p12 p11)
(conn p12 p15)
(conn p13 p10)
(conn p13 p14)
(conn p13 p16)
(conn p14 p13)
(conn p14 p11)
(conn p14 p15)
(conn p14 p17)
(conn p15 p14)
(conn p15 p12)
(conn p15 p18)
(conn p16 p13)
(conn p16 p17)
(conn p17 p16)
(conn p17 p14)
(conn p17 p18)
(conn p18 p17)
(conn p18 p15)
(conn p18 p19)
(conn p19 p18)
(conn p19 p22)
(conn p20 p21)
(conn p20 p23)
(conn p21 p20)
(conn p21 p22)
(conn p21 p24)
(conn p22 p21)
(conn p22 p19)
(conn p22 p25)
(conn p23 p20)
(conn p23 p24)
(conn p23 p26)
(conn p24 p23)
(conn p24 p21)
(conn p24 p25)
(conn p24 p27)
(conn p25 p24)
(conn p25 p22)
(conn p25 p28)
(conn p26 p23)
(conn p26 p27)
(conn p27 p26)
(conn p27 p24)
(conn p27 p28)
(conn p28 p27)
(conn p28 p25)
; Lock and key shapes
(lock-shape p9 shape0)
(lock-shape p19 shape0)
(key-shape key0 shape0)
; Key placement
(at key0 p12)
; Robot placement
(at-robot p16)
(arm-empty)
)
(:goal (at-robot p4))
)
Your plan as plain text without formatting:
122Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context
Bellow is the 1-shot prompt for the Trip Planning task.
Please solve the problem:
You plan to visit 6 European cities for 13 days in total. You only take direct flights
to commute between cities. You want to spend 3 days in Dublin. You would like to meet
your friends at Dublin between day 7 and day 9 to tour together. You would like to visit
Madrid for 2 days. You plan to visit relatives in Madrid between day 2 and day 3. You
plan to stay in Oslo for 3 days. You would like to visit London for 2 days. You want to
spend 3 days in Vilnius. You plan to stay in Berlin for 5 days. You are going to attend
a wedding in Berlin between day 3 and day 7.
Here are the cities that have direct flights:
London and Madrid, Oslo and Vilnius, Berlin and Vilnius, Madrid and Oslo, Madrid and
Dublin, London and Oslo, Madrid and Berlin, Berlin and Oslo, Dublin and Oslo, London and
Dublin, London and Berlin, Berlin and Dublin.
Find a trip plan of visiting the cities for 13 days by taking direct flights to commute
between them.
Here is the trip plan for visiting the 6 European cities for 13 days:
**Day 1-2:** Arriving in London and visit London for 2 days.
**Day 2:** Fly from London to Madrid.
**Day 2-3:** Visit Madrid for 2 days.
**Day 3:** Fly from Madrid to Berlin.
**Day 3-7:** Visit Berlin for 5 days.
**Day 7:** Fly from Berlin to Dublin.
**Day 7-9:** Visit Dublin for 3 days.
**Day 9:** Fly from Dublin to Oslo.
**Day 9-11:** Visit Oslo for 3 days.
**Day 11:** Fly from Oslo to Vilnius.
**Day 11-13:** Visit Vilnius for 3 days.
done.
Please solve the problem:
You plan to visit 6 European cities for 17 days in total. You only take direct flights
to commute between cities. You want to spend 4 days in Manchester. You plan to stay in
Florence for 5 days. You want to spend 3 days in Geneva. You are going to attend a
wedding in Geneva between day 1 and day 3. You want to spend 3 days in Seville. During
day 7 and day 9, you have to attend a conference in Seville. You would like to visit
Prague for 2 days. You plan to stay in Valencia for 5 days. From day 3 to day 7, there
is a annual show you want to attend in Valencia.
Here are the cities that have direct flights:
Manchester and Prague, Seville and Manchester, Geneva and Manchester, Valencia and
Seville, Geneva and Valencia, Valencia and Prague, Prague and Florence, Geneva and
Prague.
Find a trip plan of visiting the cities for 17 days by taking direct flights to commute
between them.
Bellow is the 1-shot prompt for the Calendar Scheduling task.
Please solve the problem:
You need to schedule a meeting for Samuel, Evelyn, Ruth and Amanda for half an hour
between the work hours of 9:00 to 17:00 on Monday.
Here are the existing schedules for everyone during the day:
Samuel is free the entire day.
Evelyn has meetings on Monday during 9:00 to 10:00, 11:00 to 12:00, 12:30 to 13:00,
123Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context
15:30 to 16:00;
Ruth has meetings on Monday during 9:30 to 11:00, 11:30 to 12:30, 13:00 to 13:30, 14:00
to 14:30, 15:00 to 16:00, 16:30 to 17:00;
Amanda has meetings on Monday during 10:00 to 10:30, 11:00 to 12:30, 13:00 to 13:30,
14:00 to 15:00, 15:30 to 16:00;
Amanda can not meet on Monday before 16:00. Find a time that works for everyone’s
schedule and constraints.
Here is the proposed time: Monday, 16:00 - 16:30
done.
Please solve the problem:
You need to schedule a meeting for Walter, Jacob, Jennifer and Joan for one hour between
the work hours of 9:00 to 17:00 on Monday.
Here are the existing schedules for everyone during the day:
Walter is busy on Monday during 9:30 to 10:00, 13:00 to 13:30;
Jacob has meetings on Monday during 11:00 to 11:30, 13:00 to 13:30;
Jennifer is busy on Monday during 9:30 to 10:30, 11:30 to 12:00, 12:30 to 15:00;
Joan has blocked their calendar on Monday during 9:30 to 10:00, 10:30 to 11:30, 12:00 to
12:30, 13:00 to 14:00, 14:30 to 15:30;
Find a time that works for everyone’s schedule and constraints.
12.9. Productivity Impact of LLMs Across Jobs
For further examples from Section 6.1.7 see Tables 49, 50, 51.
124Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context
I am working on the weekly activities calendar for my preschool class, and I need to create the calendar and the corresponding
weekly and daily themes and supplies. I would like you to start doing this for me. Each Friday afternoon, I spend about 3
hours searching for a weekly theme, making a printable calendar for the upcoming week, and designing the activity sheets
for each day.
I would like you to use the resources I provided and any other resources you have access to in order to choose a theme for
the week of May 6th through May 10th, 2024. We are located in Maple Grove, MN, and I would like the theme to be suited
for the typical climate in our location during the first week of May.
After choosing the theme, choose these components for the week:
* Color of the week
* Animal of the week
* Snack theme of the week - example: tree fruits (no nuts, processed sugars, or gluten)
* Word of the week (related to the theme)
* Song of the week
All of the "of the week" components mentioned above should be related to the theme chosen, the specific dates (ie: if one of
the dates for that week is a holiday or is known for something specific), the season, and/or our geographic location.
Next, the daily activities need to be chosen based on the theme and the "of the week" components. For each day, the following
will be needed:
* A specific snack (ie: apple slices)
* A fact about the animal of the week
* A title and author for a book of the day so I can check out the book from the library (preferably related to the theme or one
of the "of the week" components)
* A positive message "sentence of the day" that includes the word of the week
* An activity sheet for the day that can be printed out on normal 9x11 paper (coloring, tracing, dotting, watercolor, or
something similar)
Please, design me a weekly calendar that the students can refer to each day. It should include the theme of the week, the
weekly components, and the daily activities. Make sure it is visually appealing for 4-year-olds and has elements that will be
engaging for them.
Then, provide me with an audio file of the song of the week. I will also need a printable document with the lyrics for the
song, so I can teach them in sing-along format. If this print-out includes any images to help them with the progression that
would be wonderful (ie: Itsy Bitsy Spider might have images of a little spider, waterspout, rain, spider in the water, sun, rain,
small spider again.)
Next, provide me with 5 daily activity sheets to correspond with each day based on what was chosen above. These also need
to be printable and no longer than 1 page each.
Finally, provide me with a table with 3 columns: snack, book, supplies for activity sheet (ie: paint, pencils, dobbers, crayons),
and 5 rows for the day of the week. Include the following information from the calendar you created for each day: Snack,
Book, and Supplies.
Attached documents:
* Web page with pre-school themes
* Image with an example of a weekly pre-school planner
Table 49|Example task from a Pre-school Teacher.
125Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context
I need your help analyzing the metadata and filenames from my latest real estate photo shoot to assess image quality before
editing. I’ve attached a CSV file containing key details for each of the 58 photos, including:
- Filename
- Camera model
- Shutter speed
- Aperture
- ISO
- Focal length
- Timestamp
- GPS coordinates
Please analyze this data and provide a report summarizing:
1. Photos that may have quality issues based on the camera settings used, such as:
- Shutter speed slower than 1/60 (potential blur/camera shake)
- Aperture wider than f/8 (reduced sharpness)
- ISO higher than 800 (excessive noise)
2. A list of photos grouped by room/area of the house, based on timestamps and/or similar filenames (e.g. kitchen_01.jpg,
kitchen_02.jpg, etc.)
3. The 10 photos with the widest angle of view based on focal length, as these often make the best ""hero"" shots for real
estate listings.
4. Any filenames that don’t follow our standard naming convention of [room/area]_[number].jpg
I’ve also attached a reference sheet with examples of our studio’s technical quality standards and filename conventions.
Attached documents:
* CSV file containing key details for each of the 58 photos
* PDF of the studio’s technical quality standards
Table 50|Example task from a Real Estate Photographer.
126Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context
I need to generate a six-month PR plan of ideas for a chiropractor client. He loves to tie into monthly observances, so I
made a list of those in January–June to start the project. Dr. Joe is interested in any of the following activities: special
targeted client days (press release/web), fundraisers for local nonprofits (press release/web), letters to the editor about a
specific wellness issue (local newspaper), special wellness lecture nights at his office or another community venue (press
release/web), promotion of his Biggest Winner lifestyle program (press release/web), and targeted promotions to bring
clients in for an adjustment/exam (email messages).
Attached is an old PR plan that might provide some inspiration, although it is formatted differently than what we’re requesting
now. Also attached is a client story to give some perspective on the type of person Dr. Joe is.
We are looking for a one- to two-page list of publicity ideas by month from January to June, using the ideas above with
the national observances listed below. A mix of activities is optimal for publicity purposes. For example, January might tie
into a national observation with a special screening at the office of a wellness-related documentary. Then, February hosts a
fundraising day for a local nonprofit, and March observes a lecture about getting a good night’s rest. Two or three sentences
to describe an idea per month is probably sufficient. No more than two ideas should be presented per month. One idea is
also okay.
Some months have more than one observance in order to offer choices. It is not anticipated that we would use more than
one, although we could if there was a good reason to.
January: National Blood Donor Month
February: National Eating Disorder Month
March: National Sleep Awareness Week
April: Every Kid Healthy Week
Patient Experience Week
World Health Day
May: National Women’s Health Month
June: Men’s Health Month
Family Health and Fitness Day
Attached documents:
* PDF of an old PR plan
* PDF of a client story
Table 51|Example task from a PR Manager.
127Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context
12.10. Expertise QA
12.10.0.1 Background: Why evaluate the long tail?
The most impactful use cases of Generative AI (GenAI) require the model to operate on the long tail of
human knowledge and skills. Such skills are valuable to a user because they are rare, and information
and quality sparseness in training data is a challenge for GenAI models. To apply expertise, models
need to balance memorization with generalization from training data to individual use cases. The
model needs to have sufficient capacity and a precise enough fit to describe the long tail of the
distribution (c.f., GPQA, (Carlini et al., 2022; Rein et al., 2023; Saab et al., 2024)).
Measuring performance on the long tail in generative applications turns out to be challenging for
practical reasons: without in-depth knowledge of the subject matter, model responses are difficult
to judge even when golden answers are available. It is equally difficult to identify a distribution of
general topics that reflects realistic, yet aspirational use cases.
We surveyed a panel of in-house experts that were selected for their general ability to reason,
write, read, and gauge, asking for their specific expertise. Examples of such expertise included “25
years experience as a professional classical pianist”, “Shakespearean Tragic Plays - M.A.”. In Expertise
QA, we focus on hard human-interest questions, predominantly from the humanities.
These experts (𝑛𝑒 = 57) were asked to formulate questions (𝑛𝑞 = 572; 19.2 words avg.) that
required the specific expertise to do any necessary research, but also to select and combine the
obtained information into a well-crafted response. The same experts then rated and ranked model
responses to their respective questions. The models were evaluated according to their ability to answer
such questions with a high degree of accuracy, but, secondarily, completeness and informativeness.
This reflects the expected utility provided to users. Experts were unaware of the origins of each model
response.
Results show that the Gemini 1.5 models significantly and strongly outperform 1.0 Pro on the
Expert QA task, both in pointwise ‘Accurate’ and ‘Severely inaccurate’ ratings, and in side-by-side
rankings (see Figure 18), despite giving much longer, more detailed responses. The advantage also
holds when comparing answers side-by-side, which takes into account not just factual accuracy, but
also informativeness (see Table 53 in the Appendix).
12.10.1. Results
See Table 52 for pointwise accuracy judgments, and Table 53 for side-by-side comparisons derived
from the n-way rankings.
1.0 Pro 1.0 Ultra 1.5 Flash 1.5 Pro
Accurate 50.0% ±4.1 61.5% ±4.0 67.7% ±3.9 67.1% ±3.8
Severely inaccurate 26.7% ±3.6 10.1% ±2.4 7.3% ±2.2 6.3% ±2.0
Words 270 ±12 314 ±7 420 ±7 390 ±7
Table 52|Experts label each response as "fully accurate" (higher is better), "somewhat inaccurate",
and "severely inaccurate" (lower is better). The two metrics shown correspond to the proportion of
the extreme labels.𝑛𝑞 = 572.
128Gemini 1.5: Unlocking multimodal uinvalid type: string "James Phoenix  \n& Mike Taylor\nPrompt \nEngineering for \nGenerative AI\nFuture-Proof Inputs for Reliable AI Outputs\nFree \nChapterThis excerpt contains Chapter 1. The complete book is\navailable on the O’Reilly Online Learning Platform and\nthrough other retailers.\nJames Phoenix and Mike Taylor\nPrompt Engineering for\nGenerative AI\nFuture-Proof Inputs for Reliable\nAI Outputs at Scale\nBoston Farnham Sebastopol TokyoBeijing Boston Farnham Sebastopol TokyoBeijing978-1-098-15343-4\n[LSI]\nPrompt Engineering for Generative AI\nby James Phoenix and Mike Taylor\nCopyright © 2024 Saxifrage, LLC and Just Understanding Data LTD. All rights reserved.\nPrinted in the United States of America.\nPublished by O’Reilly Media, Inc., 1005 Gravenstein Highway North, Sebastopol, CA 95472.\nO’Reilly books may be purchased for educational, business, or sales promotional use. Online editions are\nalso available for most titles (http://oreilly.com). For more information, contact our corporate/institutional\nsales department: 800-998-9938 or corporate@oreilly.com.\nAcquisitions Editor: Nicole Butterfield\nDevelopment Editor: Corbin Collins\nCopyeditor: Piper Editorial Consulting, LLC\nProofreader: Kim Wimpsett\nIndexer: nSight, Inc.\nInterior Designer: David Futato\nCover Designer: Karen Montgomery\nIllustrator: Kate Dullea\nMay 2024:  First Edition\nRevision History for the First Edition\n2024-05-15: First Release\nSee http://oreilly.com/catalog/errata.csp?isbn=9781098153434 for release details.\nThe O’Reilly logo is a registered trademark of O’Reilly Media, Inc. Prompt Engineering for Generative AI,\nthe cover image, and related trade dress are trademarks of O’Reilly Media, Inc.\nThe views expressed in this work are those of the authors and do not represent the publisher’s views.\nWhile the publisher and the authors have used good faith efforts to ensure that the information and\ninstructions contained in this work are accurate, the publisher and the authors disclaim all responsibility\nfor errors or omissions, including without limitation responsibility for damages resulting from the use\nof or reliance on this work. Use of the information and instructions contained in this work is at your\nown risk. If any code samples or other technology this work contains or describes is subject to open\nsource licenses or the intellectual property rights of others, it is your responsibility to ensure that your use\nthereof complies with such licenses and/or rights.Table of Contents\n1. The Five Principles of Prompting. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  1\nOverview of the Five Principles of Prompting                                                              4\n1. Give Direction                                                                                                                8\n2. Specify Format                                                                                                              14\n3. Provide Examples                                                                                                        17\n4. Evaluate Quality                                                                                                           20\n5. Divide Labor                                                                                                                 32\nSummary                                                                                                                           38\niiiCHAPTER 1\nThe Five Principles of Prompting\nPrompt engineering is the process of discovering prompts that reliably yield useful or\ndesired results.\nA prompt is the input you provide, typically text, when interfacing with an AI model\nlike ChatGPT or Midjourney. The prompt serves as a set of instructions the model\nuses to predict the desired response: text from large language models  (LLMs) like\nChatGPT, or images from diffusion models like Midjourney.\nHere is a simple example of a prompt input for a product name generator (inspired\nby one of OpenAI’s examples), and the resulting output from ChatGPT.\nInput:\nCan I have a list of product names for a pair of shoes that\ncan fit any foot size?\nOutput:\nCertainly! Here's a list of product names for a pair of shoes\nthat can fit any foot size:\nUniFit SoleStrides\nFlexiSize All-Fit Shoes\nOmniFit FootFlex\n...\nAnyStep AdaptiWalkers\nFlexiRange UniversalSteps\nPerfectGlide MultiFit Shoes\n1This is already a remarkable response for a naive prompt, which feels like magic\nbecause we got here with very little effort. As the state-of-the-art models improve,\nthe likelihood you will get good enough  results on your first try goes up. For any\nthrowaway interactions with an AI, where you don’t plan to do the same task again,\nthe naive approach is all you need.\nHowever, if you planned to put this prompt into production, you’ d benefit from\ninvesting more work into getting it right. Mistakes cost you money in terms of the\nfees OpenAI charges based on the length of the prompt and response, as well as\nthe time spent fixing mistakes. If you were building a product name generator with\nthousands of users, there are some obvious issues you’ d want attempt to fix:\nVague direction\nY ou’re not briefing the AI on what style of name you want, or what attributes it\nshould have. Do you want a single word or a concatenation? Can the words be\nmade up, or is it important that they’re in real English? Do you want the AI to\nemulate somebody you admire who is famous for great product names?\nUnformatted output\nY ou’re getting back a list of separated names line by line, of unspecified length.\nWhen you run this prompt multiple times, you’ll see sometimes it comes back\nwith a numbered list, and often it has text at the beginning, which makes it hard\nto parse programmatically.\nMissing examples\nY ou haven’t given the AI any examples of what good names look like. It’s auto‐\ncompleting using an average of its training data, i.e., the entire internet (with all\nits inherent bias), but is that what you want? Ideally you’ d feed it examples of\nsuccessful names, common names in an industry, or even just other names you\nlike.\nLimited evaluation\nY ou have no consistent or scalable way to define which names are good or bad, so\nyou have to manually review each response. If you can institute a rating system\nor other form of measurement, you can optimize the prompt to get better results\nand identify how many times it fails.\nNo task division\nY ou’re asking a lot of a single prompt here: there are lots of factors that go into\nproduct naming, and this important task is being naively outsourced to the AI all\nin one go, with no task specialization or visibility into how it’s handling this task\nfor you.\n2 | Chapter 1: The Five Principles of PromptingAddressing these problems is the basis for the core principles we use throughout\nthis book. There are many different ways to ask an AI model to do the same task,\nand even slight changes can make a big difference. LLMs work by continuously\npredicting the next token (approximately three-fourths of a word), starting from what\nwas in your prompt. Each new token is selected based on its probability of appearing\nnext, with an element of randomness (controlled by the temperature parameter). As\ndemonstrated in Figure 1-1, the word shoes had a lower probability of coming after\nthe start of the name AnyFit (0.88%), where a more predictable response would be\nAthletic (72.35%).\nFigure 1-1. How the response breaks down into tokens\nLLMs are trained on essentially the entire text of the internet, and are then further\nfine-tuned to give helpful responses. Average prompts will return average responses,\nleading some to be underwhelmed when their results don’t live up to the hype.\nWhat you put in your prompt changes the probability of every word generated, so\nit matters a great deal to the results you’ll get. These models have seen the best and\nworst of what humans have produced and are capable of emulating almost anything\nif you know the right way to ask. OpenAI charges based on the number of tokens\nThe Five Principles of Prompting | 3used in the prompt and the response, so prompt engineers need to make these tokens\ncount by optimizing prompts for cost, quality, and reliability.\nHere’s the same example with the application of several prompt engineering tech‐\nniques. We ask for names in the style of Steve Jobs, state that we want a comma-\nseparated list, and supply examples of the task done well.\nInput:\nBrainstorm a list of product names for a shoe that fits any\nfoot size, in the style of Steve Jobs.\nReturn the results as a comma-separated list, in this format:\nProduct description: A shoe that fits any foot size\nProduct names: [list of 3 product names]\n## Examples\nProduct description: A refrigerator that dispenses beer\nProduct names: iBarFridge, iFridgeBeer, iDrinkBeerFridge\nProduct description: A watch that can tell accurate time in\nspace\nProduct names: iNaut, iSpace, iTime\nProduct description: A home milkshake maker\nProduct names: iShake, iSmoothie, iShake Mini\nOutput:\nProduct description: A shoe that fits any foot size\nProduct names: iFitFoot, iPerfectFit, iShoeSize\nWhile no prompt is ever perfect, this prompt is optimized to reliably deliver solid\nproduct names in the right format. The user of your product name generator can\nchoose somebody other than Steve Jobs to get the types of names they like, they can\nchange the response format if needed, and the output of this prompt can become the\ninput of another. Finally, you could periodically update the examples you use in the\nprompt based on user feedback, making your system smarter over time.\nOverview of the Five Principles of Prompting\nThe process for optimizing this prompt follows the Five Principles of Prompting ,\nwhich we will dissect using this example in the remainder of this chapter, and recall\nthroughout the book. They map exactly to the five issues we raised when discussing\nthe naive text prompt. Y ou’ll find references back to these principles throughout the\nrest of the book to help you connect the dots to how they’re used in practice. The Five\nPrinciples of Prompting are as follows:\n4 | Chapter 1: The Five Principles of PromptingGive Direction\nDescribe the desired style in detail, or reference a relevant persona\nSpecify Format\nDefine what rules to follow, and the required structure of the response\nProvide Examples\nInsert a diverse set of test cases where the task was done correctly\nEvaluate Quality\nIdentify errors and rate responses, testing what drives performance.\nDivide Labor\nSplit tasks into multiple steps, chained together for complex goals\nThese principles are not short-lived tips or hacks but are generally accepted conven‐\ntions that are useful for working with any level of intelligence, biological or artificial.\nThese principles are model-agnostic and should work to improve your prompt no\nmatter which generative text or image model you’re using. We first published these\nprinciples in July 2022 in the blog post “Prompt Engineering: From Words to Art\nand Copy” , and they have stood the test of time, including mapping quite closely\nto OpenAI’s own Prompt Engineering Guide , which came a year later. Anyone who\nworks closely with generative AI models is likely to converge on a similar set of\nstrategies for solving common issues, and throughout this book you’ll see hundreds\nof demonstrative examples of how they can be useful for improving your prompts.\nWe have provided downloadable one-pagers for text and image generation you can\nuse as a checklist when applying these principles. These were created for our popu‐\nlar Udemy course The Complete Prompt Engineering for AI Bootcamp  (70,000+\nstudents), which was based on the same principles but with different material to this\nbook.\n• Text Generation One-Pager•\n• Image Generation One-Pager•\nTo show these principles apply equally well to prompting image models, let’s use\nthe following example, and explain how to apply each of the Five Principles of\nPrompting to this specific scenario. Copy and paste the entire input prompt into the\nMidjourney Bot in Discord, including the link to the image at the beginning, after\ntyping /imagine to trigger the prompt box to appear (requires a free Discord account,\nand a paid Midjourney account).\nOverview of the Five Principles of Prompting | 5Input:\nhttps://s.mj.run/TKAsyhNiKmc stock photo of business meeting\nof 4 people watching on white MacBook on top of glass-top\ntable, Panasonic, DC-GH5\nFigure 1-2 shows the output.\nFigure 1-2. Stock photo of business meeting\nThis prompt takes advantage of Midjourney’s ability to take a base image as an\nexample by uploading the image to Discord and then copy and pasting the URL into\nthe prompt ( https://s.mj.run/TKAsyhNiKmc), for which the royalty-free image from\nUnsplash is used (Figure 1-3). If you run into an error with the prompt, try uploading\nthe image yourself and reviewing Midjourney’s documentation for any formatting\nchanges.\n6 | Chapter 1: The Five Principles of PromptingFigure 1-3. Photo by Mimi Thian on Unsplash\nLet’s compare this well-engineered prompt to what you get back from Midjourney if\nyou naively ask for a stock photo in the simplest way possible. Figure 1-4 shows an\nexample of what you get without prompt engineering, an image with a darker, more\nstylistic take on a stock photo than you’ d typically expect.\nInput:\npeople in a business meeting\nFigure 1-4 shows the output.\nAlthough less prominent an issue in v5 of Midjourney onwards, community feedback\nmechanisms (when users select an image to resize to a higher resolution, that choice\nmay be used to train the model) have reportedly biased the model toward a fantasy\naesthetic, which is less suitable for the stock photo use case. The early adopters of\nMidjourney came from the digital art world and naturally gravitated toward fantasy\nand sci-fi styles, which can be reflected in the results from the model even when this\naesthetic is not suitable.\nOverview of the Five Principles of Prompting | 7Figure 1-4. People in a business meeting\nThroughout this book the examples used will be compatiable with ChatGPT Plus\n(GPT-4) as the text model and Midjourney v6 or Stable Diffusion XL as the image\nmodel, though we will specify if it’s important. These foundational models are the\ncurrent state of the art and are good at a diverse range of tasks. The principles are\nintended to be future-proof as much as is possible, so if you’re reading this book\nwhen GPT-5, Midjourney v7, or Stable Diffusion XXL is out, or if you’re using\nanother vendor like Google, everything you learn here should still prove useful.\n1. Give Direction\nOne of the issues with the naive text prompt discussed earlier was that it wasn’t\nbriefing the AI on what types of product names you wanted. To some extent, naming\na product is a subjective endeavor, and without giving the AI an idea of what names\nyou like, it has a low probability of guessing right.\n8 | Chapter 1: The Five Principles of PromptingBy the way, a human would also struggle to complete this task without a good brief,\nwhich is why creative and branding agencies require a detailed briefing on any task\nfrom their clients.\nAlthough it’s not a perfect mapping, it can be helpful to imagine\nwhat context a human might need for this task and try including it\nin the prompt.\nIn the example prompt we gave direction through the use of role-playing, in that case\nemulating the style of Steve Jobs, who was famous for iconically naming products. If\nyou change this aspect of the prompt to someone else who is famous in the training\ndata (as well as matching the examples to the right style), you’ll get dramatically\ndifferent results.\nInput:\nBrainstorm a list of product names for a shoe that fits any\nfoot size, in the style of Elon Musk.\nReturn the results as a comma-separated list, in this format:\nProduct description: A shoe that fits any foot size\nProduct names: [list of 3 product names]\n## Examples\nProduct description: A refrigerator that dispenses beer\nProduct names: Teslacool, FridgeX, Neuradrink\nProduct description: A watch that can tell accurate time in\nspace\nProduct names: WatchX, iSpace, The Watch Company\nProduct description: A home milkshake maker\nProduct names: ShakeX, The Milkshake Company, HyperShake\nOutput:\nShoeX, MarsFit, The Shoe Company\nThere are also some rules or best practices you would do well to follow, which could\nbe included in the prompt as context to guide the AI toward a name that works.\nThis technique is sometimes referred to as prewarming or internal retrieval, and it\nis simple but effective ( Liu et al., 2021 ). Starting the conversation asking for best\npractice advice, then asking it to follow its own advice, can help a lot. In effect, you\nare using it to generate its own direction.\n1. Give Direction | 9Input:\nPlease give me 5 tips for naming products based on expert\nindustry advice.\nOutput:\nCertainly! Naming a product is crucial as it often gives the\nfirst impression to potential customers. Here are five expert\ntips to guide you in naming products:\n1. **Simplicity & Memorability**: Opt for names that are\nshort, easy to spell, and memorable. Long, complicated names\ncan be difficult for customers to recall. Remember, the\neasier the name is to remember, the more likely it is that\nconsumers will think of your product when making a purchase\ndecision.\n...\n5. **Positive Connotations**: Names that evoke positive\nemotions or imagery can create a favorable perception of\nyour product. Avoid names that have negative or\ncontroversial associations in any of the primary languages\nor cultures where you intend to do business. It's often\nworthwhile to conduct linguistic checks and market testing\nto ensure there aren't unintended meanings.\nLastly, once you've shortlisted a few names, it's a good\nidea to run them by diverse focus groups or conduct surveys\nto gauge reactions. Also, ensure that the name you choose is\navailable for trademarking and doesn't infringe on existing\ntrademarks.\nThen within the same chat window, where the model has the context of the past\nadvice it gave, you ask your initial prompt for the task you wanted to complete.\nInput:\nUsing this advice, can I have a list of product names for a\npair of shoes that can fit any foot size?\nOutput:\nCertainly! Based on the criteria given:\n1. **Simplicity & Memorability**\n2. **Relevance & Descriptiveness**\n3. **Differentiation & Uniqueness**\n4. **Adaptable & Scalable**\n5. **Positive Connotations**\nHere's a list of potential product names for shoes that can\nfit any foot size:\n10 | Chapter 1: The Five Principles of Prompting1. **FitAll Walkers**: A simple and memorable name that\ninstantly suggests a universal fit.\n...\n10. **TrueFit Universals**: Emphasizing genuine comfort and\nfit for all.\nBefore finalizing any name, remember to check for trademarks\nto ensure there's no conflict with existing products or\nbrands.\nAnother fruitful strategy in our experience is to take the best advice out there for the\ntask you want to accomplish and insert that context into the prompt. For example,\nyou could take Brandwatch’s 5 Golden Rules for naming a product or another trusted\nexternal resource you find, and insert that as context into the prompt. This will\nincrease the length of the prompt significantly, which costs more money (when using\nthe API as a developer), but may be worth the trade-off if the quality of the response\nimproves.\nInput:\nCan I have a list of product names for a pair of shoes that\ncan fit any foot size? Please follow the five rules below:\n1. It should be readable and writable.\nIf your product name is hard to pronounce, people won’t talk about it,\nand if they can’t write it down (and spell it correctly!)\nwhen they hear it, how do you expect them to Google it?\nKeep it simple and don’t go with any wacky spellings just for the sake of it.\n...\n5. It should evoke an emotion, feeling, or idea.\nYour product name should tie back into what your product is, what the\nfeeling you want people to have when experiencing your product is, and/or\nwhat idea are you trying to get across.\nIt should be emotive and inspiring.\nOutput:\n1. FlexFit Footwear\n2. OmniSize Sneakers\n3. AdaptStride Shoes\n...\n18. OmniComfort Kicks\n19. FlexSize Footwear\n20. Boundless Soles\n1. Give Direction | 11There are other myriad ways of providing direction. In the image generation exam‐\nple, direction was given by specifying that the business meeting is taking place\naround a glass-top table. If you change only that detail, you can get a completely\ndifferent image, as detailed in Figure 1-5.\nInput:\nhttps://s.mj.run/TKAsyhNiKmc stock photo of business meeting\nof four people gathered around a campfire outdoors in the\nwoods, Panasonic, DC-GH5\nFigure 1-5 shows the output.\nFigure 1-5. Stock photo of business meeting in the woods\nRole-playing is also important for image generation, and one of the quite powerful\nways you can give Midjourney direction is to supply the name of an artist or art style\nto emulate. One artist that features heavily in the AI art world is Van Gogh, known\nfor his bold, dramatic brush strokes and vivid use of colors. Watch what happens\nwhen you include his name in the prompt, as shown in Figure 1-6.\n12 | Chapter 1: The Five Principles of PromptingInput:\npeople in a business meeting, by Van Gogh\nFigure 1-6 shows the output.\nFigure 1-6. People in a business meeting, by Van Gogh\nTo get that last prompt to work, you need to strip back a lot of the other direction.\nFor example, losing the base image and the words stock photo as well as the camera\nPanasonic, DC-GH5 helps bring in Van Gogh’s style. The problem you may run into\nis that often with too much direction, the model can quickly get to a conflicting\ncombination that it can’t resolve. If your prompt is overly specific, there might not be\nenough samples in the training data to generate an image that’s consistent with all of\nyour criteria. In cases like these, you should choose which element is more important\n(in this case, Van Gogh) and defer to that.\nDirection is one of the most commonly used and broadest principles. It can take the\nform of simply using the right descriptive words to clarify your intent, or channeling\nthe personas of relevant business celebrities. While too much direction can narrow\nthe creativity of the model, too little direction is the more common problem.\n1. Give Direction | 132. Specify Format\nAI models are universal translators. Not only does that mean translating from French\nto English, or Urdu to Klingon, but also between data structures like JSON to YAML,\nor natural language to Python code. These models are capable of returning a response\nin almost any format, so an important part of prompt engineering is finding ways to\nspecify what format you want the response to be in.\nEvery now and again you’ll find that the same prompt will return a different format,\nfor example, a numbered list instead of comma separated. This isn’t a big deal most of\nthe time, because most prompts are one-offs and typed into ChatGPT or Midjourney.\nHowever, when you’re incorporating AI tools into production software, occasional\nflips in format can cause all kinds of errors.\nJust like when working with a human, you can avoid wasted effort by specifying up\nfront the format you expect the response to be in. For text generation models, it can\noften be helpful to output JSON instead of a simple ordered list because that’s the\nuniversal format for API responses, which can make it simpler to parse and spot\nerrors, as well as to use to render the front-end HTML of an application. YAML is\nalso another popular choice because it enforces a parseable structure while still being\nsimple and human-readable.\nIn the original prompt you gave direction through both the examples provided, and\nthe colon at the end of the prompt indicated it should complete the list inline. To\nswap the format to JSON, you need to update both and leave the JSON uncompleted,\nso GPT-4 knows to complete it.\nInput:\nReturn a comma-separated list of product names in JSON for\n\"A pair of shoes that can fit any foot size.\".\nReturn only JSON.\nExamples:\n[{\n\"Product description\": \"A home milkshake maker.\",\n\"Product names\": [\"HomeShaker\", \"Fit Shaker\",\n\"QuickShake\", \"Shake Maker\"]\n},\n{\n\"Product description\": \"A watch that can tell\naccurate time in space.\",\n\"Product names\": [\"AstroTime\", \"SpaceGuard\",\n\"Orbit-Accurate\", \"EliptoTime\"]}\n]\n14 | Chapter 1: The Five Principles of PromptingOutput:\n[\n{\n\"Product description\": \"A pair of shoes that can \\\nfit any foot size.\",\n\"Product names\": [\"FlexFit Footwear\", \"OneSize Step\",\n\"Adapt-a-Shoe\", \"Universal Walker\"]\n}\n]\nThe output we get back is the completed JSON containing the product names. This\ncan then be parsed and used programmatically, in an application or local script.\nIt’s also easy from this point to check if there’s an error in the formatting using a\nJSON parser like Python’s standard json library, because broken JSON will result in\na parsing error, which can act as a trigger to retry the prompt or investigate before\ncontinuing. If you’re still not getting the right format back, it can help to specify at the\nbeginning or end of the prompt, or in the system message if using a chat model: You\nare a helpful assistant that only responds in JSON , or specify JSON output\nin the model parameters where available (this is called grammars with Llama models.\nTo get up to speed on JSON if you’re unfamiliar, W3Schools has a\ngood introduction.\nFor image generation models, format is very important, because the opportunities for\nmodifying an image are near endless. They range from obvious formats like stock\nphoto, illustration, and oil painting, to more unusual formats like dashcam\nfootage, ice sculpture, or in Minecraft (see Figure 1-7).\nInput:\nbusiness meeting of four people watching on MacBook on top of\ntable, in Minecraft\nFigure 1-7 shows the output.\n2. Specify Format | 15Figure 1-7. Business meeting in Minecraft\nWhen setting a format, it is often necessary to remove other aspects of the prompt\nthat might clash with the specified format. For example, if you supply a base image\nof a stock photo, the result is some combination of stock photo and the format you\nwanted. To some degree, image generation models can generalize to new scenarios\nand combinations they haven’t seen before in their training set, but in our experience,\nthe more layers of unrelated elements, the more likely you are to get an unsuitable\nimage.\nThere is often some overlap between the first and second principles, Give Direction\nand Specify Format. The latter is about defining what type of output you want, for\nexample JSON format, or the format of a stock photo. The former is about the style of\nresponse you want, independent from the format, for example product names in the\nstyle of Steve Jobs, or an image of a business meeting in the style of Van Gogh. When\nthere are clashes between style and format, it’s often best to resolve them by dropping\nwhichever element is less important to your final result.\n16 | Chapter 1: The Five Principles of Prompting3. Provide Examples\nThe original prompt didn’t give the AI any examples of what you think good names\nlook like. Therefore, the response is approximate to an average of the internet, and\nyou can do better than that. Researchers would call a prompt with no examples\nzero-shot, and it’s always a pleasant surprise when AI can even do a task zero shot:\nit’s a sign of a powerful model. If you’re providing zero examples, you’re asking for\na lot without giving much in return. Even providing one example ( one-shot) helps\nconsiderably, and it’s the norm among researchers to test how models perform with\nmultiple examples (few-shot). One such piece of research is the famous GPT-3 paper\n“Language Models are Few-Shot Learners” , the results of which are illustrated in\nFigure 1-8, showing adding one example along with a prompt can improve accuracy\nin some tasks from 10% to near 50%!\nFigure 1-8. Number of examples in context\nWhen briefing a colleague or training a junior employee on a new task, it’s only\nnatural that you’ d include examples of times that task had previously been done well.\nWorking with AI is the same, and the strength of a prompt often comes down to the\nexamples used. Providing examples can sometimes be easier than trying to explain\nexactly what it is about those examples you like, so this technique is most effective\nwhen you are not a domain expert in the subject area of the task you are attempting\nto complete. The amount of text you can fit in a prompt is limited (at the time of\nwriting around 6,000 characters on Midjourney and approximately 32,000 characters\nfor the free version of ChatGPT), so a lot of the work of prompt engineering involves\nselecting and inserting diverse and instructive examples.\n3. Provide Examples | 17There’s a trade-off between reliability and creativity: go past three to five examples\nand your results will become more reliable, while sacrificing creativity. The more\nexamples you provide, and the lesser the diversity between them, the more con‐\nstrained the response will be to match your examples. If you change all of the\nexamples to animal names in the previous prompt, you’ll have a strong effect on the\nresponse, which will reliably return only names including animals.\nInput:\nBrainstorm a list of product names for a shoe that fits any\nfoot size.\nReturn the results as a comma-separated list, in this format:\nProduct description: A shoe that fits any foot size\nProduct names: [list of 3 product names]\n## Examples:\nProduct description: A home milkshake maker.\nProduct names: Fast Panda, Healthy Bear, Compact Koala\nProduct description: A watch that can tell accurate time in\nspace.\nProduct names: AstroLamb, Space Bear, Eagle Orbit\nProduct description: A refrigerator that dispenses beer\nProduct names: BearFridge, Cool Cat, PenguinBox\nOutput:\nProduct description: A shoe that fits any foot size\nProduct names: FlexiFox, ChameleonStep, PandaPaws\nOf course this runs the risk of missing out on returning a much better name that\ndoesn’t fit the limited space left for the AI to play in. Lack of diversity and variation\nin examples is also a problem in handling edge cases, or uncommon scenarios.\nIncluding one to three examples is easy and almost always has a positive effect, but\nabove that number it becomes essential to experiment with the number of examples\nyou include, as well as the similarity between them. There is some evidence ( Hsieh\net al., 2023 ) that direction works better than providing examples, and it typically\nisn’t straightforward to collect good examples, so it’s usually prudent to attempt the\nprinciple of Give Direction first.\nIn the image generation space, providing examples usually comes in the form of\nproviding a base image in the prompt, called img2img in the open source Stable\nDiffusion community. Depending on the image generation model being used, these\nimages can be used as a starting point for the model to generate from, which greatly\naffects the results. Y ou can keep everything about the prompt the same but swap out\nthe provided base image for a radically different effect, as in Figure 1-9.\n18 | Chapter 1: The Five Principles of PromptingInput:\nstock photo of business meeting of 4 people watching on\nwhite MacBook on top of glass-top table, Panasonic, DC-GH5\nFigure 1-9 shows the output.\nFigure 1-9. Stock photo of business meeting of four people\nIn this case, by substituting for the image shown in Figure 1-10, also from Unsplash,\nyou can see how the model was pulled in a different direction and incorporates\nwhiteboards and sticky notes now.\nThese examples demonstrate the capabilities of image generation\nmodels, but we would exercise caution when uploading base\nimages for use in prompts. Check the licensing of the image you\nplan to upload and use in your prompt as the base image, and\navoid using clearly copyrighted images. Doing so can land you in\nlegal trouble and is against the terms of service for all the major\nimage generation model providers.\n3. Provide Examples | 19Figure 1-10. Photo by Jason Goodman on Unsplash\n4. Evaluate Quality\nAs of yet, there has been no feedback loop to judge the quality of your responses,\nother than the basic trial and error of running the prompt and seeing the results,\nreferred to as blind prompting. This is fine when your prompts are used temporarily\nfor a single task and rarely revisited. However, when you’re reusing the same prompt\nmultiple times or building a production application that relies on a prompt, you need\nto be more rigorous with measuring results.\nThere are a number of ways performance can be evaluated, and it depends largely\non what tasks you’re hoping to accomplish. When a new AI model is released, the\nfocus tends to be on how well the model did on evals (evaluations), a standardized set\nof questions with predefined answers or grading criteria that are used to test perfor‐\nmance across models. Different models perform differently across different types of\ntasks, and there is no guarantee a prompt that worked previously will translate well to\na new model. OpenAI has made its evals framework  for benchmarking performance\nof LLMs open source and encourages others to contribute additional eval templates.\nIn addition to the standard academic evals, there are also more headline-worthy tests\nlike GPT-4 passing the bar exam. Evaluation is difficult for more subjective tasks, and\ncan be time-consuming or prohibitively costly for smaller teams. In some instances\nresearchers have turned to using more advanced models like GPT-4 to evaluate\nresponses from less sophisticated models, as was done with the release of Vicuna-13B,\na fine-tuned model based on Meta’s Llama open source model (see Figure 1-11).\n20 | Chapter 1: The Five Principles of PromptingFigure 1-11. Vicuna GPT-4 Evals\nMore rigorous evaluation techniques are necessary when writing scientific papers or\ngrading a new foundation model release, but often you will only need to go just one\nstep above basic trial and error. Y ou may find that a simple thumbs-up/thumbs-down\nrating system implemented in a Jupyter Notebook can be enough to add some rigor\nto prompt optimization, without adding too much overhead. One common test is\nto see whether providing examples is worth the additional cost in terms of prompt\nlength, or whether you can get away with providing no examples in the prompt. The\nfirst step is getting responses for multiple runs of each prompt and storing them in a\nspreadsheet, which we will do after setting up our environment.\nY ou can install the OpenAI Python package with pip install openai . If you’re\nrunning into compatability issues with this package, create a virtual environment and\ninstall our requirements.txt (instructions in the preface).\nTo utilize the API, you’ll need to create an OpenAI account  and then navigate here\nfor your API key.\nHardcoding API keys in scripts is not recommended due to secu‐\nrity reasons. Instead, utilize environment variables or configuration\nfiles to manage your keys.\nOnce you have an API key, it’s crucial to assign it as an environment variable by\nexecuting the following command, replacing api_key with your actual API key value:\nexport OPENAI_API_KEY=\"api_key\"\nOr on Windows:\nset OPENAI_API_KEY=api_key\n4. Evaluate Quality | 21Alternatively, if you’ d prefer not to preset an API key, then you can manually set the\nkey while initializing the model, or load it from an .env file using python-dotenv. First,\ninstall the library with pip install python-dotenv, and then load the environment\nvariables with the following code at the top of your script or notebook:\nfrom dotenv import load_dotenv\nload_dotenv()  # take environment variables from .env.\nThe first step is getting responses for multiple runs of each prompt and storing them\nin a spreadsheet.\nInput:\n# Define two variants of the prompt to test zero-shot\n# vs few-shot\nprompt_A = \"\"\"Product description: A pair of shoes that can\nfit any foot size.\nSeed words: adaptable, fit, omni-fit.\nProduct names:\"\"\"\nprompt_B = \"\"\"Product description: A home milkshake maker.\nSeed words: fast, healthy, compact.\nProduct names: HomeShaker, Fit Shaker, QuickShake, Shake\nMaker\nProduct description: A watch that can tell accurate time in\nspace.\nSeed words: astronaut, space-hardened, eliptical orbit\nProduct names: AstroTime, SpaceGuard, Orbit-Accurate,\nEliptoTime.\nProduct description: A pair of shoes that can fit any foot\nsize.\nSeed words: adaptable, fit, omni-fit.\nProduct names:\"\"\"\ntest_prompts = [prompt_A, prompt_B]\nimport pandas as pd\nfrom openai import OpenAI\nimport os\n# Set your OpenAI key as an environment variable\n# https://platform.openai.com/api-keys\nclient = OpenAI(\n  api_key=os.environ['OPENAI_API_KEY'],  # Default\n)\ndef get_response(prompt):\n    response = client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n22 | Chapter 1: The Five Principles of Prompting        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"You are a helpful assistant.\"\n            },\n            {\n                \"role\": \"user\",\n                \"content\": prompt\n            }\n        ]\n    )\n    return response.choices[0].message.content\n# Iterate through the prompts and get responses\nresponses = []\nnum_tests = 5\nfor idx, prompt in enumerate(test_prompts):\n    # prompt number as a letter\n    var_name = chr(ord('A') + idx)\n    for i in range(num_tests):\n        # Get a response from the model\n        response = get_response(prompt)\n        data = {\n            \"variant\": var_name,\n            \"prompt\": prompt,\n            \"response\": response\n            }\n        responses.append(data)\n# Convert responses into a dataframe\ndf = pd.DataFrame(responses)\n# Save the dataframe as a CSV file\ndf.to_csv(\"responses.csv\", index=False)\nprint(df)\nOutput:\n  variant                                             prompt\n  \\\n0       A  Product description: A pair of shoes that can ...\n1       A  Product description: A pair of shoes that can ...\n2       A  Product description: A pair of shoes that can ...\n3       A  Product description: A pair of shoes that can ...\n4       A  Product description: A pair of shoes that can ...\n5       B  Product description: A home milkshake maker.\\n...\n6       B  Product description: A home milkshake maker.\\n...\n7       B  Product description: A home milkshake maker.\\n...\n8       B  Product description: A home milkshake maker.\\n...\n4. Evaluate Quality | 239       B  Product description: A home milkshake maker.\\n...\n                                            response\n0  1. Adapt-a-Fit Shoes \\n2. Omni-Fit Footwear \\n...\n1  1. OmniFit Shoes\\n2. Adapt-a-Sneaks \\n3. OneFi...\n2  1. Adapt-a-fit\\n2. Flexi-fit shoes\\n3. Omni-fe...\n3  1. Adapt-A-Sole\\n2. FitFlex\\n3. Omni-FitX\\n4. ...\n4  1. Omni-Fit Shoes\\n2. Adapt-a-Fit Shoes\\n3. An...\n5  Adapt-a-Fit, Perfect Fit Shoes, OmniShoe, OneS...\n6       FitAll, OmniFit Shoes, SizeLess, AdaptaShoes\n7       AdaptaFit, OmniShoe, PerfectFit, AllSizeFit.\n8  FitMaster, AdaptoShoe, OmniFit, AnySize Footwe...\n9        Adapt-a-Shoe, PerfectFit, OmniSize, FitForm\nHere we’re using the OpenAI API to generate model responses to a set of prompts\nand storing the results in a dataframe, which is saved to a CSV file. Here’s how it\nworks:\n1. Two prompt variants are defined, and each variant consists of a product descrip‐1.\ntion, seed words, and potential product names, but prompt_B provides two\nexamples.\n2. Import statements are called for the Pandas library, OpenAI library, and os2.\nlibrary.\n3. The get_response function takes a prompt as input and returns a response from3.\nthe gpt-3.5-turbo model. The prompt is passed as a user message to the model,\nalong with a system message to set the model’s behavior.\n4. Two prompt variants are stored in the test_prompts list.4.\n5. An empty list responses is created to store the generated responses, and the5.\nvariable num_tests is set to 5.\n6. A nested loop is used to generate responses. The outer loop iterates over each6.\nprompt, and the inner loop generates num_tests (five in this case) number of\nresponses per prompt.\na. The enumerate function is used to get the index and value of each prompta.\nin test_prompts. This index is then converted to a corresponding uppercase\nletter (e.g., 0 becomes A, 1 becomes B) to be used as a variant name.\nb. For each iteration, the get_response function is called with the currentb.\nprompt to generate a response from the model.\nc. A dictionary is created with the variant name, the prompt, and the model’sc.\nresponse, and this dictionary is appended to the responses list.\n7. Once all responses have been generated, the responses list (which is now a list of7.\ndictionaries) is converted into a Pandas DataFrame.\n24 | Chapter 1: The Five Principles of Prompting8. This dataframe is then saved to a CSV file with the Pandas built-in to_csv8.\nfunction, making the file responses.csv with index=False so as to not write row\nindices.\n9. Finally, the dataframe is printed to the console.9.\nHaving these responses in a spreadsheet is already useful, because you can see right\naway even in the printed response that prompt_A (zero-shot) in the first five rows is\ngiving us a numbered list, whereas prompt_B (few-shot) in the last five rows tends to\noutput the desired format of a comma-separated inline list. The next step is to give\na rating on each of the responses, which is best done blind and randomized to avoid\nfavoring one prompt over another.\nInput:\nimport ipywidgets as widgets\nfrom IPython.display import display\nimport pandas as pd\n# load the responses.csv file\ndf = pd.read_csv(\"responses.csv\")\n# Shuffle the dataframe\ndf = df.sample(frac=1).reset_index(drop=True)\n# df is your dataframe and 'response' is the column with the\n# text you want to test\nresponse_index = 0\n# add a new column to store feedback\ndf['feedback'] = pd.Series(dtype='str')\ndef on_button_clicked(b):\n    global response_index\n    #  convert thumbs up / down to 1 / 0\n    user_feedback = 1 if b.description == \"\\U0001F44D\" else 0\n    # update the feedback column\n    df.at[response_index, 'feedback'] = user_feedback\n    response_index += 1\n    if response_index < len(df):\n        update_response()\n    else:\n        # save the feedback to a CSV file\n        df.to_csv(\"results.csv\", index=False)\n        print(\"A/B testing completed. Here's the results:\")\n        # Calculate score and num rows for each variant\n        summary_df = df.groupby('variant').agg(\n            count=('feedback', 'count'),\n            score=('feedback', 'mean')).reset_index()\n4. Evaluate Quality | 25        print(summary_df)\ndef update_response():\n    new_response = df.iloc[response_index]['response']\n    if pd.notna(new_response):\n        new_response = \"<p>\" + new_response + \"</p>\"\n    else:\n        new_response = \"<p>No response</p>\"\n    response.value = new_response\n    count_label.value = f\"Response: {response_index + 1}\"\n    count_label.value += f\"/{len(df)}\"\nresponse = widgets.HTML()\ncount_label = widgets.Label()\nupdate_response()\nthumbs_up_button = widgets.Button(description='\\U0001F44D')\nthumbs_up_button.on_click(on_button_clicked)\nthumbs_down_button = widgets.Button(\n    description='\\U0001F44E')\nthumbs_down_button.on_click(on_button_clicked)\nbutton_box = widgets.HBox([thumbs_down_button,\nthumbs_up_button])\ndisplay(response, button_box, count_label)\nThe output is shown in Figure 1-12:\nFigure 1-12. Thumbs-up/thumbs-down rating system\nIf you run this in a Jupyter Notebook, a widget displays each AI response, with a\nthumbs-up or thumbs-down button (see Figure 1-12) This provides a simple inter‐\nface for quickly labeling responses, with minimal overhead. If you wish to do this\noutside of a Jupyter Notebook, you could change the thumbs-up and thumbs-down\nemojis for Y and N, and implement a loop using the built-in input() function, as a\ntext-only replacement for iPyWidgets.\nOnce you’ve finished labeling the responses, you get the output, which shows you\nhow each prompt performs.\n26 | Chapter 1: The Five Principles of PromptingOutput:\nA/B testing completed. Here's the results:\n  variant  count  score\n0       A      5    0.2\n1       B      5    0.6\nThe dataframe was shuffled at random, and each response was labeled blind (without\nseeing the prompt), so you get an accurate picture of how often each prompt per‐\nformed. Here is the step-by-step explanation:\n1. Three modules are imported: ipywidgets, IPython.display, and pandas. ipy1.\nwidgets contains interactive HTML widgets for Jupyter Notebooks and the\nIPython kernel. IPython.display provides classes for displaying various types\nof output like images, sound, displaying HTML, etc. Pandas is a powerful data\nmanipulation library.\n2. The pandas library is used to read in the CSV file responses.csv, which contains2.\nthe responses you want to test. This creates a Pandas DataFrame called df.\n3. df is shuffled using the sample() function with frac=1, which means it uses3.\nall the rows. The reset_index(drop=True) is used to reset the indices to the\nstandard 0, 1, 2, …, n index.\n4. The script defines response_index as 0. This is used to track which response4.\nfrom the dataframe the user is currently viewing.\n5. A new column feedback is added to the dataframe df with the data type as str5.\nor string.\n6. Next, the script defines a function on_button_clicked(b), which will execute6.\nwhenever one of the two buttons in the interface is clicked.\na. The function first checks the description of the button clicked was thea.\nthumbs-up button (\\U0001F44D; \n ), and sets user_feedback as 1, or if it was\nthe thumbs-down button (\\U0001F44E \n ), it sets user_feedback as 0.\nb. Then it updates the feedback column of the dataframe at the currentb.\nresponse_index with user_feedback.\nc. After that, it increments response_index to move to the next response.c.\nd. If response_index is still less than the total number of responses (i.e., thed.\nlength of the dataframe), it calls the function update_response().\ne. If there are no more responses, it saves the dataframe to a new CSV filee.\nresults.csv, then prints a message, and also prints a summary of the results by\nvariant, showing the count of feedback received and the average score (mean)\nfor each variant.\n4. Evaluate Quality | 277. The function update_response() fetches the next response from the dataframe,7.\nwraps it in paragraph HTML tags (if it’s not null), updates the response widget\nto display the new response, and updates the count_label widget to reflect the\ncurrent response number and total number of responses.\n8. Two widgets, response (an HTML widget) and count_label (a Label widget),8.\nare instantiated. The update_response() function is then called to initialize\nthese widgets with the first response and the appropriate label.\n9. Two more widgets, thumbs_up_button and thumbs_down_button (both Button9.\nwidgets), are created with thumbs-up and thumbs-down emoji as their descrip‐\ntions, respectively. Both buttons are configured to call the on_button_clicked()\nfunction when clicked.\n10. The two buttons are grouped into a horizontal box ( button_box) using the HBox10.\nfunction.\n11. Finally, the response, button_box, and count_label widgets are displayed to the11.\nuser using the display() function from the IPython.display module.\nA simple rating system such as this one can be useful in judging prompt quality\nand encountering edge cases. Usually in less than 10 test runs of a prompt you\nuncover a deviation, which you otherwise wouldn’t have caught until you started\nusing it in production. The downside is that it can get tedious rating lots of responses\nmanually, and your ratings might not represent the preferences of your intended\naudience. However, even small numbers of tests can reveal large differences between\ntwo prompting strategies and reveal nonobvious issues before reaching production.\nIterating on and testing prompts can lead to radical decreases in the length of\nthe prompt and therefore the cost and latency of your system. If you can find\nanother prompt that performs equally as well (or better) but uses a shorter prompt,\nyou can afford to scale up your operation considerably. Often you’ll find in this\nprocess that many elements of a complex prompt are completely superfluous, or even\ncounterproductive.\nThe thumbs-up or other manually labeled indicators of quality don’t have to be the\nonly judging criteria. Human evaluation is generally considered to be the most accu‐\nrate form of feedback. However, it can be tedious and costly to rate many samples\nmanually. In many cases, as in math or classification use cases, it may be possible to\nestablish ground truth (reference answers to test cases) to programmatically rate the\nresults, allowing you to scale up considerably your testing and monitoring efforts.\nThe following is not an exhaustive list because there are many motivations for\nevaluating your prompt programmatically:\n28 | Chapter 1: The Five Principles of PromptingCost\nPrompts that use a lot of tokens, or work only with more expensive models,\nmight be impractical for production use.\nLatency\nEqually the more tokens there are, or the larger the model required, the longer it\ntakes to complete a task, which can harm user experience.\nCalls\nMany AI systems require multiple calls in a loop to complete a task, which can\nseriously slow down the process.\nPerformance\nImplement some form of external feedback system, for example a physics engine\nor other model for predicting real-world results.\nClassification\nDetermine how often a prompt correctly labels given text, using another AI\nmodel or rules-based labeling.\nReasoning\nWork out which instances the AI fails to apply logical reasoning or gets the math\nwrong versus reference cases.\nHallucinations\nSee how frequently you encouner hallucinations, as measured by invention of\nnew terms not included in the prompt’s context.\nSafety\nFlag any scenarios where the system might return unsafe or undesirable results\nusing a safety filter or detection system.\nRefusals\nFind out how often the system incorrectly refuses to fulfill a reasonable user\nrequest by flagging known refusal language.\nAdversarial\nMake the prompt robust against known prompt injection attacks that can get the\nmodel to run undesirable prompts instead of what you programmed.\nSimilarity\nUse shared words and phrases ( BLEU or ROGUE ) or vector distance (explained\nin Chapter 5) to measure similarity between generated and reference text.\n4. Evaluate Quality | 29Once you start rating which examples were good, you can more easily update the\nexamples used in your prompt as a way to continuously make your system smarter\nover time. The data from this feedback can also feed into examples for fine-tuning,\nwhich starts to beat prompt engineering once you can supply a few thousand exam‐\nples, as shown in Figure 1-13.\nFigure 1-13. How many data points is a prompt worth?\nGraduating from thumbs-up or thumbs-down, you can implement a 3-, 5-, or 10-\npoint rating system to get more fine-grained feedback on the quality of your prompts.\nIt’s also possible to determine aggregate relative performance through comparing\nresponses side by side, rather than looking at responses one at a time. From this you\ncan construct a fair across-model comparison using an Elo rating, as is popular in\nchess and used in the Chatbot Arena by lmsys.org.\nFor image generation, evaluation usually takes the form of permutation prompting,\nwhere you input multiple directions or formats and generate an image for each\n30 | Chapter 1: The Five Principles of Promptingcombination. Images can than be scanned or later arranged in a grid to show the\neffect that different elements of the prompt can have on the final image.\nInput:\n{stock photo, oil painting, illustration} of business\nmeeting of {four, eight} people watching on white MacBook on\ntop of glass-top table\nIn Midjourney this would be compiled into six different prompts, one for every\ncombination of the three formats (stock photo, oil painting, illustration) and two\nnumbers of people (four, eight).\nInput:\n1. stock photo of business meeting of four people watching\non white MacBook on top of glass-top table\n2. stock photo of business meeting of eight people watching\non white MacBook on top of glass-top table\n3. oil painting of business meeting of four people watching\non white MacBook on top of glass-top table\n4. oil painting of business meeting of eight people watching\non white MacBook on top of glass-top table\n5. illustration of business meeting of four people watching\non white MacBook on top of glass-top table\n6. illustration of business meeting of eight people watching\non white MacBook on top of glass-top table\nEach prompt generates its own four images as usual, which makes the output a little\nharder to see. We have selected one from each prompt to upscale and then put them\ntogether in a grid, shown as Figure 1-14. Y ou’ll notice that the model doesn’t always\nget the correct number of people (generative AI models are surprisingly bad at math),\nbut it has correctly inferred the general intention by adding more people to the\nphotos on the right than the left.\nFigure 1-14 shows the output.\n4. Evaluate Quality | 31Figure 1-14. Prompt permutations grid\nWith models that have APIs like Stable Diffusion, you can more easily manipulate the\nphotos and display them in a grid format for easy scanning. Y ou can also manipulate\nthe random seed of the image to fix a style in place for maximum reproducibility.\nWith image classifiers it may also be possible to programmatically rate images based\non their safe content, or if they contain certain elements associated with success or\nfailure.\n5. Divide Labor\nAs you build out your prompt, you start to get to the point where you’re asking a lot\nin a single call to the AI. When prompts get longer and more convoluted, you may\nfind the responses get less deterministic, and hallucinations or anomalies increase.\nEven if you manage to arrive at a reliable prompt for your task, that task is likely just\none of a number of interrelated tasks you need to do your job. It’s natural to start\nexploring how many other of these tasks could be done by AI and how you might\nstring them together.\n32 | Chapter 1: The Five Principles of PromptingOne of the core principles of engineering is to use task decomposition to break prob‐\nlems down into their component parts, so you can more easily solve each individual\nproblem and then reaggregate the results. Breaking your AI work into multiple calls\nthat are chained together can help you accomplish more complex tasks, as well as\nprovide more visibility into what part of the chain is failing.\nThere are lots of factors that go into product naming, and an important task is naively\noutsourced to the AI with no visibility into how it’s weighing the importance of these\nfactors (if at all). The way our current system works, we’re getting a list of names, but\nall names are displayed with equal importance with no further context for helping us\ndecide. Fortunately AI tools are capable of self-evaluation; if we add a second step to\nour task, we can automatically check for nondesirable outputs.\nInput:\nRate this list of product names for a pair of shoes that can\nfit any foot size. The rating should be out of 10, inline\nnext to the product name:\nUniFit SoleStrides\nFlexiSize All-Fit Shoes\nOmniFit FootFlex\n...\nAnyStep AdaptiWalkers\nFlexiRange UniversalSteps\nPerfectGlide MultiFit Shoes\nOutput:\nUniFit SoleStrides: 7/10\nFlexiSize All-Fit Shoes: 8/10\nOmniFit FootFlex: 7/10\n...\nAnyStep AdaptiWalkers: 7/10\nFlexiRange UniversalSteps: 7.5/10\nPerfectGlide MultiFit Shoes: 9/10\nThis ranking is based on clarity, memorability, and how well\nthe name communicates the unique selling point of the\nproduct (that they can fit any foot size). Remember, these\nratings are subjective and the ultimate success of a product\nname also depends on the target market, branding, and\nmarketing efforts.\nIn running this multiple times, it consistently rates the name “OneSize Glovewalkers”\nas the worst, providing context (if you ask) that the concept might be confusing in\na shoe context. Y ou may be wondering why, if the model knows this is a bad name,\ndoes it suggest it in the first place? LLMs work by predicting the next token in a\nsequence and therefore struggle to know what the overall response will be when\n5. Divide Labor | 33finished. However, when it has all the tokens from a previous response to review, it\ncan more easily predict whether this would be labeled as a good or bad response.\nWe can continue to chain multiple calls together to improve the results of our task.\nFor example, we could split this into three separate ratings: clarity, memorability, and\nhow well the name communicates the unique selling point of the product. These\nratings could then be given to a human as additional context on the final decision, or\neven calculated together to select the final name programmatically.\nThe real unlock in learning to work professionally with AI versus just playing around\nwith prompting is realizing that every part of the system can be broken down into a\nseries of iterative steps. Even with a single prompt this principles applies, as simply\nappending Let's think step by step to the prompt can lead to demonstrable gains\nin reasoning and proficiency, as well as provide an audit trail for quality assurance\nand debugging. When taking the time and tokens to reason, the ratings change and\nare more consistent with the scoring criteria.\nInput:\nLet's think step by step. Rate this list of product names\nfor a pair of shoes that can fit any foot size. The rating\nshould be out of 10, inline next to the product name:\nUniFit SoleStrides\nFlexiSize All-Fit Shoes\nOmniFit FootFlex\n...\nAnyStep AdaptiWalkers\nFlexiRange UniversalSteps\nPerfectGlide MultiFit Shoes\nOutput:\nHere's a rating out of 10 for each product name, considering\nfactors like descriptiveness, catchiness, and overall\nappeal:\n1. **UniFit SoleStrides**: 7/10 - Sounds unifying but may\nnot immediately convey the \"fit any size\" idea.\n2. **FlexiSize All-Fit Shoes**: 8.5/10 - Combines\nflexibility and the notion that it fits all sizes.\n3. **OmniFit FootFlex**: 8/10 - Omnipresence (fitting all)\nand flexibility nicely conveyed.\n...\n18. **AnyStep AdaptiWalkers**: 7.5/10 - \"AnyStep\" suggests\nversatility, but the overall name might be less direct.\n19. **FlexiRange UniversalSteps**: 8/10 - Combines\nflexibility and universal fit.\n20. **PerfectGlide MultiFit Shoes**: 8.5/10 - \"MultiFit\" is\nclear, and \"PerfectGlide\" suggests a comfortable experience.\n34 | Chapter 1: The Five Principles of PromptingRemember, the best name can also be influenced by your\ntarget audience, branding strategy, and the overall message\nyou want to convey.\nOpenAI calls this  “giving the model time to think, ” and it is a key tenet of prompt\nengineering. In effect, chain of thought  techniques like this, where the model is\nencouraged to list out its steps, are like dividing a task within the same prompt. Once\nwe’ve automated product naming given a product idea, we can call ChatGPT again\nto describe each product, which in turn can be fed into Midjourney to generate an\nimage of each product. Using an AI model to generate a prompt for an AI model\nis meta prompting, and it works because LLMs are human-level prompt engineers\n(Zhou, 2022).\nInput:\nDescribe the new product \"OneFit UltraStride Shoes,\" a shoe\nthat can fit any foot size, in great detail:\nOutput:\nIntroducing the revolutionary \"OneFit UltraStride Shoes,\"\nthe footwear innovation designed to fit any foot size with\nunparalleled comfort and style. These shoes are a result of\ncutting-edge technology and meticulous craftsmanship,\nensuring a truly personalized and enjoyable walking or\nrunning experience for everyone.\nDesign and Materials:\nThe OneFit UltraStride Shoes boast an elegant and modern\ndesign that appeals to both fashion-forward individuals and\nsports enthusiasts. They feature a sleek and streamlined\nsilhouette, crafted using premium materials for durability\nand aesthetic appeal. The shoe's upper is constructed from a\ncombination of breathable mesh and flexible synthetic\nmaterials, allowing for excellent airflow and a lightweight\nfeel.\n...\nColor and Customization:\nOneFit UltraStride Shoes are available in a variety of\nvibrant colors and stylish patterns, catering to diverse\npersonal preferences. You can choose from classic\nmonochromatic options for a timeless look or opt for bold\nand vibrant hues to make a fashion statement. The ability to\ncustomize the shoe's appearance allows individuals to\nexpress their unique style while enjoying the benefits of\nthe OneFit technology.\nConclusion:\nThe OneFit UltraStride Shoes represent a breakthrough in\nfootwear technology, eliminating the hassle of finding the\n5. Divide Labor | 35perfect fit for your feet. With their adaptive design,\nsuperior comfort, and stylish aesthetics, these shoes offer\na truly personalized experience for individuals of all foot\nsizes. Stay comfortable, confident, and on-trend with the\nOneFit UltraStride Shoes, the ultimate choice for those\nseeking unmatched versatility and a perfect fit.\nDALL-E is well-known by GPT-4, and therefore you can invoke its name within\nChatGPT and it does a reasonable job at crafting a prompt for an image generation\ntool. If you were planning on using this prompt in production, you may consider\napplying the prompting principle of providing examples, but it does a good enough\njob for our purposes without examples.\nInput:\nConvert this description into a short prompt for an AI image\ngeneration model such as DALL-E:\nOutput:\nCreate an eye-catching image that visually represents the\n\"OneFit UltraStride Shoes,\" a revolutionary footwear product\nthat adapts to any foot size. Showcase a sleek and modern\nshoe design incorporating a stretchable upper, an adjustable\nlacing system, and a cushioned insole for optimal comfort.\nEmphasize the shoe's versatility by displaying it in action\non various surfaces, highlighting its excellent traction.\nIncorporate vibrant colors and patterns to convey the\ncustomizable nature of the product. The image should capture\nthe essence of innovation, style, and personalized fit that\nthe OneFit UltraStride Shoes offer.\nThe output of this prompt can now be plugged into image generation tools like\nDALL-E or Midjourney as a prompt, which can give you a good starting point for\nvisualizing what the product might look like. Although this might not be the final\ndesign you go with, seeing an image is more evocative and helps people form an\nopinion faster. It’s easier cognitively to criticize or compliment an existing image than\nit is to imagine a new image from a blank page or section of text.\nFigure 1-15 shows the output.\n36 | Chapter 1: The Five Principles of PromptingFigure 1-15. OneFit UltraStride shoes\nIt’s common practice when working with AI professionally to chain multiple calls\nto AI together, and even multiple models, to accomplish more complex goals. Even\nsingle-prompt applications are often built dynamically, based on outside context\nqueried from various databases or other calls to an AI model. The library LangChain\nhas developed tooling for chaining multiple prompt templates and queries together,\nmaking this process more observable and well structured. A foundational example is\nprogressive summarization, where text that is too large to fit into a context window\ncan be split into multiple chunks of text, with each being summarized, before finally\nsummarizing the summaries. If you talk to builders of early AI products, you’ll find\nthey’re all under the hood chaining multiple prompts together, called AI chaining, to\naccomplish better results in the final output.\nThe Reason and Act (ReAct)  framework was one of the first popular attempts at\nAI agents, including the open source projects BabyAGI, AgentGPT and Microsoft\nAutoGen. In effect, these agents are the result of chaining multiple AI calls together\nin order to plan, observe, act, and then evaluate the results of the action. Autonomous\nagents will be covered in Chapter 6 but are still not widely used in production at the\n5. Divide Labor | 37time of writing. This practice of self-reasoning agents is still early and prone to errors,\nbut there are promising signs this approach can be useful in achieving complex tasks,\nand is likely to be part of the next stage in evolution for AI systems.\nThere is an AI battle occurring between large tech firms like Microsoft and Google,\nas well as a wide array of open source projects on Hugging Face, and venture-funded\nstart-ups like OpenAI and Anthropic. As new models continue to proliferate, they’re\ndiversifying in order to compete for different segments of the growing market. For\nexample, Anthropic’s Claude 2 had an 100,000-token context window , compared\nto GPT-4’s standard 8,192 tokens . OpenAI soon responded with a 128,000-token\nwindow version of GPT-4 , and Google touts a 1 million token context length with\nGemini 1.5. For comparison, one of the Harry Potter books would be around 185,000\ntokens, so it may become common for an entire book to fit inside a single prompt,\nthough processing millions of tokens with each API call may be cost prohibitive for\nmost use cases.\nThis book focuses on GPT-4 for text generation techniques, as well as Midjourney v6\nand Stable Diffusion XL for image generation techniques, but within months these\nmodels may no longer be state of the art. This means it will become increasingly\nimportant to be able to select the right model for the job and chain multiple AI\nsystems together. Prompt templates are rarely comparable when transferring to a new\nmodel, but the effect of the Five Prompting Principles will consistently improve any\nprompt you use, for any model, getting you more reliable results.\nSummary\nIn this chapter, you learned about the importance of prompt engineering in the\ncontext of generative AI. We defined prompt engineering as the process of develop‐\ning effective prompts that yield desired results when interacting with AI models.\nY ou discovered that providing clear direction, formatting the output, incorporating\nexamples, establishing an evaluation system, and dividing complex tasks into smaller\nprompts are key principles of prompt engineering. By applying these principles and\nusing common prompting techniques, you can improve the quality and reliability of\nAI-generated outputs.\nY ou also explored the role of prompt engineering in generating product names and\nimages. Y ou saw how specifying the desired format and providing instructive exam‐\nples can greatly influence the AI’s output. Additionally, you learned about the concept\nof role-playing, where you can ask the AI to generate outputs as if it were a famous\nperson like Steve Jobs. The chapter emphasized the need for clear direction and\ncontext to achieve desired outcomes when using generative AI models. Furthermore,\nyou discovered the importance of evaluating the performance of AI models and the\nvarious methods used for measuring results, as well as the trade-offs between quality\nand token usage, cost, and latency.\n38 | Chapter 1: The Five Principles of PromptingIn the next chapter, you will be introduced to text generation models. Y ou will learn\nabout the different types of foundation models and their capabilities, as well as their\nlimitations. The chapter will also review the standard OpenAI offerings, as well as\ncompetitors and open source alternatives. By the end of the chapter, you will have\na solid understanding of the history of text generation models and their relative\nstrengths and weaknesses. This book will return to image generation prompting in\nChapters 7, 8, and 9, so you should feel free to skip ahead 