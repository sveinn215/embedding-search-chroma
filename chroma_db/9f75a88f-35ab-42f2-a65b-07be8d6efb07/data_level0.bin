{"source":"AbsenceBench.pdf","chroma:document":"arXiv:2506.11440v1  [cs.CL]  13 Jun 2025\nAbsenceBench: Language Models Can’t Tell What’s\nMissing\nHarvey Yiyun Fu∗,1, Aryan Shrivastava1, Jared Moore2\nPeter West2, Chenhao Tan1, Ari Holtzman1\n1University of Chicago 2Stanford University\nAbstract\nLarge language models (LLMs) are increasingly capable of processing long inputs\nand locating specific information within them, as evidenced by their performance\non the Needle in a Haystack (NIAH) test. However, while models excel at recalling\nsurprising information, they still struggle to identify clearly omittedinformation.\nWe introduce AbsenceBench to assesses LLMs’ capacity to detect missing\ninformation across three domains: numerical sequences, poetry, and GitHub pull\nrequests. AbsenceBench asks models to identify which pieces of a document\nwere deliberately removed, given access to both the original and edited contexts.\nDespite the apparent straightforwardness of these tasks, our experiments reveal that\neven state-of-the-art models like Claude-3.7-Sonnet achieve only 69.6% F1-score\nwith a modest average context length of 5K tokens. Our analysis suggests this\npoor performance stems from a fundamental limitation: Transformer attention\nmechanisms cannot easily attend to “gaps” in documents since these absences\ndon’t correspond to any specific keys that can be attended to. Overall, our results\nand analysis provide a case study of the close proximity of tasks where models\nare already superhuman (NIAH) and tasks where models breakdown unexpectedly\n(AbsenceBench).\nNeedle-in-a-haystack Absence Bench\nDo not go gentle into that good night,\nRage, rage against the dying of  the light\n…\n…\nWild men caught and sang the sun in ﬂight,\nAnd learn, too late, they grieved it on its way ,\nDo not go gentle into that good night\nRage, rage against the dying of  the light\n…\n…\nContext Window\nOriginal Context\nAnd learn, too late, they grieved it on \nits way\nomission\nModiﬁed Context\nDo not go gentle into that good night,\nRage, rage against the dying of  the light\n…\n…\nWild men caught and sang the sun in ﬂight,\nContext Window\n Context\nThe best thing to do in San Diego is to \nwalk by the sea.\ninsertion\nQuestion\nWhat is the best thing to do in San Diego?\nQuestion\nWhat is omitted in the modiﬁed context?\n: walk by the sea\nWild men caught and sang the sun in ﬂight,\n: And learn …, I forgot\nModel Response\nAnd learn, … way\nomission\nomitted\n20\n40\n60\n80\n100\nNIAH\n Absence Bench\n(a) (b)\n(c)\nFigure 1: (a) An overview of the difference between the Needle-in-a-haystack (NIAH) test setting\nand AbsenceBench task setting. AbsenceBench is asking models to identify omitted pieces\nof content. (b) Performance of 5 SoTA LLMs on AbsenceBench is significantly lower than on\nthe NIAH test, measured by F1-score. (c) an illustration of our task setting using the “haystack”\nmetaphor, generated by ChatGPT.\n∗Corresponding author(s):harveyfu@uchicago.edu\nPreprint. Under review.1 Introduction\nRecent Large Language Models (LLMs) have shown exceptional abilities across a wide range of\nlong-context tasks (Bai et al., 2024; Zhang et al., 2024, inter alia). In particular, the Needle-in-a-\nHaystack (NIAH) test (Kamradt, 2023; Hsieh et al., 2024) has been used to evaluate whether models\ncan find small bits of surprising information in extremely long inputs, with recent models pushing\nthe frontier of NIAH into the millions of tokens. We ask the converse question: Can LLMs spot\ninformation that has clearly been left out?\nTo answer this question, we present AbsenceBench: a new benchmark designed to evaluate the\nabilities of LLMs in locating conspicuously missing information from long inputs. Instead of asking\nLLMs to find off-topic information (the ‘needle’ in NIAH), LLMs are prompted to identify and\nrecall intentionally omitted information. Cutting-edge, closed-source models perform poorly on\nAbsenceBench, despite the apparent similarity to NIAH and the fact that AbsenceBench is\nsimple to describe and entirely unambiguous.\nAbsenceBench includes three domains: poetry, numerical sequences, and GitHub pull requests\n(PRs). The average context length of AbsenceBench is 5K, significantly shorter than most long-\ncontext benchmarks—we thus call it a medium-context benchmark. We benchmark a total of 14\nLLMs on AbsenceBench, including cutting-edge models such as GPT-4 (OpenAI et al., 2024b),\nClaude-3.7-Sonnet (Anthropic, 2024), and Gemini-2.5-flash (Google, 2025)2, as well as inference-\ntime compute models such as o3-mini (OpenAI, 2025), Grok-3-mini (xAI, 2025) and DeepSeek-R1\n(DeepSeek-AI et al., 2025). We further study the impact of context length and the rate of omission.\nOur results suggest that: (1) Longer context length makes the task harder especially under the poetry\ndomain. (2) Using inference-time compute boost models’ performance by only 7.9% at the cost\nof generating an average of extra 8K thinking tokens—nearly 3x the average document length! (3)\nCounter-intuitively, alower rate of omission in the task set-up leads to worse model performance.\nAbsenceBench is also significantly more difficult for LLMs than the NIAH test. To illustrate this,\nwe compare the performance of three LLMs on both our proposed task setting and the original NIAH\ntest setting, observing a a massive 56.9% drop in F1-score on average. Why is AbsenceBench so\nmuch more difficult than the NIAH test? We hypothesize that the Transformer (Vaswani et al., 2017)\nattention may have trouble addressing “gaps” in the document, leading us to experiment with adding\nplaceholder strings where information is omitted. This boosts the performance by a dramatic 35.7%\non average (see §4.2).\nWhat explains these results? While LLMs often generate deceptively human-like responses, and may\neven have human-like cognitive-biases (Koo et al., 2024), the contrast in success between NIAH and\nAbsenceBench suggests that models fail for completely different reasons than humans do. This\nhas practical implications: if LLMs cannot properly notice when information is missing, can we rely\non LLM-as-a-Judge (Zheng et al., 2023)? We hope that AbsenceBench can serve as a starting\npoint for examining LLMs’ ability to tell what is missing, and as a case-study of an LLM-specific\ncognitive bias.\nOverall, our contributions are as follows:\n• We release a new medium-length-context benchmark for evaluating LLMs’ abilities in locating\nintentionally-omitted information across three diverse domains.\n• We evaluate 14 popular LLMs, including those with inference-time compute, and show that our\nbenchmark is challenging even for cutting-edge language models.\n• We show that while the the NIAH test is essentially solved for very long contexts,\nAbsenceBench tasks have low performance on medium length contexts, suggesting LLMs\nfind it harder to identify omissions than insertions.\n• We analyze the effect of inference-time compute, finding that it offers only a modest performance\nimprovement at a high cost—an average chain-of-thought length that is nearly 3x the size of the\naverage document length.\n• Explicitly marking omissions improves models’ performance by 35.7% on average. This\nsuggests that AbsenceBench may be caused by inherent weaknesses within Transformer-\nstyle self-attention mechanisms.\n2Claude-3.7-Sonnet and Gemini-2.5-flash have the option to leverage inference-time compute and we evaluate\nthem both with and without this feature.\n20 2000 4000 6000 8000 10000 12000 14000\n0\n10\n20\n30\n40\n50\n60\n70\n80\nFrequency\nPoetry\n0 500 1000 1500 2000 2500 3000 3500\n0\n20\n40\n60\n80\n100\n120\n140\nNumerical Sequence\n0 2000 4000 6000 8000 10000 12000 14000\n0\n50\n100\n150\n200\nGitHub PRs\nNumber of T okens\nFigure 2: The three domains in AbsenceBench test models’ abilities across a variety of\ndocument lengths and omission probabilities . Frequency reports the number of tasks in the\ndomain within a given range of document lengths. The average context length across all tasks in\nAbsenceBench is 5K tokens. On the document level, the average document length is 2.7K, while\nit is 4.7K for poetry, 1.5K for numerical sequences, and 1.7K for Github pull requests. We use the\nGPT-4 Tokenizer4to measure document and context lengths.\nDomain Original Modified\nPoetry\n. . .And so, to you, who always were\nTo me, I give these weedy rhymes\nIn memory of early times. . .\n. . .And so, to you, who always were\nIn memory of early times. . .\nNumerical 117, 121 , 125, 129, 133, 137 . . . 117, 125, 129, 133 . . .\nGitHub PRs\n. . .\n+ $replacements = [\n+ ‘[’ => ‘ \\[’,\n+ ‘<’ => ‘&lt;’,\n+ ‘>’ => ’&gt;’,\n+ ];\n. . .\n+ $replacements = [\n+ ‘[’ => ‘ \\[’,\n+ ‘>’ => ‘&gt;’,\n+ ];\nAbsence Token\n. . .And so, to you, who always were\nTo me, I give these weedy rhymes\nIn memory of early times. . .\n. . .And so, to you, who always were\n<missing line>\nIn memory of early times. . .\nTable 1: Examples of tasks in AbsenceBench by domain. Models are given both the original and\nmodified documents and are asked to identify which elements are missing (in blue ). The benchmark\nincludes the Poetry, Numerical Sequences, and Github PRs domains, while the Absence Token\ndomain includes an omission tokenthat is used for analysis in §4.2\n2 AbsenceBench\nAbsenceBench3 is built on a simple idea: LLMs have trouble keeping track of what’s missing.\n2.1 Task Definition\nWe formulate all tasks in AbsenceBench into a straightforward controlled generation framework:\ngiven an original document composed of n elements Dorig = {e1, . . . , en}, we intentionally omit p\npercent of the elements Domit = {er1, er2, . . . , erk}, with k = p · n, and produce a modified version\nof the document Dmodified = Dorig\\Domit. We present both versions of the document Dorig and\nDmodified to the LLMs and then prompt them to generate the exact set of omitted elements Domit.\nWe use “document length” to denote the token count of the original document, and “context length”\nto indicate the token count of the entire input context.\n2.2 Dataset Construction\n3Our code is available at https://github.com/harvey-fin/absence-bench\n4https://github.com/openai/tiktoken?tab=readme-ov-file#-tiktoken\n3System Prompt\nYou are helping a student practice memorizing poems. The student will\nrecite a poem, but they may have missed some lines. Your task is to\nidentify exactly which lines are missing from their recitation.\nList only the missing lines, nothing else.\nUser Message\nHere is the complete original poem:\n{original poem}\nNow, here is my recitation which may be missing some lines:\n{modified poem}\nWhat lines did I miss? Please list only the missing lines, nothing else.\nTable 2: Default prompt template used for evaluating language models under the poetry domain\nAbsenceBench covers three distinct domains: poetry, numerical sequences, and GitHub pull\nrequests (PRs). Poetry and GitHub PRs are realistic data directly collected from open sources, while\nnumerical sequences are synthetic. We choose p = 0.1 for all domains. Overall, AbsenceBench\ncontains 4302 instances in total, with an average context length of 5K tokens. We plot the distribution\nof document lengths for each domain in Figure 2.\nPoetry We use poems from the Gutenberg Poetry Corpus (Parrish, 2018). For each poem, we omit\nat a line level and use the newline character as the delimiter between different lines. In order to create\ndiversity in document length, we truncate the poems such that the number of lines for each poem are\nuniformly distributed from 100 to 1000.\nNumerical Sequences LLMs demonstrate impressive abilities in solving mathematical tasks\n(Frieder et al., 2023), and have seen numbers frequently in the pre-training data. Nonetheless,\nit is unclear whether LLMs are able to reliably keep track of numerical sequences. We generate a total\nof 1200 numerical sequences. Each sequence consists of n decimal numbers {a(1), a(2), . . . , a(n)}\nthat are arranged in a particular order L ∈ {ascending, descending, random}, with a step size\ns ∈ {1, 4, 7, 13} between two consecutive numbers. The first number a(1) is randomly chosen from\n0 to 9999. We omit each number from the set with a certain probability, and delimit numbers with a\nnewline character.\nGitHub PRs GitHub is one of the largest open-source code repositories and is frequently used as a\nhigh-quality data source for (LLMs) such as Code Llama (Rozière et al., 2024). We retrieve the PRs\nfrom the top 20 GitHub repositories5 with the most PRs using GitHub API, and only keep those PRs\nwith a diff that includes 10 to 200 updated lines (i.e., a line that starts with “+” or “-”). We format\nthis task similarly to the poetry domain: omissions are applied at a line level, and we use the newline\ncharacter as the delimiter. In particular, we only omit the updated lines that are unique within each\nPR’s diff. This domain has practical application to current LLM-usage: an LLM that resolve and\nverify merge conflicts must inherently be able to detect omissions in file diffs.\n3 Experiments\n3.1 Experiment Setup\nModels We evaluate a total of 14 LLMs, including seven models that leverage inference-time\ncompute (Gemini-2.5-flash (Google, 2025), Claude-3.7-Sonnet (Anthropic, 2024), o3-mini (OpenAI,\n2025), DeepSeek R1 (DeepSeek-AI et al., 2025), Grok-3-mini-Beta (xAI, 2025), Qwen3-235B\n(Qwen, 2025a), QwQ-32B (Qwen, 2025b)), three OpenAI models (GPT4o(OpenAI et al., 2024b),\nGPT-4.1, GPT-4.1-mini), and four open-weights models (Llama-4-Maverick, Llama-3.3-70B-Instruct\n(Meta, 2025), Qwen2.5-72B-Instruct (Yang et al., 2024), Mixtral-8x7B-Instruct (Jiang et al., 2024)).\nWe run both Gemini-2.5-flash and Claude-3.7-Sonnet with and without inference-time compute (i.e.,\n“thinking mode”). All selected models have a claimed context length of over 32K tokens. All model\ninferences are obtained through API requests (see Appendix B for details).\n5https://top1000repos.com/based-on-pr\n4Models Poetry Numerical Sequences GitHub PRs Average\nGemini-2.5-flash* 87.3 95.4 30.9 71.2\nClaude-3.7-Sonnet* 72.7 96.0 40.0 69.6\nClaude-3.7-Sonnet 73.5 91.4 35.7 66.9\nGemini-2.5-flash 79.3 85.2 26.2 63.6\no3-mini* 65.0 78.1 38.9 60.7\nGPT-4.1 54.3 57.5 36.2 49.3\nGrok-3-mini-Beta* 40.7 56.3 36.4 44.5\nGPT-4o 38.4 48.1 39.4 42.0\nQwQ-32B* 32.1 57.7 31.6 40.5\nLlama-4-Maverick 32.8 58.7 29.0 40.2\nGPT-4.1-mini 30.2 45.0 31.3 35.5\nLlama-3.3-70B-Instruct 25.3 37.7 28.7 30.6\nQwen2.5-72B-Instruct 19.0 45.4 26.8 30.4\nDeepSeek-R1* 38.7 29.5 23.1 30.4\nQwen3-235B* 26.1 18.5 24.6 23.1\nMixtral-8x7B-Instruct 4.9 21.9 17.3 14.7\nTable 3: Micro F1-score (%) of 14 LLMs evaluated on all three domains of Absence Bench. * indicates\nthe model uses inference-time compute during evaluation. GitHub PRs represent the most challenging\ndomain: models that perform well in other domains often struggle there.\nEvaluation Metric Our evaluation is two-fold: (1) we use exact match to check whether each\nelement (e.g., a line of a poem) is present in a model’s response, and (2) we use themicro F1-score to\nevaluate whether models generate the correct set of elements. Note that we use a different evaluation\nmetric than the recall accuracy metric that is frequently used in the NIAH test. In our task setting,\na model could achieve a perfect recall score simply by copy-pasting the entire original context.\nHowever, this would result in many false positives cases (i.e., when model generates non-omitted\nelements) that recall accuracy does not account for.\nPrompt Templates & In-context Learning Table 2 presents an example of the prompt template\nused for evaluation in the poetry domain. See Appendix A for detailed prompts under all domains.\nDue to compute and context length constraints, we include limited perturbation studies on the prompt\ntemplate and in-context learning examples in this paper and leave it for future work. See §4.1 for a\nperturbation study on the prompt template and §7 for further discussion.\n3.2 Main Results\nWe show the evaluation results of 14 LLMs on AbsenceBench in Table 3. Gemini-2.5-flash\n(thinking)6 outperforms the rest of the models on poetry, numerical sequences, and overall by\na significant margin, followed by Claude-3.7-Sonnet (thinking). Most open-weights models are\ngenerally weaker in performance and struggle to reach 40% F1-score except for QwQ-32B and\nLlama-4-Maverick. GitHub PRs is a challenging domain for all models: the highest score is only\n40.0% by Claude-3.7-Sonnet (thinking). It is important to highlight that while Mixtral-8x7B achieves\na perfect score on the NIAH test at an equivalent context length, it attains only a 14.7% F1-score\non the AbsenceBench, indicating substantial space for improvement among smaller-scale models.\nOverall, AbsenceBench presents a surprisingly challenging task to cutting-edge language models.\nInference-time compute is helpful but costly We consider seven models that include inference-\ntime compute capabilities. We plot the distribution of thinking token ratio: the ratio between the\ntotal thinking tokens generated by seven inference-time compute models and the original document\nlength under all three domains in Figure 3. The figure shows that reasoning models typically generate\nthinking tokens several times greater than the document length, which further suggests that the models\nmay attempt to reconstruct the original document using thinking tokens as a way to identify omissions.\nNotably, Gemini-2.5-flash shows the highest thinking token ratio of 5.7 on average across three\ndomains, partly accounting for its significant advantage in benchmark performance. Additionally,\nwe compare the performance of Gemini-2.5-flash and Claude-3.7-Sonnet under conditions with and\n6We use (thinking) to denote the model that use inference-time compute during evaluation\n5Gemini-2.5-flash\nClaude-3.7-Sonnet\no3-mini\nGrok-3-mini-Beta\nQwQ-32B\nQwen3-235BDeepSeek R1\n0.0\n2.5\n5.0\n7.5\n10.0\nThinking T oken Ratio\nPoetry\nGemini-2.5-flash\nClaude-3.7-Sonnet\no3-mini\nGrok-3-mini-Beta\nQwQ-32B\nQwen3-235BDeepSeek R1\n0\n10\n20\nNumerical Sequences\nGemini-2.5-flash\nClaude-3.7-Sonnet\no3-mini\nGrok-3-mini-Beta\nQwQ-32B\nQwen3-235BDeepSeek R1\n0\n10\n20\n30\nGitHub PRs\nLanguage models\nFigure 3: Reasoning models often generate an order of magnitude more text than input doc-\nument. Distribution of the thinking token ratio(number of generated thinking tokens divided by\nnumber of tokens in the original document) for four inference-time compute models under each\ndomain. We set the parameters of the boxplot to capture 99% of the distribution. The outliers are\nhidden for better clarity (see Figure 8 for the full distribution).\n1.75 2.00 2.25 2.50 2.75 3.00 3.25 3.50\n20\n30\n40\n50\n60\n70\n80\n90Micro F1 (%)\nPoetry\n3.5 4.0 4.5 5.0 5.5 6.0 6.5\nNumerical Sequences\n3 4 5 6 7\nGithub PRs\nThinking T oken Ratio\nGemini-2.5-flash Claude-3.7-Sonnet o3-mini Grok-3-mini-Beta QwQ-32B Qwen3-235B DeepSeek R1\nFigure 4: Closed-source models (reds) perform better than open-weights models (blues) on\nAbsenceBench, while generating more thinking tokens. Each plot shows the average F1-score\n(x-axis) and the average thinking token ratio (y-axis). The grey line presents a visual boundary.\nwithout the use of “thinking mode”. We observe that inference-time compute leads to a modest\nperformance improvement of 7.9%, but it comes at the cost of producing an additional 8K tokens for\nintermediate reasoning steps on average—significantly longer than our average context length of 5K\ntokens.\nClosed-source models outperform open-weights models We plot the average F1-score and\nthinking token ratio of four closed-source models and three open-weights models in Figure 4. We\nobserve that closed-source models generally achieve higher average F1-scores than open-weight\nmodels across all domains, with the exception of QwQ-32B, which outperforms Grok-3-mini-Beta in\nnumerical sequences and Gemini-2.5-flash in GitHub PRs. On the other hand, the higher performance\nof closed-source models come at the cost of generating 42.0% more thinking tokens than open-weights\nmodels on average across all domains.\nLocating omissions are harder than insertions The increased difficulty of AbsenceBench\nrelative to the original NIAH test could potentially be attributed to differences in the task design and\nthe higher frequency of document modifications (insertions or omissions). To investigate this, we\nrun Claude-3.7-Sonnet, GPT-4.1-mini, and Llama-4-Maverick under an “insertion bench” setting:\nrather than omitting elements from the context, we insert \"needles\" at an equivalent frequency. We\nchoose the Harry Potter dataset7 as the needles and two realistic domains (poetry and GitHub PRs)\nas the haystack. All three models achieve nearly 99.5% F1-score on the poetry domain, and at\nleast 86.2% F1-score on Github PRs (see Appendix C for full results). By comparison, models\nexperience a substantial average performance drop of 56.9% under theAbsenceBench task setting,\nsuggesting that the observed difficulty likely arises from the distinction between omissions and\ninsertions. A possible explanation for this observation is that the Transformer attention mechanism\n7https://www.kaggle.com/datasets/gulsahdemiryurek/harry-potter-dataset\n60 5000 10000 15000 20000 25000\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nMicro F1\nPoetry\nR2=0.55\n0 2000 4000 6000 8000\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nNumerical Sequences\nR2=0.33\n0 5000 10000 15000 20000\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nGitHub PRs\nR2=0.08\n 0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35\n0.40\nOmission Rate\nT otal Context Length (in tokens)\nFigure 5: GPT-4.1-mini performs worse on longer tasks in Poetry,but the relationship is not clear\nin Numerical Sequences and Github PRs. Each plot shows the F1-score (y-axis) and the total context\nlength (x-axis). Dark blue represents a lower and dark red represents a higher percentage of omission\n(number of omitted lines divided by total number of lines across each of three domains). Each dot\non the graph represents the performance on a single instance. The dashed lines represent the least\nsquares fit, with R2 indicating the strength of correlation between the two axes.\nmight struggle to effectively handle \"gaps\" when processing documents containing omissions. We\ncarry out additional experiments to assess this hypothesis in Section 4.2.\n4 Analysis\nWe study the effect of perturbing the context length as well as the percentage of omissions on the\nmodels’ performance. We additionally perform an error analysis and present a simple strategy that\neffectively addresses the difficulty in AbsenceBench. For the sake of cost, we limit our extended\nanalysis to three LLMs: Claude-3.7-Sonnet, GPT-4.1-mini, and Llama-4-Maverick.\n4.1 Perturbation Studies\nWe construct a perturbed dataset with a higher variance of omission rates in order to lower the impact\nof various factors on the difficulty of AbsenceBench tasks. We randomly sample the probability\nof omission to be p ∼ U(0, 0.5) for each document, and then randomly omit each element from\nthat document with probability p. Figure 5 illustrates the relationship between the performance of\nGPT-4.1-mini and total context length. The analysis of Claude-3.7-Sonnet and Llama-4-Maverick are\nshown in Figure 6 and 7 in Appendix D.\nContext length has a major impact on performance For contexts shorter than 5K tokens, the\nperformance varies significantly with changes in context length. This finding contrasts with observa-\ntions from the NIAH test, where context length affects performance only when contexts exceed 100K\ntokens. We perform a linear regression analysis to assess the correlation between F1-score and total\ncontext length. Results across three LLMs indicate a stronger average correlation (R² = 0.55) within\nthe poetry domain compared to numerical sequences (0.19) and GitHub PRs (0.08).\nLLMs perform worse with lower omission rate In Figure 5, we observe a notable pattern (as\nindicated by color) that shows a positive correlation between model’s performance and the omission\nrate, especially in the poetry and numerical sequences domain. In other words, models actually\nperform worse when required to recallfewer lines. While this may initially appear counterintuitive, we\nhypothesize that a higher omission rate increases the likelihood of consecutive omissions occurring.\nConsequently, language models may be able generate text consistently until encountering a non-\nomitted element that disrupts their generation process. On the other hand, language models encounter\ngreater difficulty with contexts containing fewer omissions, as numerous missing spans may interrupt\nthe continuity between segments that must be generated.\nAlternative prompt template reduces model performance Alongside the default prompt template\n(see Table 2), we curate a perturbed template where the task instructions are positioned after the\n7Models Poetry Numerical Sequences GitHub PRs\ndefault post-instruction default post-instruction default post-instruction\nClaude-3.7-Sonnet 78.8 66.1 94.3 93.7 35.1 34.7\nGPT-4.1-mini 45.4 39.2 54.2 42.5 32.9 35.4\nLlama-4-Maverick 48.7 36.6 71.5 63.6 29.6 30.5\nTable 4: Micro F1-score (%) of three LLMs evaluated on the AbsenceBench with two different\nprompt templates. All three models exhibit reduced performance with the post-instruction template\nin the poetry and numerical sequences domains, but remain robust in the GitHub PRs domain.\nModels Poetry Numerical Sequences GitHub PRs Average\nPlaceholder: None(baseline)\nClaude-3.7-Sonnet 73.5 91.4 35.7 66.9\nGPT-4.1-mini 30.2 45.0 31.3 35.5\nLlama-4-Maverick 32.8 58.7 29.0 40.2\nPlaceholder: <missing line>\nClaude-3.7-Sonnet 87.4(+18.9%) 97.7(+6.9%) 64.9(+81.8%) 83.3(+24.5%)\nGPT-4.1-mini 50.4(+66.9%) 59.2(+31.5%) 46.8(+49.5%) 52.1(+46.8%)\nLlama-4-Maverick 60.7(+85.0%) 78.9(+34.4%) 46.8(+61.4%) 62.1(+54.5%)\nPlaceholder: __________\nClaude-3.7-Sonnet 85.5(+16.3%) 97.2(+6.3%) 61.3(+71.7%) 81.3(+21.5%)\nGPT-4.1-mini 45.9(+52.0) 57.3(+27.3%) 36.5(+16.6%) 46.5(+31.2)\nLlama-4-Maverick 53.9(+64.3%) 73.0(+24.4) 36.5(+25.9%) 54.5(+35.5%)\nTable 5: Micro F1-score of three LLMs evaluated on all three domains of Absence Bench using 2\ndifferent placeholders: “<missing line>” and “__________” (10 consecutive underlines), along with\nthe percentage increase in F1-score compared to the none-placeholder baseline.\noriginal and modified documents. We refer to this perturbed prompt template as “post-instruction”.\nTable 4 shows that the performance of three LLMs prompted with both prompt templates. All\nthree models exhibit reduced performance with the post-instruction template under both the poetry\nand numerical sequences domains, suggesting that LLMs more effectively detect omissions when\ninstructions are presented beforehand. Meanwhile, LLMs remain robust in the GitHub PRs domain.\nAll detailed prompt templates are included in Appendix A.\n4.2 Marking omissions with placeholders\nTo assess whether the Transformer’s attention mechanism is the reason for AbsenceBench’s\ndifficulty, we explicitly mark omissions in the modified context usingplaceholders, special tokens\nthat explicitly signal omitted segments. Specifically, we test across all domains (see Table 5 for full\nresults) and consider two different placeholders: “<missing line>” and 10 consecutive underlines.\nAcross all three domains, using “<missing line>” as the placeholder performs better, with an average\nboost in performance of 41.9%. Notably, Llama-4-Maverick achieves the highest improvement of\n54.5% overall. The improved performance is comparable to that of o3-mini, yet o3-mini generates\n9.1K extra thinking tokens on average.\nWhy does having a placeholder help so much? We hypothesize that AbsenceBench is so difficult\nfor models because, when a segment of text is omitted, there is no position to attend to that corresponds\ndirectly to that omission. Transformer-based LLMs are incredibly good at finding the correct piece of\ninformation to attend to from a document, but what happens when the information is a conspicuous\nabsence? What should the Transformer attend to? The fact that including a placeholder improves\nperformance so drastically suggests that Transformer self-attention struggles to anchor on information\ngaps. This is a key area for future architecture and inference-time research, as omissions are\ncommon—from missing paperwork to comments deliberately left out of a recommendation letter.\n85 Discussion\nOur experiments highlight a failure of cutting-edge models to consistently succeed at the simple\ntask of detecting absences, despite very strong performance on existing benchmarks. This suggests\nthat popular evaluations do not effectively cover the capability of absence recognition, although it is\nfundamental for applications such as LLM-as-a-judge in which models must recognize which rubric\nelements are not covered. Our results are especially surprising given the simplicity of our evaluation:\nwe only ask about surface-form absence (rather than absence on a higher semantic level), and directly\nprovide models with both the original and modified documents for correct comparison. Indeed, our\ntasks could be solved by a simple program in linear time, yet deployed LLMs with hundreds of\nbillions of parameters consistently struggle on them.\nWe propose an initial hypothesis explaining this behavior: identifying presence is simpler than\nabsence with the attention mechanisms underlying Transformers (Vaswani et al., 2017). Information\nincluded in a document can be directly attended to, while the absence of information cannot. We\nprovide initial evidence of this hypothesis in §4.2. Rather than simply deleting elements from the\noriginal document, we include an absence “placeholder” here (e.g. “<missing line>”) to provide a\nspan of text for the transformer to attend to, indicating absence. This causes consistent, significant\nimprovements for models, suggesting that the lack of a sequence to attend to is at least one aspect of\nthis problem.\nOne important question raised by our work is how to make models more effective at handling and\nidentifying absence. The strong performance of Gemini-2.5-flash in 2 of our 3 domains suggests that\ninference-time compute and reasoning mechanisms may help, although this may not hold in general.\nHowever there is a cost: we find that Gemini uses an extremely large number of reasoning tokens\nin many cases (Figure 3), in many cases exceeding the length of the original document by an order\nof magnitude. This could be the result of naive solutions, such as regenerating the full document\nmultiple times. While this would work for the simple versions of absence studied here, it would likely\nfail on more complex notions of absence that look beyond missing surface forms.\nWe suggest a few directions where future work can go. Reasoning may improve absence detection,\nbut must be studied on more complex, semantic notions of absence to be sufficiently validated—\nAbsenceBench is merely a necessary bar. New architectures, with mechanisms significantly\ndifferent from attention, may be required to address the deeper problem. Such work should be\nsupported by a more mechanistic understanding of why existing models often fail on these tasks, and\nhow attention is connected to this question. We hope that AbsenceBench will provide a natural\nplayground for such mechanistic studies. This will also be key in guiding how existing models are\nused–if they fail at these simple notions of absence, it is not clear that models will be trustworthy for\nmore complex tasks that require this capability, such as model-as-a-judge evaluations using rubrics.\nRegardless of how this problem might be solved, our work supports the notion that LLMs show novel\nintelligence, not well explained by analogy to humans. LLMs fail at this simple notion of absence\nwhile performing at superhuman levels on tasks that are quite similar (e.g. Needle-in-a-haystack) and\nmany tasks that seem much more complex and challenging (e.g. math problem solving). The “shape”\nof LLM intelligence is simply not that similar to humans. While improving LLM performance on\nabsence should be one goal, this also poses an opportunity to consider: how might we map the axes\nof variation in LLM intelligence?\nOur findings suggest that models are not effective at understanding the conspicuous absence of infor-\nmation, an often overlooked yet foundational component of comprehension. Standard benchmarks\noverwhelmingly focus on whether models can identify and reason about what is present, which\ndoes not appear to tightly correlate with a model’s ability to identify absent information. However\nuse-cases such as LLM-as-a-judge, and tasks such as grading, legal reasoning, and misinformation\ndetection, often hinge on recognizing what is missing. The gap in performance between NIAH and\nAbsenceBench underscores how misleading current evaluations might be if they ignore absence.\nEvaluators and developers should thus augment rubric-based assessments with systematic tests for\nabsence sensitivity, both at surface and semantic levels. More broadly, absence may be a useful\nconceptual lens through which to understand model failure. Rather than treating hallucinations,\nmisjudgments, or oversights as separate phenomena, they may all be related to a common limitation:\nmodels’ weak grasp of what is not there. As a result, better understanding and diagnosing absence\nfailure may reveal general-purpose principles for more robust and trustworthy LLM behavior.\n96 Related Work\nLong-context Language Models The context window of language models has significantly in-\ncreased due to recent efforts in training long-context language models with optimized data mixture\n(Grattafiori et al., 2024; Gao et al., 2025; AI et al., 2025), exploring novel model architectures (Gu\nand Dao, 2024; Dao and Gu, 2024; Fu et al., 2023; Peng et al., 2023; Bertsch et al., 2023), developing\nencoding mechanisms (Su et al., 2023; Yen et al., 2024) as well as length extrapolation techniques\n(Press et al., 2022; Sun et al., 2022; Chen et al., 2023).\nLong-context Benchmarks Many existing benchmarks evaluate language models’ long-context\nabilities along multiple dimensions. ZeroSCROLLS (Shaham et al., 2023) focus on long-context\ninformation gathering tasks. Long-bench (Bai et al., 2024) introduces a bilingual benchmark and\nstudies context-length variations. L-Eval (An et al., 2023) aim for a more diverse and high-quality\ncollection of datasets. Infinite-Bench (Zhang et al., 2024) extend the context-length to over 100K\ntokens. HELMET (Yen et al., 2025) presents a holistic evaluation beyond synthetic data. Other\nprevious work focus on specific tasks, such as retrieval-augmented generation (Lee et al., 2024),\nquestion answering (Koˇciský et al., 2017; Wang et al., 2025), in-context learning (Agarwal et al.,\n2024; Anil et al., 2024; Xu et al., 2024), coreference resolution (V odrahalli et al., 2024), and document\nsummarization(Chang et al., 2024; Kim et al., 2024). Compared to those canonical tasks designed for\nbenchmarking long-context understanding abilities, AbsenceBench features the task of handling\nlong contexts at a finer granularity. Similar to the NIAH test (Kamradt, 2023; Yen et al., 2025)\nthat evaluates the ability of LLMs to locate and retrieve crucial pieces of information, we formulate\nAbsenceBench into a much more straightforward task of identifying omissions in context, which\nturns out to be more challenging than identify the presence of information.\n7 Limitations\nWhile AbsenceBench exposes fundamental challenges in how current LLMs process omitted\ncontent, we note several limitations.\nSurface-form omission only. We chose to keep the tasks exceedingly simple; even though our\nbenchmark is solvable by simple heuristics models fail to use these heuristics. Our benchmark\nexclusively evaluates surface-form omission, where the removed content is a simple deletion. This\nsimplification was intentional and helps make the evaluation unambiguous and trivially automatic.\nHowever, it does not capture more subtle or semantic notions of absence—such as omitted reasoning\nsteps or missing evidence—which are critical in many real-world applications like rubric-based\ngrading or code review.\nStructured task format. AbsenceBench presents models with both the original and modified\ndocuments side-by-side and asks for the omitted elements directly. This setup may not reflect\nnaturalistic settings where users do not provide such explicit comparisons. As a result, our evaluation\nmay overestimate how well models would perform under more realistic situations with omitted\ninformation. Still, given the poor performance on AbsenceBench, current models are clearly\nnot performant. However, if a new generation of models solve AbsenceBench, that will not be\nsufficient evidence to say that models can handle omission elegantly.\nEvaluation scope. Our benchmark covers three domains (poetry, numerical sequences, and GitHub\npull requests) with medium-length contexts (5K tokens). While this breadth offers early insight into\nthe difficulty of absence detection, our findings may not generalize to other modalities (e.g., vision,\naudio), domains (e.g., legal, medical), or longer contexts. However, we expect these different settings\nto be more difficult, not less.\nNo prompt or instruction tuning. We only explored two different prompts and did not explore\nprompt tuning or in-context examples, due to the overwhelming cost of running long-context models,\nespecially the most performant ones that make use of inference-time compute. Note that many\ninference-time models consistently use more the three times more “thinking tokens” than the original\nlength of the document they are scanning for omissions. Thus, it remains an open question whether\n10more elaborate prompting strategies, could significantly improve model performance, something we\nhope to examine in future work.\nStatistical significance. Finally, we did not compute error bars or significance testing for our\nevaluation across runs due to API cost constraints. Although our findings are consistent across\ntasks and models, additional replication would help quantify variability and confirm robustness. We\nespecially would have liked to run a document perturbation study, to see whether small variations in\nthe document or prompt affect performance.\n8 Conclusion\nWe introduce AbsenceBench, a benchmark that tests LLMs’ ability to detect omitted informa-\ntion—an ability distinct from the well-studied task of recalling present content. Despite the bench-\nmark’s simplicity, models perform poorly, with surprising trends: fewer omissions make the task\nharder, and inference-time compute models often generate three times as many “thinking tokens”\nthan the document length itself. Explicitly inserting placeholders where content is missing substan-\ntially improves performance, supporting the hypothesis that attention struggles to represent absence.\nThese results reveal a core limitation in current models and motivate future work on absence-aware\narchitectures and evaluation.\n9 Acknowledgement\nWe thank OpenAI for API credits granted via their Researcher Access Program. We thank Aswathy\nAjith, Todd Nief, and Chenghao Yang for their thoughtful feedback and suggestions.\nReferences\nRishabh Agarwal, Avi Singh, Lei M. Zhang, Bernd Bohnet, Luis Rosias, Stephanie Chan, Biao\nZhang, Ankesh Anand, Zaheer Abbas, Azade Nova, John D. Co-Reyes, Eric Chu, Feryal Behbahani,\nAleksandra Faust, and Hugo Larochelle. Many-shot in-context learning, 2024. URL https:\n//arxiv.org/abs/2404.11018.\n01. AI, :, Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang, Guanwei Zhang, Guoyin\nWang, Heng Li, Jiangcheng Zhu, Jianqun Chen, Jing Chang, Kaidong Yu, Peng Liu, Qiang\nLiu, Shawn Yue, Senbin Yang, Shiming Yang, Wen Xie, Wenhao Huang, Xiaohui Hu, Xiaoyi\nRen, Xinyao Niu, Pengcheng Nie, Yanpeng Li, Yuchi Xu, Yudong Liu, Yue Wang, Yuxuan Cai,\nZhenyu Gu, Zhiyuan Liu, and Zonghong Dai. Yi: Open foundation models by 01.ai, 2025. URL\nhttps://arxiv.org/abs/2403.04652.\nChenxin An, Shansan Gong, Ming Zhong, Xingjian Zhao, Mukai Li, Jun Zhang, Lingpeng Kong, and\nXipeng Qiu. L-eval: Instituting standardized evaluation for long context language models, 2023.\nURL https://arxiv.org/abs/2307.11088.\nCem Anil, Esin DURMUS, Nina Rimsky, Mrinank Sharma, Joe Benton, Sandipan Kundu, Joshua Bat-\nson, Meg Tong, Jesse Mu, Daniel J Ford, Francesco Mosconi, Rajashree Agrawal, Naomi Bashkan-\nsky Rylan Schaeffer, Samuel Svenningsen, Mike Lambert, Ansh Radhakrishnan, Carson Deni-\nson, Evan J Hubinger, Yuntao Bai, Trenton Bricken, Timothy Maxwell, Nicholas Schiefer,\nJames Sullyand Alex Tamkin, Tamera Lanham, Karina Nguyen, Tomasz Korbak, Jared Ka-\nplan, Deep Ganguli, Samuel R. Bowman, Ethan Perez, Roger Baker Grosse, and David Duve-\nnaud. Many-shot jailbreaking, 2024. URL https://www.anthropic.com/research/\nmany-shot-jailbreaking.\nAnthropic. Claude 3 haiku: our fastest model yet, 2024. URL https://www.anthropic.com/\nnews/claude-3-haiku.\nYushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du,\nXiao Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li. Longbench: A bilingual,\nmultitask benchmark for long context understanding, 2024. URL https://arxiv.org/abs/\n2308.14508.\n11Amanda Bertsch, Uri Alon, Graham Neubig, and Matthew R. Gormley. Unlimiformer: Long-range\ntransformers with unlimited length input, 2023. URL https://arxiv.org/abs/2305.\n01625.\nYapei Chang, Kyle Lo, Tanya Goyal, and Mohit Iyyer. Booookscore: A systematic exploration of\nbook-length summarization in the era of llms, 2024. URL https://arxiv.org/abs/2310.\n00785.\nShouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. Extending context window of\nlarge language models via positional interpolation, 2023. URL https://arxiv.org/abs/\n2306.15595.\nTri Dao and Albert Gu. Transformers are ssms: Generalized models and efficient algorithms through\nstructured state space duality, 2024. URL https://arxiv.org/abs/2405.21060.\nDeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu,\nQihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu,\nZhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao\nWu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan,\nDamai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao,\nGuanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding,\nHuajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang\nChen, Jingyang Yuan, Junjie Qiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong,\nKai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao,\nLitong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang,\nMeng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang,\nQinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L.\nJin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang,\nShuiping Yu, Shunfeng Zhou, Shuting Pan, S. S. Li, Shuang Zhou, Shaoqing Wu, Shengfeng\nYe, Tao Yun, Tian Pei, Tianyu Sun, T. Wang, Wangding Zeng, Wanjia Zhao, Wen Liu, Wenfeng\nLiang, Wenjun Gao, Wenqin Yu, Wentao Zhang, W. L. Xiao, Wei An, Xiaodong Liu, Xiaohan\nWang, Xiaokang Chen, Xiaotao Nie, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xinyu Yang,\nXinyuan Li, Xuecheng Su, Xuheng Lin, X. Q. Li, Xiangyue Jin, Xiaojin Shen, Xiaosha Chen,\nXiaowen Sun, Xiaoxiang Wang, Xinnan Song, Xinyi Zhou, Xianzu Wang, Xinxia Shan, Y . K. Li,\nY . Q. Wang, Y . X. Wei, Yang Zhang, Yanhong Xu, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Wang,\nYi Yu, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Yishi Piao, Yisong Wang, Yixuan Tan,\nYiyang Ma, Yiyuan Liu, Yongqiang Guo, Yuan Ou, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia\nHe, Yunfan Xiong, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Y . X. Zhu, Yanhong\nXu, Yanping Huang, Yaohui Li, Yi Zheng, Yuchen Zhu, Yunxian Ma, Ying Tang, Yukun Zha,\nYuting Yan, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhenda Xie, Zhengyan Zhang,\nZhewen Hao, Zhicheng Ma, Zhigang Yan, Zhiyu Wu, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li,\nZiwei Xie, Ziyang Song, Zizheng Pan, Zhen Huang, Zhipeng Xu, Zhongyu Zhang, and Zhen\nZhang. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025.\nURL https://arxiv.org/abs/2501.12948.\nSimon Frieder, Luca Pinchetti, Alexis Chevalier, Ryan-Rhys Griffiths, Tommaso Salvatori, Thomas\nLukasiewicz, Philipp Christian Petersen, and Julius Berner. Mathematical capabilities of chatgpt,\n2023. URL https://arxiv.org/abs/2301.13867.\nDaniel Y . Fu, Elliot L. Epstein, Eric Nguyen, Armin W. Thomas, Michael Zhang, Tri Dao, Atri Rudra,\nand Christopher Ré. Simple hardware-efficient long convolutions for sequence modeling, 2023.\nURL https://arxiv.org/abs/2302.06646.\nTianyu Gao, Alexander Wettig, Howard Yen, and Danqi Chen. How to train long-context language\nmodels (effectively), 2025. URL https://arxiv.org/abs/2410.02660.\nGoogle. Gemini 2.5: Our most intelligent ai model, 2025.\nURL https://blog.google/technology/google-deepmind/\ngemini-model-thinking-updates-march-2025/ .\nAaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad\nAl-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, Amy Yang, Angela Fan,\n12Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev,\nArthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, Austen Gregerson, Ava Spataru,\nBaptiste Roziere, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak,\nChloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu,\nCorinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle\nPintz, Danny Livshits, Danny Wyatt, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego\nGarcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova,\nEmily Dinan, Eric Michael Smith, Filip Radenovic, Francisco Guzmán, Frank Zhang, Gabriel\nSynnaeve, Gabrielle Lee, Georgia Lewis Anderson, Govind Thattai, Graeme Nail, Gregoire Mialon,\nGuan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan\nZarov, Imanol Arrieta Ibarra, Isabel Kloumann, Ishan Misra, Ivan Evtimov, Jack Zhang, Jade Copet,\nJaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde,\nJennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie\nWang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua\nSaxe, Junteng Jia, Kalyan Vasuden Alwala, Karthik Prasad, Kartikeya Upasani, Kate Plawiak,\nKe Li, Kenneth Heafield, Kevin Stone, Khalid El-Arini, Krithika Iyer, Kshitiz Malik, Kuenley\nChiu, Kunal Bhalla, Kushal Lakhotia, Lauren Rantala-Yeary, Laurens van der Maaten, Lawrence\nChen, Liang Tan, Liz Jenkins, Louis Martin, Lovish Madaan, Lubo Malo, Lukas Blecher, Lukas\nLandzaat, Luke de Oliveira, Madeline Muzzi, Mahesh Pasupuleti, Mannat Singh, Manohar Paluri,\nMarcin Kardas, Maria Tsimpoukelli, Mathew Oldham, Mathieu Rita, Maya Pavlova, Melanie\nKambadur, Mike Lewis, Min Si, Mitesh Kumar Singh, Mona Hassan, Naman Goyal, Narjes\nTorabi, Nikolay Bashlykov, Nikolay Bogoychev, Niladri Chatterji, Ning Zhang, Olivier Duchenne,\nOnur Çelebi, Patrick Alrassy, Pengchuan Zhang, Pengwei Li, Petar Vasic, Peter Weng, Prajjwal\nBhargava, Pratik Dubal, Praveen Krishnan, Punit Singh Koura, Puxin Xu, Qing He, Qingxiao Dong,\nRagavan Srinivasan, Raj Ganapathy, Ramon Calderer, Ricardo Silveira Cabral, Robert Stojnic,\nRoberta Raileanu, Rohan Maheswari, Rohit Girdhar, Rohit Patel, Romain Sauvestre, Ronnie\nPolidoro, Roshan Sumbaly, Ross Taylor, Ruan Silva, Rui Hou, Rui Wang, Saghar Hosseini, Sahana\nChennabasappa, Sanjay Singh, Sean Bell, Seohyun Sonia Kim, Sergey Edunov, Shaoliang Nie,\nSharan Narang, Sharath Raparthy, Sheng Shen, Shengye Wan, Shruti Bhosale, Shun Zhang, Simon\nVandenhende, Soumya Batra, Spencer Whitman, Sten Sootla, Stephane Collot, Suchin Gururangan,\nSydney Borodinsky, Tamar Herman, Tara Fowler, Tarek Sheasha, Thomas Georgiou, Thomas\nScialom, Tobias Speckbacher, Todor Mihaylov, Tong Xiao, Ujjwal Karn, Vedanuj Goswami,\nVibhor Gupta, Vignesh Ramanathan, Viktor Kerkez, Vincent Gonguet, Virginie Do, Vish V ogeti,\nVítor Albiero, Vladan Petrovic, Weiwei Chu, Wenhan Xiong, Wenyin Fu, Whitney Meers, Xavier\nMartinet, Xiaodong Wang, Xiaofang Wang, Xiaoqing Ellen Tan, Xide Xia, Xinfeng Xie, Xuchao\nJia, Xuewei Wang, Yaelle Goldschlag, Yashesh Gaur, Yasmine Babaei, Yi Wen, Yiwen Song,\nYuchen Zhang, Yue Li, Yuning Mao, Zacharie Delpierre Coudert, Zheng Yan, Zhengxing Chen, Zoe\nPapakipos, Aaditya Singh, Aayushi Srivastava, Abha Jain, Adam Kelsey, Adam Shajnfeld, Adithya\nGangidi, Adolfo Victoria, Ahuva Goldstand, Ajay Menon, Ajay Sharma, Alex Boesenberg, Alexei\nBaevski, Allie Feinstein, Amanda Kallet, Amit Sangani, Amos Teo, Anam Yunus, Andrei Lupu,\nAndres Alvarado, Andrew Caples, Andrew Gu, Andrew Ho, Andrew Poulton, Andrew Ryan, Ankit\nRamchandani, Annie Dong, Annie Franco, Anuj Goyal, Aparajita Saraf, Arkabandhu Chowdhury,\nAshley Gabriel, Ashwin Bharambe, Assaf Eisenman, Azadeh Yazdan, Beau James, Ben Maurer,\nBenjamin Leonhardi, Bernie Huang, Beth Loyd, Beto De Paola, Bhargavi Paranjape, Bing Liu,\nBo Wu, Boyu Ni, Braden Hancock, Bram Wasti, Brandon Spence, Brani Stojkovic, Brian Gamido,\nBritt Montalvo, Carl Parker, Carly Burton, Catalina Mejia, Ce Liu, Changhan Wang, Changkyu\nKim, Chao Zhou, Chester Hu, Ching-Hsiang Chu, Chris Cai, Chris Tindal, Christoph Feichtenhofer,\nCynthia Gao, Damon Civin, Dana Beaty, Daniel Kreymer, Daniel Li, David Adkins, David Xu,\nDavide Testuggine, Delia David, Devi Parikh, Diana Liskovich, Didem Foss, Dingkang Wang, Duc\nLe, Dustin Holland, Edward Dowling, Eissa Jamil, Elaine Montgomery, Eleonora Presani, Emily\nHahn, Emily Wood, Eric-Tuan Le, Erik Brinkman, Esteban Arcaute, Evan Dunbar, Evan Smothers,\nFei Sun, Felix Kreuk, Feng Tian, Filippos Kokkinos, Firat Ozgenel, Francesco Caggioni, Frank\nKanayet, Frank Seide, Gabriela Medina Florez, Gabriella Schwarz, Gada Badeer, Georgia Swee,\nGil Halpern, Grant Herman, Grigory Sizov, Guangyi, Zhang, Guna Lakshminarayanan, Hakan Inan,\nHamid Shojanazeri, Han Zou, Hannah Wang, Hanwen Zha, Haroun Habeeb, Harrison Rudolph,\nHelen Suk, Henry Aspegren, Hunter Goldman, Hongyuan Zhan, Ibrahim Damlaj, Igor Molybog,\nIgor Tufanov, Ilias Leontiadis, Irina-Elena Veliche, Itai Gat, Jake Weissman, James Geboski, James\nKohli, Janice Lam, Japhet Asher, Jean-Baptiste Gaya, Jeff Marcus, Jeff Tang, Jennifer Chan, Jenny\nZhen, Jeremy Reizenstein, Jeremy Teboul, Jessica Zhong, Jian Jin, Jingyi Yang, Joe Cummings,\n13Jon Carvill, Jon Shepard, Jonathan McPhie, Jonathan Torres, Josh Ginsburg, Junjie Wang, Kai\nWu, Kam Hou U, Karan Saxena, Kartikay Khandelwal, Katayoun Zand, Kathy Matosich, Kaushik\nVeeraraghavan, Kelly Michelena, Keqian Li, Kiran Jagadeesh, Kun Huang, Kunal Chawla, Kyle\nHuang, Lailin Chen, Lakshya Garg, Lavender A, Leandro Silva, Lee Bell, Lei Zhang, Liangpeng\nGuo, Licheng Yu, Liron Moshkovich, Luca Wehrstedt, Madian Khabsa, Manav Avalani, Manish\nBhatt, Martynas Mankus, Matan Hasson, Matthew Lennie, Matthias Reso, Maxim Groshev, Maxim\nNaumov, Maya Lathi, Meghan Keneally, Miao Liu, Michael L. Seltzer, Michal Valko, Michelle\nRestrepo, Mihir Patel, Mik Vyatskov, Mikayel Samvelyan, Mike Clark, Mike Macey, Mike Wang,\nMiquel Jubert Hermoso, Mo Metanat, Mohammad Rastegari, Munish Bansal, Nandhini Santhanam,\nNatascha Parks, Natasha White, Navyata Bawa, Nayan Singhal, Nick Egebo, Nicolas Usunier,\nNikhil Mehta, Nikolay Pavlovich Laptev, Ning Dong, Norman Cheng, Oleg Chernoguz, Olivia\nHart, Omkar Salpekar, Ozlem Kalinli, Parkin Kent, Parth Parekh, Paul Saab, Pavan Balaji, Pedro\nRittner, Philip Bontrager, Pierre Roux, Piotr Dollar, Polina Zvyagina, Prashant Ratanchandani,\nPritish Yuvraj, Qian Liang, Rachad Alao, Rachel Rodriguez, Rafi Ayub, Raghotham Murthy,\nRaghu Nayani, Rahul Mitra, Rangaprabhu Parthasarathy, Raymond Li, Rebekkah Hogan, Robin\nBattey, Rocky Wang, Russ Howes, Ruty Rinott, Sachin Mehta, Sachin Siby, Sai Jayesh Bondu,\nSamyak Datta, Sara Chugh, Sara Hunt, Sargun Dhillon, Sasha Sidorov, Satadru Pan, Saurabh\nMahajan, Saurabh Verma, Seiji Yamamoto, Sharadh Ramaswamy, Shaun Lindsay, Shaun Lindsay,\nSheng Feng, Shenghao Lin, Shengxin Cindy Zha, Shishir Patil, Shiva Shankar, Shuqiang Zhang,\nShuqiang Zhang, Sinong Wang, Sneha Agarwal, Soji Sajuyigbe, Soumith Chintala, Stephanie\nMax, Stephen Chen, Steve Kehoe, Steve Satterfield, Sudarshan Govindaprasad, Sumit Gupta,\nSummer Deng, Sungmin Cho, Sunny Virk, Suraj Subramanian, Sy Choudhury, Sydney Goldman,\nTal Remez, Tamar Glaser, Tamara Best, Thilo Koehler, Thomas Robinson, Tianhe Li, Tianjun\nZhang, Tim Matthews, Timothy Chou, Tzook Shaked, Varun V ontimitta, Victoria Ajayi, Victoria\nMontanez, Vijai Mohan, Vinay Satish Kumar, Vishal Mangla, Vlad Ionescu, Vlad Poenaru,\nVlad Tiberiu Mihailescu, Vladimir Ivanov, Wei Li, Wenchen Wang, Wenwen Jiang, Wes Bouaziz,\nWill Constable, Xiaocheng Tang, Xiaojian Wu, Xiaolan Wang, Xilun Wu, Xinbo Gao, Yaniv\nKleinman, Yanjun Chen, Ye Hu, Ye Jia, Ye Qi, Yenda Li, Yilin Zhang, Ying Zhang, Yossi Adi,\nYoungjin Nam, Yu, Wang, Yu Zhao, Yuchen Hao, Yundi Qian, Yunlu Li, Yuzi He, Zach Rait,\nZachary DeVito, Zef Rosnbrick, Zhaoduo Wen, Zhenyu Yang, Zhiwei Zhao, and Zhiyu Ma. The\nllama 3 herd of models, 2024. URL https://arxiv.org/abs/2407.21783.\nAlbert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces, 2024.\nURL https://arxiv.org/abs/2312.00752.\nCheng-Ping Hsieh, Simeng Sun, Samuel Kriman, Shantanu Acharya, Dima Rekesh, Fei Jia, Yang\nZhang, and Boris Ginsburg. Ruler: What’s the real context size of your long-context language\nmodels?, 2024. URL https://arxiv.org/abs/2404.06654.\nAlbert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris\nBamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand,\nGianna Lengyel, Guillaume Bour, Guillaume Lample, Lélio Renard Lavaud, Lucile Saulnier, Marie-\nAnne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le\nScao, Théophile Gervet, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed.\nMixtral of experts, 2024. URL https://arxiv.org/abs/2401.04088.\nGregory Kamradt. Needle in a haystack - pressure testing llms, 2023. URL https://github.\ncom/gkamradt/LLMTest_NeedleInAHaystack.\nYekyung Kim, Yapei Chang, Marzena Karpinska, Aparna Garimella, Varun Manjunatha, Kyle Lo,\nTanya Goyal, and Mohit Iyyer. Fables: Evaluating faithfulness and content selection in book-length\nsummarization, 2024. URL https://arxiv.org/abs/2404.01261.\nRyan Koo, Minhwa Lee, Vipul Raheja, Jong Inn Park, Zae Myung Kim, and Dongyeop Kang.\nBenchmarking cognitive biases in large language models as evaluators, 2024. URL https:\n//arxiv.org/abs/2309.17012.\nTomáš Koˇciský, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, Gábor Melis,\nand Edward Grefenstette. The narrativeqa reading comprehension challenge, 2017. URL https:\n//arxiv.org/abs/1712.07040.\n14Jinhyuk Lee, Anthony Chen, Zhuyun Dai, Dheeru Dua, Devendra Singh Sachan, Michael Boratko,\nYi Luan, Sébastien M. R. Arnold, Vincent Perot, Siddharth Dalmia, Hexiang Hu, Xudong Lin,\nPanupong Pasupat, Aida Amini, Jeremy R. Cole, Sebastian Riedel, Iftekhar Naim, Ming-Wei\nChang, and Kelvin Guu. Can long-context language models subsume retrieval, rag, sql, and more?,\n2024. URL https://arxiv.org/abs/2406.13121.\nMeta. The llama 4 herd: The beginning of a new era of natively multimodal ai innovation, 2025.\nURL https://ai.meta.com/blog/llama-4-multimodal-intelligence/ .\nOpenAI. Openai o3-mini, pushing the frontier of cost-effective reasoning., 2025. URL https:\n//openai.com/index/openai-o3-mini/.\nOpenAI, :, Aaron Hurst, Adam Lerer, Adam P. Goucher, Adam Perelman, Aditya Ramesh, Aidan\nClark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, Aleksander M ˛ adry, Alex Baker-\nWhitcomb, Alex Beutel, Alex Borzunov, Alex Carney, Alex Chow, Alex Kirillov, Alex Nichol, Alex\nPaino, Alex Renzin, Alex Tachard Passos, Alexander Kirillov, Alexi Christakis, Alexis Conneau,\nAli Kamali, Allan Jabri, Allison Moyer, Allison Tam, Amadou Crookes, Amin Tootoochian,\nAmin Tootoonchian, Ananya Kumar, Andrea Vallone, Andrej Karpathy, Andrew Braunstein,\nAndrew Cann, Andrew Codispoti, Andrew Galu, Andrew Kondrich, Andrew Tulloch, Andrey\nMishchenko, Angela Baek, Angela Jiang, Antoine Pelisse, Antonia Woodford, Anuj Gosalia,\nArka Dhar, Ashley Pantuliano, Avi Nayak, Avital Oliver, Barret Zoph, Behrooz Ghorbani, Ben\nLeimberger, Ben Rossen, Ben Sokolowsky, Ben Wang, Benjamin Zweig, Beth Hoover, Blake\nSamic, Bob McGrew, Bobby Spero, Bogo Giertler, Bowen Cheng, Brad Lightcap, Brandon\nWalkin, Brendan Quinn, Brian Guarraci, Brian Hsu, Bright Kellogg, Brydon Eastman, Camillo\nLugaresi, Carroll Wainwright, Cary Bassin, Cary Hudson, Casey Chu, Chad Nelson, Chak Li,\nChan Jun Shern, Channing Conger, Charlotte Barette, Chelsea V oss, Chen Ding, Cheng Lu,\nChong Zhang, Chris Beaumont, Chris Hallacy, Chris Koch, Christian Gibson, Christina Kim,\nChristine Choi, Christine McLeavey, Christopher Hesse, Claudia Fischer, Clemens Winter, Coley\nCzarnecki, Colin Jarvis, Colin Wei, Constantin Koumouzelis, Dane Sherburn, Daniel Kappler,\nDaniel Levin, Daniel Levy, David Carr, David Farhi, David Mely, David Robinson, David Sasaki,\nDenny Jin, Dev Valladares, Dimitris Tsipras, Doug Li, Duc Phong Nguyen, Duncan Findlay,\nEdede Oiwoh, Edmund Wong, Ehsan Asdar, Elizabeth Proehl, Elizabeth Yang, Eric Antonow, Eric\nKramer, Eric Peterson, Eric Sigler, Eric Wallace, Eugene Brevdo, Evan Mays, Farzad Khorasani,\nFelipe Petroski Such, Filippo Raso, Francis Zhang, Fred von Lohmann, Freddie Sulit, Gabriel Goh,\nGene Oden, Geoff Salmon, Giulio Starace, Greg Brockman, Hadi Salman, Haiming Bao, Haitang\nHu, Hannah Wong, Haoyu Wang, Heather Schmidt, Heather Whitney, Heewoo Jun, Hendrik\nKirchner, Henrique Ponde de Oliveira Pinto, Hongyu Ren, Huiwen Chang, Hyung Won Chung,\nIan Kivlichan, Ian O’Connell, Ian O’Connell, Ian Osband, Ian Silber, Ian Sohl, Ibrahim Okuyucu,\nIkai Lan, Ilya Kostrikov, Ilya Sutskever, Ingmar Kanitscheider, Ishaan Gulrajani, Jacob Coxon,\nJacob Menick, Jakub Pachocki, James Aung, James Betker, James Crooks, James Lennon, Jamie\nKiros, Jan Leike, Jane Park, Jason Kwon, Jason Phang, Jason Teplitz, Jason Wei, Jason Wolfe,\nJay Chen, Jeff Harris, Jenia Varavva, Jessica Gan Lee, Jessica Shieh, Ji Lin, Jiahui Yu, Jiayi\nWeng, Jie Tang, Jieqi Yu, Joanne Jang, Joaquin Quinonero Candela, Joe Beutler, Joe Landers,\nJoel Parish, Johannes Heidecke, John Schulman, Jonathan Lachman, Jonathan McKay, Jonathan\nUesato, Jonathan Ward, Jong Wook Kim, Joost Huizinga, Jordan Sitkin, Jos Kraaijeveld, Josh\nGross, Josh Kaplan, Josh Snyder, Joshua Achiam, Joy Jiao, Joyce Lee, Juntang Zhuang, Justyn\nHarriman, Kai Fricke, Kai Hayashi, Karan Singhal, Katy Shi, Kavin Karthik, Kayla Wood, Kendra\nRimbach, Kenny Hsu, Kenny Nguyen, Keren Gu-Lemberg, Kevin Button, Kevin Liu, Kiel Howe,\nKrithika Muthukumar, Kyle Luther, Lama Ahmad, Larry Kai, Lauren Itow, Lauren Workman,\nLeher Pathak, Leo Chen, Li Jing, Lia Guy, Liam Fedus, Liang Zhou, Lien Mamitsuka, Lilian Weng,\nLindsay McCallum, Lindsey Held, Long Ouyang, Louis Feuvrier, Lu Zhang, Lukas Kondraciuk,\nLukasz Kaiser, Luke Hewitt, Luke Metz, Lyric Doshi, Mada Aflak, Maddie Simens, Madelaine\nBoyd, Madeleine Thompson, Marat Dukhan, Mark Chen, Mark Gray, Mark Hudnall, Marvin\nZhang, Marwan Aljubeh, Mateusz Litwin, Matthew Zeng, Max Johnson, Maya Shetty, Mayank\nGupta, Meghan Shah, Mehmet Yatbaz, Meng Jia Yang, Mengchao Zhong, Mia Glaese, Mianna\nChen, Michael Janner, Michael Lampe, Michael Petrov, Michael Wu, Michele Wang, Michelle\nFradin, Michelle Pokrass, Miguel Castro, Miguel Oom Temudo de Castro, Mikhail Pavlov, Miles\nBrundage, Miles Wang, Minal Khan, Mira Murati, Mo Bavarian, Molly Lin, Murat Yesildal, Nacho\nSoto, Natalia Gimelshein, Natalie Cone, Natalie Staudacher, Natalie Summers, Natan LaFontaine,\nNeil Chowdhury, Nick Ryder, Nick Stathas, Nick Turley, Nik Tezak, Niko Felix, Nithanth Kudige,\n15Nitish Keskar, Noah Deutsch, Noel Bundick, Nora Puckett, Ofir Nachum, Ola Okelola, Oleg Boiko,\nOleg Murk, Oliver Jaffe, Olivia Watkins, Olivier Godement, Owen Campbell-Moore, Patrick\nChao, Paul McMillan, Pavel Belov, Peng Su, Peter Bak, Peter Bakkum, Peter Deng, Peter Dolan,\nPeter Hoeschele, Peter Welinder, Phil Tillet, Philip Pronin, Philippe Tillet, Prafulla Dhariwal,\nQiming Yuan, Rachel Dias, Rachel Lim, Rahul Arora, Rajan Troll, Randall Lin, Rapha Gontijo\nLopes, Raul Puri, Reah Miyara, Reimar Leike, Renaud Gaubert, Reza Zamani, Ricky Wang, Rob\nDonnelly, Rob Honsby, Rocky Smith, Rohan Sahai, Rohit Ramchandani, Romain Huet, Rory\nCarmichael, Rowan Zellers, Roy Chen, Ruby Chen, Ruslan Nigmatullin, Ryan Cheu, Saachi\nJain, Sam Altman, Sam Schoenholz, Sam Toizer, Samuel Miserendino, Sandhini Agarwal, Sara\nCulver, Scott Ethersmith, Scott Gray, Sean Grove, Sean Metzger, Shamez Hermani, Shantanu\nJain, Shengjia Zhao, Sherwin Wu, Shino Jomoto, Shirong Wu, Shuaiqi, Xia, Sonia Phene, Spencer\nPapay, Srinivas Narayanan, Steve Coffey, Steve Lee, Stewart Hall, Suchir Balaji, Tal Broda, Tal\nStramer, Tao Xu, Tarun Gogineni, Taya Christianson, Ted Sanders, Tejal Patwardhan, Thomas\nCunninghman, Thomas Degry, Thomas Dimson, Thomas Raoux, Thomas Shadwell, Tianhao\nZheng, Todd Underwood, Todor Markov, Toki Sherbakov, Tom Rubin, Tom Stasi, Tomer Kaftan,\nTristan Heywood, Troy Peterson, Tyce Walters, Tyna Eloundou, Valerie Qi, Veit Moeller, Vinnie\nMonaco, Vishal Kuo, Vlad Fomenko, Wayne Chang, Weiyi Zheng, Wenda Zhou, Wesam Manassra,\nWill Sheu, Wojciech Zaremba, Yash Patil, Yilei Qian, Yongjik Kim, Youlong Cheng, Yu Zhang,\nYuchen He, Yuchen Zhang, Yujia Jin, Yunxing Dai, and Yury Malkov. Gpt-4o system card, 2024a.\nURL https://arxiv.org/abs/2410.21276.\nOpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni\nAleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, Red Avila, Igor\nBabuschkin, Suchir Balaji, Valerie Balcom, Paul Baltescu, Haiming Bao, Mohammad Bavarian,\nJeff Belgum, Irwan Bello, Jake Berdine, Gabriel Bernadett-Shapiro, Christopher Berner, Lenny\nBogdonoff, Oleg Boiko, Madelaine Boyd, Anna-Luisa Brakman, Greg Brockman, Tim Brooks,\nMiles Brundage, Kevin Button, Trevor Cai, Rosie Campbell, Andrew Cann, Brittany Carey, Chelsea\nCarlson, Rory Carmichael, Brooke Chan, Che Chang, Fotis Chantzis, Derek Chen, Sully Chen,\nRuby Chen, Jason Chen, Mark Chen, Ben Chess, Chester Cho, Casey Chu, Hyung Won Chung,\nDave Cummings, Jeremiah Currier, Yunxing Dai, Cory Decareaux, Thomas Degry, Noah Deutsch,\nDamien Deville, Arka Dhar, David Dohan, Steve Dowling, Sheila Dunning, Adrien Ecoffet, Atty\nEleti, Tyna Eloundou, David Farhi, Liam Fedus, Niko Felix, Simón Posada Fishman, Juston Forte,\nIsabella Fulford, Leo Gao, Elie Georges, Christian Gibson, Vik Goel, Tarun Gogineni, Gabriel\nGoh, Rapha Gontijo-Lopes, Jonathan Gordon, Morgan Grafstein, Scott Gray, Ryan Greene, Joshua\nGross, Shixiang Shane Gu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff Harris, Yuchen He, Mike\nHeaton, Johannes Heidecke, Chris Hesse, Alan Hickey, Wade Hickey, Peter Hoeschele, Brandon\nHoughton, Kenny Hsu, Shengli Hu, Xin Hu, Joost Huizinga, Shantanu Jain, Shawn Jain, Joanne\nJang, Angela Jiang, Roger Jiang, Haozhun Jin, Denny Jin, Shino Jomoto, Billie Jonn, Heewoo\nJun, Tomer Kaftan, Łukasz Kaiser, Ali Kamali, Ingmar Kanitscheider, Nitish Shirish Keskar,\nTabarak Khan, Logan Kilpatrick, Jong Wook Kim, Christina Kim, Yongjik Kim, Jan Hendrik\nKirchner, Jamie Kiros, Matt Knight, Daniel Kokotajlo, Łukasz Kondraciuk, Andrew Kondrich,\nAris Konstantinidis, Kyle Kosic, Gretchen Krueger, Vishal Kuo, Michael Lampe, Ikai Lan, Teddy\nLee, Jan Leike, Jade Leung, Daniel Levy, Chak Ming Li, Rachel Lim, Molly Lin, Stephanie\nLin, Mateusz Litwin, Theresa Lopez, Ryan Lowe, Patricia Lue, Anna Makanju, Kim Malfacini,\nSam Manning, Todor Markov, Yaniv Markovski, Bianca Martin, Katie Mayer, Andrew Mayne,\nBob McGrew, Scott Mayer McKinney, Christine McLeavey, Paul McMillan, Jake McNeil, David\nMedina, Aalok Mehta, Jacob Menick, Luke Metz, Andrey Mishchenko, Pamela Mishkin, Vinnie\nMonaco, Evan Morikawa, Daniel Mossing, Tong Mu, Mira Murati, Oleg Murk, David Mély,\nAshvin Nair, Reiichiro Nakano, Rajeev Nayak, Arvind Neelakantan, Richard Ngo, Hyeonwoo\nNoh, Long Ouyang, Cullen O’Keefe, Jakub Pachocki, Alex Paino, Joe Palermo, Ashley Pantuliano,\nGiambattista Parascandolo, Joel Parish, Emy Parparita, Alex Passos, Mikhail Pavlov, Andrew Peng,\nAdam Perelman, Filipe de Avila Belbute Peres, Michael Petrov, Henrique Ponde de Oliveira Pinto,\nMichael, Pokorny, Michelle Pokrass, Vitchyr H. Pong, Tolly Powell, Alethea Power, Boris Power,\nElizabeth Proehl, Raul Puri, Alec Radford, Jack Rae, Aditya Ramesh, Cameron Raymond, Francis\nReal, Kendra Rimbach, Carl Ross, Bob Rotsted, Henri Roussez, Nick Ryder, Mario Saltarelli, Ted\nSanders, Shibani Santurkar, Girish Sastry, Heather Schmidt, David Schnurr, John Schulman, Daniel\nSelsam, Kyla Sheppard, Toki Sherbakov, Jessica Shieh, Sarah Shoker, Pranav Shyam, Szymon\nSidor, Eric Sigler, Maddie Simens, Jordan Sitkin, Katarina Slama, Ian Sohl, Benjamin Sokolowsky,\nYang Song, Natalie Staudacher, Felipe Petroski Such, Natalie Summers, Ilya Sutskever, Jie\n16Tang, Nikolas Tezak, Madeleine B. Thompson, Phil Tillet, Amin Tootoonchian, Elizabeth Tseng,\nPreston Tuggle, Nick Turley, Jerry Tworek, Juan Felipe Cerón Uribe, Andrea Vallone, Arun\nVijayvergiya, Chelsea V oss, Carroll Wainwright, Justin Jay Wang, Alvin Wang, Ben Wang,\nJonathan Ward, Jason Wei, CJ Weinmann, Akila Welihinda, Peter Welinder, Jiayi Weng, Lilian\nWeng, Matt Wiethoff, Dave Willner, Clemens Winter, Samuel Wolrich, Hannah Wong, Lauren\nWorkman, Sherwin Wu, Jeff Wu, Michael Wu, Kai Xiao, Tao Xu, Sarah Yoo, Kevin Yu, Qiming\nYuan, Wojciech Zaremba, Rowan Zellers, Chong Zhang, Marvin Zhang, Shengjia Zhao, Tianhao\nZheng, Juntang Zhuang, William Zhuk, and Barret Zoph. Gpt-4 technical report, 2024b. URL\nhttps://arxiv.org/abs/2303.08774.\nAllison Parrish. gutenberg-poetry-corpus: A corpus of poetry from project gutenberg, 2018.\nBo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Stella Biderman, Huanqi\nCao, Xin Cheng, Michael Chung, Matteo Grella, Kranthi Kiran GV , Xuzheng He, Haowen Hou,\nJiaju Lin, Przemyslaw Kazienko, Jan Kocon, Jiaming Kong, Bartlomiej Koptyra, Hayden Lau,\nKrishna Sri Ipsit Mantri, Ferdinand Mom, Atsushi Saito, Guangyu Song, Xiangru Tang, Bolun\nWang, Johan S. Wind, Stanislaw Wozniak, Ruichong Zhang, Zhenyuan Zhang, Qihang Zhao, Peng\nZhou, Qinghua Zhou, Jian Zhu, and Rui-Jie Zhu. Rwkv: Reinventing rnns for the transformer era,\n2023. URL https://arxiv.org/abs/2305.13048.\nOfir Press, Noah A. Smith, and Mike Lewis. Train short, test long: Attention with linear biases\nenables input length extrapolation, 2022. URL https://arxiv.org/abs/2108.12409.\nQwen. Qwen3 technical report, 2025a. URL https://github.com/QwenLM/Qwen3/blob/\nmain/Qwen3_Technical_Report.pdf.\nQwen. Qwq-32b: Embracing the power of reinforcement learning, 2025b. URLhttps://qwenlm.\ngithub.io/blog/qwq-32b/.\nBaptiste Rozière, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi\nAdi, Jingyu Liu, Romain Sauvestre, Tal Remez, Jérémy Rapin, Artyom Kozhevnikov, Ivan Evtimov,\nJoanna Bitton, Manish Bhatt, Cristian Canton Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre\nDéfossez, Jade Copet, Faisal Azhar, Hugo Touvron, Louis Martin, Nicolas Usunier, Thomas\nScialom, and Gabriel Synnaeve. Code llama: Open foundation models for code, 2024. URL\nhttps://arxiv.org/abs/2308.12950.\nUri Shaham, Maor Ivgi, Avia Efrat, Jonathan Berant, and Omer Levy. ZeroSCROLLS: A\nzero-shot benchmark for long text understanding. In Houda Bouamor, Juan Pino, and Ka-\nlika Bali, editors, Findings of the Association for Computational Linguistics: EMNLP 2023,\npages 7977–7989, Singapore, December 2023. Association for Computational Linguistics.\ndoi: 10.18653/v1/2023.findings-emnlp.536. URL https://aclanthology.org/2023.\nfindings-emnlp.536/.\nJianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. Roformer: Enhanced\ntransformer with rotary position embedding, 2023. URL https://arxiv.org/abs/2104.\n09864.\nYutao Sun, Li Dong, Barun Patra, Shuming Ma, Shaohan Huang, Alon Benhaim, Vishrav Chaudhary,\nXia Song, and Furu Wei. A length-extrapolatable transformer, 2022. URL https://arxiv.\norg/abs/2212.10554.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing\nsystems, 30, 2017.\nKiran V odrahalli, Santiago Ontanon, Nilesh Tripuraneni, Kelvin Xu, Sanil Jain, Rakesh Shivanna,\nJeffrey Hui, Nishanth Dikkala, Mehran Kazemi, Bahare Fatemi, Rohan Anil, Ethan Dyer, Siamak\nShakeri, Roopali Vij, Harsh Mehta, Vinay Ramasesh, Quoc Le, Ed Chi, Yifeng Lu, Orhan Firat,\nAngeliki Lazaridou, Jean-Baptiste Lespiau, Nithya Attaluri, and Kate Olszewska. Michelangelo:\nLong context evaluations beyond haystacks via latent structure queries, 2024. URL https:\n//arxiv.org/abs/2409.12640.\n17Cunxiang Wang, Ruoxi Ning, Boqi Pan, Tonghui Wu, Qipeng Guo, Cheng Deng, Guangsheng Bao,\nXiangkun Hu, Zheng Zhang, Qian Wang, and Yue Zhang. Novelqa: Benchmarking question\nanswering on documents exceeding 200k tokens, 2025. URL https://arxiv.org/abs/\n2403.12766.\nxAI. Grok 3 beta — the age of reasoning agents, 2025. URL https://x.ai/news/grok-3.\nXiaoyue Xu, Qinyuan Ye, and Xiang Ren. Stress-testing long-context language models with lifelong\nicl and task haystack, 2024. URL https://arxiv.org/abs/2407.16695.\nAn Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li,\nDayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin\nYang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang,\nLe Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tingyu Xia,\nXingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu\nCui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report. arXiv preprint arXiv:2412.15115,\n2024.\nHoward Yen, Tianyu Gao, and Danqi Chen. Long-context language modeling with parallel context\nencoding, 2024. URL https://arxiv.org/abs/2402.16617.\nHoward Yen, Tianyu Gao, Minmin Hou, Ke Ding, Daniel Fleischer, Peter Izsak, Moshe Wasserblat,\nand Danqi Chen. Helmet: How to evaluate long-context language models effectively and thor-\noughly, 2025. URL https://arxiv.org/abs/2410.02694.\nXinrong Zhang, Yingfa Chen, Shengding Hu, Zihang Xu, Junhao Chen, Moo Khai Hao, Xu Han,\nZhen Leng Thai, Shuo Wang, Zhiyuan Liu, and Maosong Sun. ∞bench: Extending long context\nevaluation beyond 100k tokens, 2024. URL https://arxiv.org/abs/2402.13718.\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang,\nZi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica.\nJudging llm-as-a-judge with mt-bench and chatbot arena, 2023. URL https://arxiv.org/\nabs/2306.05685.\n18A Prompt Templates\nWe show the detailed prompt templates for language model evaluation across each domain included\nin AbsenceBench in Table 7 that are used to obtain the evaluation results in Table 3. Additionally,\nTable 8 presents prompt templates with adapted instructions used for assessing the NIAH test under\nthe poetry and GitHub PRs domains.\nPoetry(default)\nSystem Prompt\nYou are helping a student practice memorizing poems. The student will\nrecite a poem, but they may have missed some lines. Your task is to\nidentify exactly which lines are missing from their recitation.\nList only the missing lines, nothing else.\nUser Message\nHere is the complete original poem:\n{original poem}\nNow, here is my recitation which may be missing some lines:\n{modified poem}\nWhat lines did I miss? Please list only the missing lines, nothing else.\nNumerical Sequences(default)\nSystem Prompt\nYou are helping a student practice reciting sequences. The student will\nrecite a sequence, but they may have missed some numbers. Your task is to\nidentify exactly which numbers are missing from their recitation.\nList only the missing numbers, nothing else\nUser Message\nHere is a sequence of numbers:\n{original sequence}\nNow, here is my recitation of the sequence which may be missing some numbers:\n{modified sequence}\nWhat numbers did I miss? Please list only the missing numbers, nothing else.\nGitHub PRs(default)\nSystem Prompt\nYou are helping a software developer determine if their merge of a pull\nrequest was successful. The developer had to edit the commit history\nand just wants to make sure that they have not changed what will be merged.\nThey will list the changed lines. Your job is to figure out if they have\nmissed any insertions or deletions from the original merge. Only pay\nattention to the insertions and deletions (ignore the context of the diff).\nUser Message\nHere is the complete original diff:\n{original diff}\nAnd here is the merge diff after the developer fixed the commit history:\n{modified diff}\nWhat changed lines (insertions or deletions) present in the original diff\nare missing in the merge diff (if any)?\nList only the missing changed lines, nothing else.\nTable 6: Detailed prompt templates used for evaluating language models under each domain of\nAbsenceBench\n19Poetry(post-instruction)\nSystem Prompt\nYou are helping a student practice memorizing poems. The student will\nrecite a poem, but they may have missed some lines. Your task is to\nidentify exactly which lines are missing from their recitation.\nList only the missing lines, nothing else.\nUser Message\n{original poem}\n{modified poem}\nThe original poem is followed by my recitation of the poem which may be\nmissing some lines.\nWhat lines did I miss? Please list only the missing lines, nothing else.\nNumerical Sequences(post-instruction)\nSystem Prompt\nYou are helping a student practice reciting sequences. The student will\nrecite a sequence, but they may have missed some numbers. Your task is to\nidentify exactly which numbers are missing from their recitation.\nList only the missing numbers, nothing else\nUser Message\n{original sequence}\n{modified sequence}\nThe original sequence is followed by my recitation of the sequence which\nmay be missing some numbers.\nWhat numbers did I miss? Please list only the missing numbers, nothing else.\nGitHub PRs(post-instruction)\nSystem Prompt\nYou are helping a software developer determine if their merge of a pull\nrequest was successful. The developer had to edit the commit history\nand just wants to make sure that they have not changed what will be merged.\nThey will list the changed lines. Your job is to figure out if they have\nmissed any insertions or deletions from the original merge. Only pay\nattention to the insertions and deletions (ignore the context of the diff).\nUser Message\n{original diff}\n{modified diff}\nThe original diff is followed by a merge diff after the developer fixed\nthe commit history\nWhat changed lines (insertions or deletions) present in the original diff\nare missing in the merge diff (if any)?\nList only the missing changed lines, nothing else.\nTable 7: Perturbed prompt templates where the task instructions are positioned after the original and\nmodified documents.\n20Poetry(NIAH)\nSystem Prompt\nYou are helping a student practice memorizing poems. The student will\nrecite a poem, but they may have added some random lines that related\nto Harry Potter characters. Your task is to identify exactly which lines\nare not in the original poem. List only the missing lines, nothing else.\nUser Message\nHere is the complete original poem:\n{original poem}\nNow, here is my recitation with some extra lines that is related to\nHarry Potter novel series:\n{modified poem}\nWhat lines did I miss? Please list only the extra lines, nothing else.\nGitHub PRs(NIAH)\nSystem Prompt\nYou are helping a software developer determine if their merge of a pull\nrequest was successful. The developer had to edit the commit history\nand accidently added some random lines related to Harry Potter characters.\nThey will list the changed lines. Your job is to figure out if they have\nadded any insertions from the original merge. Only pay attention to the\ninsertions.\nUser Message\nHere is the complete original diff:\n{original diff}\nAnd here is the merge diff after the developer fixed the commit history:\n{modified diff}\nWhat changed lines (insertions or deletions) present in the original diff\nare missing in the merge diff (if any)?\nList only the missing changed lines, nothing else.\nTable 8: Modified prompt templates used for evaluating language models under the NIAH test setting\nwithin the Poetry and GitHub PRs domains.\nB Inference Details\nWe evaluate a total of 14 LLMs onAbsenceBench. All model inferences are obtained through API\nrequests. We show the detailed information of each model’s maximum context length, API provider\nand reference in Table 9.\nModels Context Length API Provider API Reference\nGemini-2.5-flash (Google, 2025) 1M Google models/gemini-2.5-flash-preview-04-17Claude-3.7-Sonnet (Anthropic, 2024) 200K Anthropic claude-3-7-sonnet-latesto3-mini (OpenAI, 2025) 200K OpenAI o3-miniGPT-4.1 (OpenAI et al., 2024b) 1M OpenAI gpt-4.1GPT-4.1-mini (OpenAI et al., 2024b) 1M OpenAI gpt-4.1-miniGPT-4o (OpenAI et al., 2024a) 128K OpenAI gpt-4oGrok-3-mini-Beta (xAI, 2025) 131K xAI grok-3-mini-betaLlama-4-Maverick (Meta, 2025) 1M TogetherAI meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8Llama-3.3-70B-Instruct (Grattafiori et al., 2024) 131K TogetherAI meta-llama/Llama-3.3-70B-Instruct-TurboQwen3-235B (Qwen, 2025a) 41K TogetherAI Qwen/Qwen3-235B-A22B-fp8-tputQwen2.5-72B-Instruct (Yang et al., 2024) 32K TogetherAI Qwen/Qwen2.5-72B-Instruct-TurboQwQ-32B (Qwen, 2025b) 32K TogetherAI Qwen/QwQ-32BDeepSeek-R1 (DeepSeek-AI et al., 2025) 128K TogetherAI deepseek-ai/DeepSeek-R1Mixtral-8x7B-Instruct (Jiang et al., 2024) 32K TogetherAI mistralai/Mixtral-8x7B-Instruct-v0.1\nTable 9: Detailed information of models evaluated on AbsenceBench\n21C Comparison to the NIAH test\nWe extend the analysis of Section 3.2 on the comparison between the task design ofAbsenceBench\nand the NIAH test. In table 10, we show 3 LLMs achieve near perfect results in the NIAH test under\nthe poetry domain, as well as substantially improved results within the GitHub PRs domain.\nModels Poetry Poetry (NIAH) GitHub PRs GitHub PRs (NIAH)\nClaude-3.7-Sonnet 73.5 99.52 35.7 97.1\nGPT-4.1-mini 30.2 99.5 31.3 92.1\nLlama-4-Maverick 32.8 99.47 29.0 86.2\nTable 10: Micro F1-score (%) of three LLMs evaluated on the AbsenceBench and the NIAH test\nsetting under the Poetry and GitHub PRs domains.\nD Additional Results\n0 5000 10000 15000 20000 25000\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nMicro F1\nPoetry\nR2=0.50\n0 2000 4000 6000 8000\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nNumerical Sequences\nR2=0.10\n0 5000 10000 15000 20000\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nGitHub PRs\nR2=0.05\n 0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35\n0.40\nOmission Rate\nT otal Context Length (in tokens)\nFigure 6: Micro-F1 score of Claude-3.7-Sonnet (y-axis) as a function of the total context length\n(x-axis) as well as the percentage of omission (color)\n0 5000 10000 15000 20000 25000\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nMicro F1\nPoetry\nR2=0.60\n0 2000 4000 6000 8000\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nNumerical Sequences\nR2=0.15\n0 5000 10000 15000 20000\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nGitHub PRs\nR2=0.11\n 0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35\n0.40\nOmission Rate\nT otal Context Length (in tokens)\nFigure 7: Micro-F1 score of Llama-4-Maverick (y-axis) as a function of the total context length\n(x-axis) as well as the percentage of omission (color)\n22Gemini-2.5-flash\nClaude-3.7-Sonnet\no3-mini\nGrok-3-mini-Beta\nQwQ-32B\nQwen3-235BDeepSeek R1\n0\n100\n200\n300\nThinking T oken Ratio\nPoetry\nGemini-2.5-flash\nClaude-3.7-Sonnet\no3-mini\nGrok-3-mini-Beta\nQwQ-32B\nQwen3-235BDeepSeek R1\n0\n20\n40\n60\nNumerical Sequences\nGemini-2.5-flash\nClaude-3.7-Sonnet\no3-mini\nGrok-3-mini-Beta\nQwQ-32B\nQwen3-235BDeepSeek R1\n0\n10\n20\n30\nGitHub PRs\nLanguage models\nFigure 8: The thinking token ratio (number of generated thinking tokens divided by number of tokens\nin the original context) for four inference-time compute models under each domain.\n23"}   aLK>X팾HV8ԽR=kKp>3\Xa4C>;bp<En>ʯIP ?8_Y?
ʾ S8e?#1(Ud>D$;e)࠿{=&?wS?h>L?C;>鿕&	>cS
wzy@c?ϐm>=4̾6׿GվkV>>i]
>:=k2>	Uw>=ƒ(<ľ
 t}r=L>dǶ׽ .
?n ͿN	D&B+4>->?J𒿁l;MHS{?Lٗþ<3W,wj ?n%%{ƿ+>j`Ġ?rl5E>O0?6>^R?B;ﾺD.CKyo?q~3ᠷ;}J>Q(?m?Ns b?^̿$lM2,?eP@@?6J,=+03`C3̘"@>a?1?	({>σ'rOοwJAuvM_7PsJdw|>e9\Y>ʙ_|>:>6࿚(&Rz^=@4?Ծ5<&8!=d	K9VqڿɦЃ!;CZiQf5i
K'@c?>4??jn弙?|7ǿJ>ÿA)?۠w7վ\Q<Bݿ0, B=hl>R@Wބ>>mT?ٕ>	?#> ԿP2W!rK_.Fʿ>6T2?Jm}&?+w>4>ٿ\1 ]>~dҢ[mB?|Kz<c=aN T˿7#ֿ>"御=;&?)5?S>se3gZO?KZyV=^"YǺ¿2L=|bLr݉;lV>;>?yo
P?ǍHB6է<>3?pȾ$ =I]?M:=*s`>V?Ǿ֣>e{&?{rsɯ.b>܏>YWGh?Լxj;?jL_>ֿ%>cRO<fFky*V辉4ͿՏ౿;ؾ*	I)?F?Rs<8!ᵿV,>)-s6>F?F@'Bg38ݿ^ԎI>A-9E3r[<gY=:z?W>";(#2E<濡qJփ<_z6?t῜>"7yg>(]we{>en5Li!=f+`jҾQA<_&G?.>\yڽ>7q=VM6<s@ɝ>_mS a`h`A{>)t
[8N>|57mY7@#!Bh<%M!> ߿C۰CtO]pgѾ?dɿ?6+ƿ5̿Du/Ӿ}vNxꑿ6οԾ<Ǿzn4XWv_S?vr~?e,ӿ08p[Y~$xsj,=i`e,i?Ož;:?*ֿfӷb(C`"<J[>a=ǀzp,'>/o?ߋKpTv1駾.ٰ	ѽJ-'|	g$ǾCyИU=:71rKSlݚmvRA?{8]&ڿ08B=>z=B4v6453/	\&[>9q.>&Ygm<<P)P>}b?HF4龔@9iѾ|z>K{Y=d}@2a KS7݊?̿Oտ`<#u:5ʼ۶=)	p><7p+ྒRN=NͿpA>'>	bѸ>>24F"AqSܜ&>/.Bc=Z(lC!8F"1?_N˿)c>R{KHcƾ4==kXX=Ǿ:o`ƿϊ!տws={Ⱦ徇CӿL!d7]@$A8RaCG-;;S
ƿ%8Jj<c=C?¿X"ʹ	1b>ᆿTi>Sڊ0@оOedJiVھKf}N<˾.z̿NG>>=F򾬹	z$>&p/?|Ӟ2#3ƋE>e]9.v׿_Ȗ,Y<7x ?0{Fؙ$[v>6?AsKdo7?ѾI
QX%?Zh^-ӿYhi?0Ŷdp?@"b-YBג>x=MM¿%2
Dy;'k?ڡ8283Fo.?a wrjgҿ.?N16o:7ƾھ??[5<~O_>b!=>g>鴾v$>/<߾i?chQ=fýS EzbIGQlF>.{þ$Ӥl<?[rc̿+>Qݷξ5n㾱̾?X."fİr@Ds?4Ö]/=	Rν?H#NW>OP;ݿ-T,*/>p"=M5>6>(ҥt͔#3%>^u !>q?8H?'7վ],`*VQE岾r<φ>3<XgwJX*?K኿W-o.̾	<%ꉿ,9t>U1in`  q=zJ?evzz>Pί?t>YH?ʹ3$þ^CԿ6*=W<ӾkͿ`}BZ<߾ ;&'T<5ʿ`9"E;??,T"tS[:1E]#`={)>EN6DĭȾUp_t ޾޹Th5v$[*\*ܿ^hA^B<E㾾?sHg?濄Ia龕̿>r?18ȿ>?=sDJ8ƿ>aмPG?#[>^OjlIw?_?rݿhھhCzֿ4=lge9p=y'x1;ǿ8NZ<$R\>fQdG2>H#=,>rؗ>٭~=wg$\۴ύ8~qal>Ƒ
b?7={%7T+?$ٿ;aɒο)P2?H>t-3ӛ?|H]pP>)辳Uԧ濗R=t+k0=-V"Z=w^?8ؿbJ-IÚ Y>7`>WJJ$3|~n}a9 k?YW>$?4n y>EaӿAܿ܊ ?C H>Jh?ݿ><|%<76?Z"P(?+NRǋMnb/ǓJ-=!jINǾ3}dbdC=p ?%^?wG*>]cg=Z;Cʽ>#h#<=^G)H|@
ӿ-B>EB+%%Z7T>HcD ῇɾ'B4UaBv##T>62.LYeOG=o'LW'?>:z>A">;bMU>6f4U>^e׏<V`>'?K&'2UĿ]W<".lwL=6<?_^kY@)>^?&%ɽ)N$g7d89So3$R =>;M:Ō3=oԪ1?%>HW>>辄u<yf=<2"_Eܾ[.>Mv_=?=UY.1տ {xHiV?سw슐-v徠MT~>M>Ӻ~c[ᾃW ?'ݦ=M>jvl4> A?>= f?9޾iUhzp+UR!*-]>VJRj_\i>s?>q>X<,%ȽXs+
U>տT&O|URt>*8ׄ>'!2'	:Ym@1?q򠔽*&о+Ͽ͋?>ɚW߿e0=t o&&߼ܾݵquM8>߱Ͼ?ZȆUo>ʧ?A
=:<a1>>ԚUrfu? -Zr>DڱoO!\Ŧeuj[e\w>vm.% >>@g?$LG>>#?EȾIz	?1ʾ1v9H+S"&_I,*!>ԎY6mYgx	Wʫxۿ>{״&bC&S,wD>q7-ܓ@|>,M.cz˿v)e@'KB?eZ<X	> ߿@Tؿ8pھ@?zٍ>6 +	FTM A"O?P.2r
$f?ǿNV-:"uwY,#t>^)fnfC" t&:޿V>K^⿒{5=ƒj^eNA?3>Y7딾|'OJY S:o>훾Q>;0]-)>ʟȿǥ!+FㅿkHS0?3>?0G۾K>+SGK+0L-?q'2lIC[@c>}r>=MKp>y?8ziľ	e|<B?Ij>׿ [ɿuyW>~w;>H윿[bP>_\ >Kע>,)C>9l?ٿ Vv6?.SW~:>x[S/?༾Ɔ羾?o꾈Idl )>53>ѲҿJ>Kx=ogu{PV{ܾ>8 ?[>"]O C˿>"==owb$#Ce>\(䏾Po1&<!>%P>F7ÿ'Ǡ J#2P?5><nyQn9?<M?nscյk>&6Тq>̿&jeV	Vlx>Ц>"?glwHk?Ά({@cuv=왇Y{pMLkP/x>Ec}%?Bs6w|>=h>֭tp{*?^tsѧ^l"qm5c?N ѿ`+>qJ޿P<>_@ý3E Z>>>xѿ_rm8>0kV@O'SjHd>JJ෿MfHſP;>6(3"@վ"e-ۙܻC>q>k,?mj>Sb?F?R:H(>`X!ݿzN9Bf)߽޿)
p>.sЅο<_=5)?=\?<^">Pÿο<}i5-hX8|ŷ>*Mտ[ʾu>S>tj[pux	x{8~<:Gy<zۖg>8qD}QT/N3LпmɚZ)=vBap>wyͧZ8`Q>"w>t@=I>c̓Qj._tyt,'oM|4=&61 ?,?]?i{9Gk>G俒ſ*=:/k-R@v(v?">0ῲ>T9Xe?1IZb鎿==GӾ["ctӾ<I6ouHԿ>{3y*|> nѾ_[֤$nϿꄿR۾Je꿕 sY־GU]3Rll䝼&W}^Ӿ֑䩾,׿7?\֙>ǿa>d>>*ꕿ#O=-Y=߾\b(*?㌿R6>Z=-w>plqUW~> 3~>7콰6?J5=F
̿{e*Ⱦ^g?s_
w՝q>7Uz|C{ɚS)Bw.EnXʑdTFL?vm=ZʿRJϾr濾qnUE?B2Fa:>IPgXο˟()C>9%I\"<ԿeN;|.a0־q?_YQ>_q# 3=p/>Å XiS>IJQbl"7>3ٔſ1lɨ2?=ocσ%<oL?R G`=v3\jb> U>e!)&@PXLVK$>lm?͙
LؾTB<8Cs(ȿT,,G,=ZˢQ>=n<_<׃Dy<#<Hs>llE-]?Xhnat>LD:=mѢ[V>C8%M.Խlz7w%kd^y1Ҿ	¾ڐ=ٻ̿/m	ľp\\VV@Ahk7J=>ʿ ֿa= -߿Z}chEYJH[4G a'-d;zz=1:X3B??ގJY')%M*>' Ͼ@v8ᑿ=7=3%?q⿿)4W_`լ+ 8Rܘ쿸>('Ͽ3<Υyk k`,2ÿNq>fg?׿	Fd#5`hҿyq)q쑿nmQRvԿw5%s>	ͿV㢿m=\/|_*IiWL&CO
~nĖ,JǺ	'½!I=D⡿>9?aCpfd_>յ2ٓɾ(B=NWȽ+N!Ͽ]և>v
?X>)>ɿ<Y%g3d:J=-$/|Ak%R)>r/Ѿ짿" fqϿJo&DkH<KRG{ȿ[!yj?J:Af?4ʨW_I]Cݶ;vVʴqȥK>wп~;޿tVytB%笽;{Z=`l,pf?P 6=.࿬d>>O˿4XX44Z6E')>V$>S2Ekjlq]E?t%ÿþ?حw6~Y#n@یо=E')nݿuVL]_ŊȝѣqO:RCkh}sDʾ3><Z;X'嘉}>Z?Ѓ%ǾC
]?]կ#S;{Z3D'Qq>]:RkdrrY\T̿񭨿3{>ĿǾd0]Ҿ} ۂH˿=1>`mF> t	?Ylu߶?A=#&\K)-Tl>@/`H>B9,?b"H?B'"XJK>*0E>8-6ٿͿ3пK&&!Cĭ>mۿD缾Hoq@>0^uʶ>6Z>O>qN G?s4> M>fY^b|3?)N(zIc&4j>=e5T,=x-# =k;>LT>P8g]AXؿ0A(5@]Ҿ0StȿF6yHgD=qD{G հ_ޑt{ѿ!v> GȽv׿qXT+n[¿^$Wb	<l̖¾xI`¾o{|#F MGA?G%u?#ot?Ϳ`~>?ۆR&v>Adӿc]=k𿎵6b{Y1*=]=B{'q=+=Y/t
+nvygGK"-?o$f!>R0ҿї<Sb*e$fy^Xc ~ڿ/"ɇu:NB¼Oɿ%et?YU=ꋾ"(\N-.<	C'!bW%?ЉKNG}Hdwξq50X˿=޾hj> {gB@Zmȿ"#rjC>,XH ο=Z<[AEIL_=pN77_Kx\< Qe<[n=>>3&+E-3H俺G5y?о U\ÿ }>ƿ跰dA㿄"=
׾CFJ[ɝ%Nzgn\p>
ۿ'ʢ>Ѿŀɵvqɿ8kqմ%>D޾QrFpm \XB4(ʾc忲rԤ.`yX8]ʏBkc=|o>$>	?┾qca<wf)d=\<ټΑif羲>ZfIY7";>Ŀ	ܾ~'꘤>Ar{#j%rv=}Mxg皿{GkK, 8l*7[;(ޣې=>- <YaWQA#><)>it/ڪY'''^+M$X!a|A/ǔd-\}C>]h=C%1JRsdnT/=$zJ ?Cļ⊤?s=$#?฾DN=c}5>գb-Ke>!O\2ZϽ#<?:ҿe=hK3Jb4΄>dG>/Կfr㾫6=1@ 6v=U##׾'D\¿d)S?uҿ{85RԿ^>qDt>r5̿?¿)iBiEWȾy^ǿm$dԾ"|rJYfD>>Yة >/E>@KbJd"L彾0>pc꿙h#ƾ)S.E>?Y.]Կ=>A_
/z޿ߥƯw㜿E ^:>	䯈*qHXT&Ɨ=5h̾"KwAw$ھk6P>鄿T(d|꾡57S ࿬t\hp2d#/wvN>0@	ҿr
i2!8`<=e>v@Xh6OUfHQDQDjܾ;r>Zz׋(1A
jÿI8=˒3>5?vF=&ب:3%$ygx>;0VpvTK	hվ)ÿl>志ݻ={G0vz¹Y"Ӎ`Q6ڿ4le#KZy JcѿmUw4Ο=ؠƿѿʿ1kG.pg52vY5?ZOLS)o?忔}$O=!iſyԾܮƨqְi\|WfuUUK5?¿񂫿﬿ΰhYXq
>A6_|l7KNHk~0D>uT)?뮽rVnf=Xd<F~>m]M[icؾ.{?*I/9-s1좜ÿ>1>6]"LE^W~[mſ$nuܞAe齢>羿'ɿۇH7̿eKqerݾȾ[
[>z)<nq[׾S`D=~KHuS:X@c=5h^n_>9>>wr1!
ZyCgǾžg767?SnY=.,D潿\hWosbs%ľee)>_Bcٽ%HFU p-j!f5L#OhnPvƿ	'>p Xh=O HпmԒ\L\c} 𽿃>L#?ah}v8fi3\ֿVPɽZK캟mؿ2W>İe-ƾ7咿?-/۾ѩ4\r
+U0 fJ	@*Ysqܾ g禬Ve.°Kԭ	ӾS¾s*k>iMa<=TS|r=_%n&
忎_E{с?fXT2hi4ɐC }
m'֭6q,f>|vZY.	[,~e̢J|>/L-똿c>{>ma㿇<>Cx3z־#LNPǿlS_FпG
a=]ٜ;h6eֿ]>(>񥭿ձ濋>v~¼žQ2)0v=9~>o{=r?ؽFMɫҾGG[>_$oM=XC1?J<$+ȿt{fJdR[>r۷pU}>Lgk(ycQhv܊ݾxRq7rP8,F!>a>^YfRN):lg!ˉ	>MF""迓&Bޱ?~BRKR߿VRj<AmFUo==?pȾ,?%*"
\|a=΄Na.~qwn!Cjp<8މ!*0@	Ͽ˾Fkľ̿"￲c24>?,7K|&W= ǿ}cAiTqZ,>+EV7|R>'w)yAֿ9}&_ -?
,_Dિ9)$˿*'㾈yC9˶ߊ[PſFGR$?ʾ*X{ʿw2>tܖB)ο>;<Xʿ鐾4s߽>EC+I۪\o)K7ϾWϿJ7>kP,=xrz5<=ʖ뾬hq?<Uˤ}W>Uxhx<28ƾ➾uNAFRտXlP?j%OOƽvۏ?[5U/*7q{cX|b.hlݪ9m>a<̤JFvqF?l?X=N]Z#WEE>`>gݶ9?D_<?;pp.>ݿ2Z&辯x?>gL	&^{ѿd>$h>S=:>4>b>@}~>qoڿ!Sܿc>_L>4"ԕ묿䏾j>7p9=B4Op?!W%T6>h@m{y澇f % FLO's@?	E&g">HJ9ýY[/a?֞%!'!߁
$=+Z+M?=|߿0dɾJE#`
ɾr6F#<םmc_ q>hT.*R3c=3>:8oC&.>H?5=}M=ǆj?;sq>1\8n?Ͽl>þ:<AR?[Կ༁ʸIƖT0E VfWʦ4Bʿgm?7=t6If~㑾m8>	X=;tk2`?>[e.*ڪ2G=6iѼ>5+Qkؾ*?P=vOÝn}w a7UcQƿ=≿ͿgI>ƥ5Vl.|h2?ܿߗ(Ky,Nܑ~H6%(\4?wO7T,JJcwLrn :(wޫ#}>tX9룽 >KkԾ<>RVξBCO L<txa+ʽ(%VuпJ*)r#:/?ڝmǓ%Ǿ&C>|jϿв5>f轟Pnk0?A>ٿ~T4](W=p>>s @R,9?*?>OvZ<9=u> C[2l3-QW>FnYP+|몾S>v> ?؆uPFD鶿c?3jh£ɾɿSd1;o?NlWFr5]tP?cҿ:%ﲿfH<".T#U>?ASĕM澏3*ξ=Ľֻ>E)=!μV4{M?ude{~ӾX {$$=ƾ8s=o=3\`
?+?0?HLE O'>Pq2]-ӿ<)Ͽ@
Ӿ>5=Tt߿5?\lQ1x`t?w?f12#7Zpj*܃=D?Z>#<=?@W%W:Wuɾ(u>]ȿQhD^ɜ^.j3YKy@y R9)Bj=X俉UC`R?,ʾu],,?JJعp{?vm8̖BœbEs>*@qS?[.>V;~5:bk?1)?4>@ >jnQѽ=SR?芿ɪ<lѿƊgXΟ~>]?Gn]`KK>z&Nf۔@lCLy7o{+U=x%<=ۆ&sL??4oi߿ZB;0@9vؿ}<]>dI>Ō33"}=E>e9>I>_߾N	=9п؃>{"chroma:document":"Large Language Models Can Be Easily Distracted by Irrelevant Context\nFreda Shi 1 2 * Xinyun Chen 1 * Kanishka Misra 1 3 Nathan Scales 1 David Dohan 1 Ed Chi 1\nNathanael Sch¨arli 1 Denny Zhou 1\nAbstract\nLarge language models have achieved impressive\nperformance on various natural language process-\ning tasks. However, so far they have been eval-\nuated primarily on benchmarks where all infor-\nmation in the input context is relevant for solving\nthe task. In this work, we investigate the dis-\ntractibility of large language models, i.e., how\nthe model problem-solving accuracy can be influ-\nenced by irrelevant context. In particular, we intro-\nduce Grade-School Math with Irrelevant Context\n(GSM-IC ), an arithmetic reasoning dataset with\nirrelevant information in the problem description.\nWe use this benchmark to measure the distractibil-\nity of cutting-edge prompting techniques for large\nlanguage models, and find that the model perfor-\nmance is dramatically decreased when irrelevant\ninformation is included. We also identify several\napproaches for mitigating this deficiency, such\nas decoding with self-consistency and adding to\nthe prompt an instruction that tells the language\nmodel to ignore the irrelevant information.1\n1. Introduction\nPrompting large language models performs decently well in\na variety of domains (Brown et al., 2020; Chowdhery et al.,\n2022, inter alia). However, for most of theses evaluation\nbenchmarks, all the information provided in the problem de-\nscription is relevant to the problem solution, as the problems\nin exams. This is different from real-world situations, where\nproblems usually come with several pieces of contextually\nWork done while FS and KM are student researchers at\nGoogle DeepMind. *Equal contribution 1Google Deep-\nMind 2Toyota Technological Institute at Chicago 3Purdue Uni-\nversity. Correspondence to: Freda Shi <freda@ttic.edu>,\nXinyun Chen <xinyunchen@google.com>, Denny Zhou <den-\nnyzhou@google.com>.\nProceedings of the 40 th International Conference on Machine\nLearning, Honolulu, Hawaii, USA. PMLR 202, 2023. Copyright\n2023 by the author(s).\n1Dataset is available at https://github.com/\ngoogle-research-datasets/GSM-IC .\nOriginal Problem\nJessica is six years older than Claire. In two years,\nClaire will be 20 years old. How old is Jessica now?\nModified Problem\nJessica is six years older than Claire. In two years, Claire\nwill be 20 years old. Twenty years ago, the age of\nClaire’s father is 3 times of Jessica’s age. How old is\nJessica now?\nStandard Answer 24\nTable 1.An example problem from GSM-IC . An irrelevant sen-\ntence (italic and underlined) that does not affect the standard an-\nswer is added immediately before the question.\nrelated information, which may or may not be relevant to\nthe problems that we want to solve. We have to identify\nwhat information is actually necessary during solving those\nproblems. Studies in psychology have shown that irrelevant\ninformation may significantly decrease some children and\neven adults problem-solving accuracy (Hoyer et al., 1979;\nPasolunghi et al., 1999; Marzocchi et al., 2002, inter alia).\nIn this work, we study the distractibility of large language\nmodels for various prompting techniques; i.e., how is large\nlanguage model prompting affected by irrelevant context,\nand what strategies can be used to improve performance? To\nmeasure distractibility, we construct the GSM-IC dataset, a\ngrade-school math problem dataset derived from GSM8K\n(Cobbe et al., 2021) and introduce two different metrics. In\ncontrast to prior work that derives benchmark variations by\nsubstituting sentences of the base problems with variations\n(Patel et al., 2021; Kumar et al., 2021, inter alia), we keep\nthe base problem description and add to it one irrelevant sen-\ntence, while making sure that it does not affect the solution\nof the problem (Table 1).\nWe use Codex ( code-davinci-002) and GPT-3.5 (\ntext-davinci-003) in the GPT3 model family to eval-\nuate state-of-the-art prompting techniques on GSM-IC ,2\nincluding chain-of-thought prompting ( COT; Wei et al.,\n2022), zero-shot chain-of-thought prompting (0-COT; Ko-\njima et al., 2022), least-to-most-prompting ( LTM; Zhou\net al., 2022), and prompting with programs ( PROGRAM ;\n2http://openai.com/api/\n1\narXiv:2302.00093v3  [cs.CL]  6 Jun 2023Large Language Models Can Be Easily Distracted by Irrelevant Context\nChowdhery et al., 2022).We find that their performance\non GSM-IC greatly decreases compared to the original\nGSM8K (without irrelevant context). We then investigate\nseveral approaches to mitigate this weakness, including\nself-consistency (Wang et al., 2022c) and adding irrelevant\ninformation to the exemplars in the prompt. In addition\nto demonstrating how to handle irrelevant information via\nexemplars, we also investigate the usage of task-specific\ninstructions (Wei et al., 2021; Sanh et al., 2021; Ouyang\net al., 2022; Suzgun et al., 2022; Chung et al., 2022), where\nwe prepend an instruction sentence “feel free to ignore ir-\nrelevant information in the problem description” to the\nexemplars. We summarize our key findings below:\n1. All investigated prompting techniques are sensitive to\nirrelevant information in the problem description. In par-\nticular, among the original problems that can be solved\nby baseline prompts with greedy decoding, no more than\n18% of them can be consistently solved for all types of\nirrelevant information, showing that the large language\nmodel is easily distracted and produces inconsistent pre-\ndictions when adding a small amount of irrelevant infor-\nmation to the problem description.\n2. Self-consistency improves the performance of all prompt-\ning techniques on GSM-IC . In particular, the recall rate\nof the correct answer for GSM-IC is as high as 99.7%\nwith 20 samples per problem, i.e., at least one of the 20\nsolutions result in the correct final answer, which means\nthat using multiple samples allows the model to almost\nalways retrieve the correct answer.\n3. Adding irrelevant information to the exemplars shown\nin the prompt consistently boosts the performance, and\nthe same holds for adding an instruction to ignore ir-\nrelevant context. This suggests that language models\nare—to some extent—able to learn to ignore irrelevant\ninformation by following examples or instructions.\n4. We identify different factors of the irrelevant information\nthat affect the model’s sensitivity to irrelevant context.\nOur breakdown analysis shows that varying the numbers\nin the irrelevant information does not notably change the\nmodel performance, while the degree of lexical overlap\nwith the original problem description matters.\nFiltering out irrelevant information is essential for han-\ndling real-world tasks. Our evaluation indicates that de-\nspite the strong performance on challenging reasoning prob-\nlems, state-of-the-art language models still have fundamen-\ntal weaknesses in context understanding and identifying the\nrelevant information from the input. Our findings suggest\nthat in order to gain a more holistic understanding of the rea-\nsoning capability of language models, future work should\nalso consider the model sensitivity to irrelevant context, in\naddition to solving more challenging problems.\n2. Related Work\nFew-shot prompting. Few-shot prompting (Brown et al.,\n2020; Chowdhery et al., 2022, inter alia) has been signifi-\ncantly boosted with various techniques, including generating\nintermediate steps (Ling et al., 2017; Cobbe et al., 2021;\nNye et al., 2021; Wei et al., 2022; Suzgun et al., 2022; Shi\net al., 2022b, inter alia), problem decomposition (Zhou\net al., 2022; Drozdov et al., 2022; Dohan et al., 2022; Khot\net al., 2022; Press et al., 2022, inter alia), generating pro-\ngrams (Austin et al., 2021; Chowdhery et al., 2022; Gao\net al., 2022; Chen et al., 2022, inter alia), marginalizing\nintermediate steps that share the same result (Wang et al.,\n2022c; Shi et al., 2022a), and ensemble (Wang et al., 2022b;\nDrozdov et al., 2022). In addition, Kojima et al. (2022)\ndemonstrate that appropriate hint in prompts also leads to\ndecent performance, even without any exemplar. In this\nwork, we examine these cutting-edge prompting techniques\n(Wei et al., 2022; Zhou et al., 2022; Kojima et al., 2022;\nWang et al., 2022c) on our benchmark, and demonstrate that\nthey are sensitive to irrelevant input context.\nNatural language benchmarks with input perturbations.\nThere has been a long line of work on adding input perturba-\ntions for natural language tasks, including model-agnostic\ninput transformations (Liang et al., 2022; Ravichander et al.,\n2022, inter alia) and adversarial example generation against\nindividual models (Jia & Liang, 2017; Shi et al., 2018; Mor-\nris et al., 2020; Wang et al., 2021). In particular, prior work\nhas constructed arithmetic reasoning benchmarks through\nparaphrasing or rewriting sentences in the base problems\nfrom clean datasets (Patel et al., 2021; Kumar et al., 2021).\nMeanwhile, Liang et al. (2022) evaluate various large lan-\nguage models under several metrics, including accuracy,\nrobustness, fairness, etc. Specifically, the input transfor-\nmations in their robustness evaluation include semantics-\npreserving and semantics-altering perturbations, such as in-\njecting typos and modifying sentences to change the ground-\ntruth classification labels. In contrast the above work where\nthe meaning of problem descriptions may be changed with\nperturbations, we keep all sentences in the original prob-\nlem description, and introduce an irrelevant sentence that is\nensured not to affect the standard answer.\nNatural language benchmarks with irrelevant input con-\ntext. Jia & Liang (2017) have shown that neural ques-\ntion answering systems are largely affected by adversarial\ndistracting sentences, whereas follow up work (Khashabi\net al., 2017; Ni et al., 2019) proposes learning strategies that\nmitigate the problem. Similar issues have been found for\ngeneral-purpose pretrained language models, on the tasks\nof factual reasoning (Kassner & Sch ¨utze, 2020; Pandia &\nEttinger, 2021; Misra et al., 2023; Li et al., 2022), code\ngeneration (Jones & Steinhardt, 2022), and syntactic gener-\nalization (Chaves & Richter, 2021). In particular, Li et al.\n2Large Language Models Can Be Easily Distracted by Irrelevant Context\n(2022) evaluated T5 (Raffel et al., 2020) and PaLM (Chowd-\nhery et al., 2022) with few-shot prompts, and proposed\nknowledge-aware finetuning that finetunes the model on\nproblems with counterfactual and irrelevant context, which\nstrengthens the model robustness to noisy context. In our\nevaluation, we show that without training or finetuning,\nadding irrelevant context into demonstrations in the prompt\nalso mitigates the distractibility of the underlying language\nmodel and significantly improves the model performance\non our GSM-IC benchmark.\nThere exist some logical reasoning benchmarks that con-\ntain irrelevant content in task descriptions (Weston et al.,\n2015; Sinha et al., 2019; Clark et al., 2021; Han et al., 2022;\nTafjord et al., 2020, inter alia). However, previous work\nlargely focuses on designing models that require extra train-\ning, and prompting alone still hardly achieves the same level\nof performance as finetuned models for these tasks (Han\net al., 2022; Creswell et al., 2022). In our work, we focus\non arithmetic reasoning, where prompting techniques have\nachieved the state-of-the-art results, e.g., on GSM8K, while\nwe show that adding a single irrelevant sentence into the\nproblem description significantly degrades the performance.\nPrompting with noisy ground truth. A line of work stud-\nies the model performance with incorrect prompting ex-\nemplars, i.e., the example problems are paired with wrong\nanswers (Min et al., 2022; Kim et al., 2022). In addition,\nprior work has investigated the model sensitivity to other\nparts of the prompt, such as instruction tuning with mislead-\ning and irrelevant instructions (Webson & Pavlick, 2021)\nand wrong reasoning steps in the examples (Madaan & Yaz-\ndanbakhsh, 2022; Wang et al., 2022a). In particular, Madaan\n& Yazdanbakhsh (2022) conclude that the correctness of\nnumbers and equations in chain-of-thought prompts does\nnot play a key role in model performance, but using wrong\nentities and removing either equations or text explanation\nin the reasoning steps drastically hamper the performance.\nDifferent from this line of work, we always include correct\nanswers to example problems in the prompt, and ensure\nthat the irrelevant context added to the problem description\ndoes not change the ground truth answer. We show that the\nmodel performance significantly drops when presented with\nirrelevant context in problem descriptions, and different dis-\ntributions of numbers and entities in the irrelevant context\nalso lead to different levels of performance degradation.\n3. The GSM-IC Dataset\nIn this section, we introduce the creation process of the\nGSM-IC dataset (§3.1) and the evaluation metrics (§3.2).\n3.1. Dataset Creation\nWe randomly choose 1,000 problems from the GSM8K train-\ning set as a development set. To construct our base dataset,\nCOT L TM P ROGRAM 0-COT\n95.0 94.0 83.0 44.0\n+ SC 96.0 99.0 91.0 76.0\nTable 2.Accuracy ( ×100) on the base 100-example dataset\nusing code-davinci-002. See Table 3 for results with\ntext-davinci-003.\nFigure 1.Illustration of the considered factors when creating the\nGSM-IC dataset. Best viewed in color.\nwe then choose 100 problems from this development set\nthat can be correctly solved by at least one of the prompting\ntechniques mentioned in this paper;3 that is, our base dataset\nis an “easy” subset of GSM8K (Table 2). Each base prob-\nlem requires two to seven reasoning steps to solve.4 Among\nthe 100 base problems, 60 of them can be solved with two\nreasoning steps. The full dataset statistics can be found in\nAppendix A.\nWe then generate the examples of our new dataset by adding\nto each base problem one sentence containing irrelevant\ninformation. We use a template-based method (Figure 1) to\ngenerate these sentences, which can be characterized by the\nfollowing three factors:\n• Topic of the inserted sentence. We write templates for\nboth in-topic and off-topic sentences. In-topic sentences\nare closely related to the topic of the original problem,\nwhereas off-topic sentences are about a different topic.\n• Role name overlap . Most sentence templates contain\nsome role name blanks, which can be filled with names\nthat may or may not overlap with the role names that occur\nin the problem. For blank fillers that have overlap with\noriginal role names, we: (1) randomly pick a role name\nA from the original problem description and (2) create\nthe blank fillers with template such as A’s father and\nA’s sister.\n3We do not generate new examples or perform analysis on the\ntest set to avoid potential tuning-on-test-set issues.\n4The number of reasoning steps of a problem is given by the\nnumber of sentences in its standard answer (Cobbe et al., 2021).\n3Large Language Models Can Be Easily Distracted by Irrelevant Context\n• Range of numbers. Since we focus on arithmetic reason-\ning, most sentence templates also contain a number blank.\nWe can choose to fill in the number blank with a number\nof similar or different magnitude to those in the original\nproblem description. Concretely, for a number a, if there\nexists a number b in the original problem description or\nsolution such that 1\n10 ≤ a\nb ≤ 10, we consider a as an\nin-range number, and otherwise an out-of-range number.\nSince the standard answer to GSM8K problems are all\npositive integers, we only consider positive integers as the\nnumber blank fillers.\nWe manually verify that (1) all the generated sentences are\nacceptable in English and that (2) adding them does not\naffect the standard solution of the base problem. Because\nthe above factors are orthogonal, we generate for each base\nexample a set of derived examples with different factor\ncombinations. The full GSM-IC benchmark consists of\n58,052 examples. More details about the dataset creation\nprocess can be found in Appendix A.\n3.2. Evaluation Metrics\nFor a problem p, we denote its standard solution by s(p),\nand the solution of method M by M(p). To evaluate the\ndistractibility of M, we consider the following two metrics:\n• Micro accuracy Accmicro(M; P) is the average accuracy\nof method M over all the test problems P.\nAccmicro(M; P) =\nP\np∈P 1 [M(p) =s(p)]\n|P|\nThis means that the micro accuracy weighs all the individ-\nual test problems equally.\n• Macro accuracy Accmacro(M; B) is the average accuracy\nof method M over classes of test problems, where each\nclass P(b) consists of the set of test examples derived\nfrom the base example b ∈ B. We define M’s prediction\nfor a class P(b) to be correct if and only ifM’s prediction\nfor all problems in this class are correct.\nAccmacro(M; B) =\nP\nb∈B 1\nhV\np∈P(b) [M(p) =s(p)]\ni\n|B|\nThis means that the macro accuracy is the fraction of base\nproblems that can be consistently solved no matter what\nirrelevant sentence is being added.\n• Normalized accuracy measures how a method is affected\nby the distractors, considering its accuracy on base prob-\nlems. For a micro or macro accuracy aM achieved by\nmethod M, we calculate its corresponding normalized\naccuracy by\nnorm(aM; M) =aM\nnM\n,\nwhere nM denotes the base problem accuracy of method\nM (Table 2).\n4. Investigated Solutions\nIn the following section, we review the investigated prompt-\ning techniques (§4.1), present the formats of our prompts\n(§4.2), and introduce instructed prompting (§4.3).\n4.1. Base Techniques\nChain-of-thought prompting (COT; Wei et al., 2022) is\na prompting technique that guides the language models to\nsolve a problem in a step-by-step manner. By presenting\nexemplars that solve the corresponding problems with inter-\nmediate reasoning steps in the prompts, COT significantly\nimproves the reasoning performance over direct answer pre-\ndiction without such intermediate reasoning steps.\nZero-shot chain-of-thought prompting (0-COT; Kojima\net al., 2022) is a variation of COT where the prompt does\nnot contain any exemplar. Instead, the model is prompted\ndirectly with the problem of interest followed by the instruc-\ntion “Let’s think step by step:”.\nLeast-to-most prompting ( LTM; Zhou et al., 2022)\nteaches language models to (1) break down a problem into\nsubproblems, and (2) solve those subproblems sequentially\nusing COT. The final answer is that to the last subproblem.\nProgram prompts (PROGRAM ; Chowdhery et al., 2022)\nrepresent the arithmetic reasoning process as a program.\nFollowing prior work on solving GSM8K problems with\ncode (Chowdhery et al., 2022; Gao et al., 2022; Chen et al.,\n2022), we include a Python program as the problem solution\nin the prompt, and execute the generated Python code using\nan external Python interpreter to obtain the final answer.\nSelf-consistency (SC; Wang et al., 2022c; Shi et al., 2022a)\nmay further boost the reasoning performance by marginal-\nizing over intermediate reasoning steps that share the same\nfinal result. In practice, SC can be implemented by (1) sam-\npling several solutions from the large language model and\n(2) taking the majority vote. Note that SC is orthogonal to\nabove techniques, and can be combined with any of them.\n4.2. Prompt Design\nWe present some example prompts used in our experiments\n(Figure 2). For few-shot prompting techniques (i.e., COT,\nLTM and PROGRAM ), the input prompt includes exemplar\nproblems and their solutions before the problem of interest.\nIn order to keep simplicity and avoid over-fitting in prompt\nengineering, we follow Zhou et al. (2022) on exemplar cre-\nation; that is, we only use one simple exemplar for our main\nexperiments. This exemplar is either based on the [Original\nProblem] or the [Problem with Irrelevant Context], which\n4Large Language Models Can Be Easily Distracted by Irrelevant Context\n[Original Problem] = \nQ: Elsa has 5 apples. Anna has 2 more apples than Elsa. \nHow many apples do they have together?\n[Problem with Irrelevant Context] = \nQ: Elsa has 5 apples. Anna has 2 more apples than Elsa. Liz \nhas 4 peaches. How many apples do they have together?\n[COT Solution] = \nA: Anna has 2 more apples than Elsa, so Anna has 2 + 5 = 7 \napples. Elsa and Anna have 5 + 7 = 12 apples together. The \nanswer is 12.\n[LTM Solution] = \nA: Let's break down this problem: 1. How many apples does \nAnna have? 2. How many apples do Elsa and Anna have \ntogether?\n1. Anna has 2 more apples than Elsa. So Anna has 2 + 5 = 7 \napples.\n2. Elsa and Anna have 5 + 7 = 12 apples together.\n[PROGRAM Solution] = \nA: Let’s solve the problem by a Python program:\nElsa_apples = 5\nAnna_apples = 2 + Elsa_apples\nElsa_Anna_apples = Elsa_apples + Anna_apples\nprint(Elsa_Anna_apples)\n[Problem of Interest] = \nQ: Jeanne wants to ride the Ferris wheel, the roller coaster, \nand the bumper cars. The Ferris wheel costs 5 tickets, the \nroller coaster costs 4 tickets and the bumper cars cost 4 \ntickets. Jeanne has 5 tickets. Jeanne’s neighbor rides 8 \nkilometers to the bus station every day. How many more \ntickets should Jeanne buy?\nFigure 2.Prompt formats for the investigated techniques on the right, which are constructed from building blocks on the left (best viewed\nin color). The [Problem with Irrelevant Context] is obtained by adding an irrelevant sentence ( italic and underlined) to the original\nproblem description and it can be used as an alternative to the [Original Problem] in the prompts on the right. In these prompts, identifiers\nhighlighted and wrapped by brackets (e.g., [Problem of Interest]) are replaced by the contents of the corresponding building blocks. The\nprompts for all settings can be found in Appendix C.\nallows us to investigate the effect of irrelevant information\nin the prompt exemplar. For 0-COT, we adhere to Kojima\net al. (2022) and directly present the problem of interest\nfollowed by “A: Let’s think step by step:”.\n4.3. Instructed Prompting\nIn addition to presenting irrelevant information in the exem-\nplars, we also investigate whether natural language instruc-\ntions help language models ignore irrelevant context and\nbecome less distracted. Extending the line of work (Suzgun\net al., 2022; Sanh et al., 2021; Ouyang et al., 2022) that\nincludes a general task description before exemplars, we\nadd the sentence “Solve grade school math problems. Feel\nfree to ignore irrelevant information given in the questions. ”\nbefore our exemplars in the prompt (Figure 2), which ex-\nplicitly instructs the language model to ignore irrelevant\ninformation in the problem description.\n5. Experiments\nBeing mindful of the experiment costs, we uniformly sam-\nple 4,000 examples from the GSM-IC dataset (denoted\nby GSM-IC -4K)5 for evaluation and analysis purposes\nthroughout this paper. Unless otherwise specified, we\nmainly use code-davinci-002 in our experiments, and\nwe also evaluate text-davinci-003 which is a model\ntrained with RLHF to better follow instructions (Ouyang\net al., 2022). For experiments without self-consistency de-\ncoding, we use greedy decoding (i.e., temperature τ = 0);\nfor self-consistency experiments that require multiple sam-\nples for a problem, we sample 20 responses with tempera-\nture τ = 0.7 following Wang et al. (2022c).\n5.1. Main Results on GSM-IC\nWe compare the performance of different prompting tech-\nniques on GSM-IC -4K (Table 3), in terms of both micro\n5Our sampled GSM-IC-4K covers all 100 base problems.\n5Large Language Models Can Be Easily Distracted by Irrelevant Context\nMethod Micro Accuracy Macro Accuracy\n2 Steps >2 Steps Overall Norm 2 Steps >2 Steps Overall Norm\nPrompting Exemplar w/o Irrelevant Context, code-davinci-002\nCOT 73.5 70.8 72.4 76.2 8.3 2.5 6.0 6.3\nCOT + INST. 79.0 76.0 77.8 81.8 20.0 7.0 15.0 15.8\n0-COT 29.0 29.1 29.0 65.9 1.7 0.0 1.0 2.3\n0-COT +INST. 31.6 28.8 30.5 69.3 1.7 0.0 1.0 2.3\nLTM 74.9 81.5 77.5 82.4 16.7 20.0 18.0 19.1\nLTM + INST. 80.1 81.3 80.6 85.7 18.3 35.0 25.0 26.6\nPROGRAM 59.1 47.4 54.4 65.5 6.7 2.5 5.0 6.0\nPROGRAM + INST. 60.6 50.9 56.7 68.3 6.7 5.0 6.0 7.2\nCOT + SC 87.6 90.1 88.1 91.8 29.0 28.3 30.0 31.3\n0-COT + SC 61.6 68.4 64.3 84.6 0.0 2.5 1.0 1.3\nLTM + SC 92.4 94.8 93.4 94.3 51.6 35.0 45.0 45.5\nPROGRAM + SC 73.5 76.1 74.6 82.0 16.7 7.5 13.0 14.3\nPrompting Exemplar w/o Irrelevant Context, text-davinci-003\nCOT 69.3 66.9 68.4 85.4 10.0 7.5 9.0 11.3\nCOT + INST. 72.0 70.3 71.3 89.1 11.7 12.5 12.0 15.0\nLTM 78.0 73.6 76.3 94.2 5.0 0.0 5.0 6.2\nLTM + INST. 80.5 70.9 76.7 94.7 5.0 0.0 5.0 6.2\nPrompting Exemplar w/ Irrelevant Context, code-davinci-002\nCOT 79.8 72.4 76.8 80.8 16.7 10.0 14.0 14.7\nCOT + INST. 80.5 74.4 78.1 82.2 20.0 12.0 17.0 17.9\nLTM 78.1 84.6 80.7 85.9 23.3 35.0 28.0 29.8\nLTM + INST. 81.0 85.4 82.8 88.1 23.3 35.0 28.0 29.8\nPROGRAM 67.0 55.0 62.2 74.9 11.7 5.0 9.0 10.8\nPROGRAM + INST. 68.8 54.8 63.2 76.1 15.0 7.5 12.0 14.5\nTable 3.Micro and macro accuracies (×100) on the GSM-IC -4K dataset. SC denotes self-consistency. Norm is the overall accuracy\nnormalized by the fraction of solved base problems (Table 2), which is a measure for robustness w.r.t. irrelevant information. For\ntext-davinci-003, the base problem accuracy with COT is 80.0, and the base problem accuracy with LTM is 81.0. The best\nnumbers in each column for each section (i.e., whether usingcode-davinci-002 or text-davinci-003, whether using exemplar\nwith irrelevant context or not, and whether using self-consistency or not) are in boldface.\nand macro accuracies, as well as their corresponding nor-\nmalized accuracies. Overall, we observe significant perfor-\nmance drop for both models with all prompting techniques.\nThe drop on macro accuracy is especially large, showing\nthat fewer than 30% of the base problems are consistently\nsolved after adding distractors. Comparing the results of two\nmodels, text-davinci-003 achieves better normalized\nmicro accuracy than code-davinci-002, though its\nmacro accuracy is mostly worse. In Figure 3, we present\na GSM-IC -4K example where a single irrelevant sentence\ncauses different types of errors in investigated prompting\ntechniques. One common error type is wrongly using the\nnumber in the irrelevant sentence, as shown in the LTM\nprediction and other examples in Appendix B. Even if the\nmodel does not directly use the irrelevant number for numer-\nical calculation, the presence of the irrelevant sentence in\nthe reasoning steps alone can still cause a wrong prediction,\nas shown in the COT prediction.\nLTM is generally the most robust technique to ir-\nrelevant context. In terms of micro accuracy, LTM\noutperforms all other prompting methods across mod-\nels. Using code-davinci-002, LTM achieves about\ndouble macro accuracy of COT. Interestingly, with\ntext-davinci-003, despite that LTM outperforms\nCOT on the micro accuracy, its macro accuracy is lower.\nSpecifically, text-davinci-003 is highly susceptible\nto irrelevant context with role overlap; e.g., such irrelevant\nsentences decrease the macro accuracy to 0 on problems\nwith more than 2 reasoning steps. See Table 4 for the break-\ndown performance on different types of irrelevant context.\nSelecting exemplars with distractors mitigates the dis-\ntractibility. For few-shot prompts, we find that using ex-\nemplars with distractors (i.e., including problems with irrel-\nevant context) consistently outperforms using the original\nexemplars without distractors across prompting techniques.\nWhile prior work has shown that training and fine-tuning\nwith different types of problems improves model robustness\n(Li et al., 2022), our results show that prompting with exem-\nplars that demonstrate how to ignore irrelevant context also\nresults in significant robustness improvement. In Table 5, we\nfurther show that using exemplars with distractors does not\ncause a performance drop on the original GSM8K dataset,\n6Large Language Models Can Be Easily Distracted by Irrelevant Context\nModiﬁed Problem\nMaria buys a large bar of French soap that lasts her for 2 months. She \nspends $8.00 per bar of soap. Every 10 months, Maria's neighbor buys a new \nshampoo and moisturizer for Maria's neighbor. If Maria wants to stock up for \nthe entire year, how much will she spend on soap?\nCorrect COT Output to the Original Problem\nMaria spends $8.00 per bar of soap. She buys a bar of soap every 2 \nmonths. There are 12 months in a year. Maria will spend $8.00 x 12 / 2 = \n$48.00 on soap for the year. The answer is $48.00.\nOutput to the Modiﬁed Problem\nCOT \nMaria buys a large bar of French soap that lasts her for 2 months. She \nspends $8.00 per bar of soap. Every 10 months, Maria's neighbor buys a \nnew shampoo and moisturizer for Maria's neighbor. If Maria wants to \nstock up for the entire year, she will spend $8.00 x 12 = $96.00 on soap. \nThe answer is $96.00.\nLTM \nLet's break down this problem: 1. How much does Maria spend on soap \nevery 10 months? 2. How much does Maria spend on soap every year?\n1. Maria spends $8.00 per bar of soap. She buys a new bar of soap every \n10 months. So Maria spends $8.00 * 10 = $80.00 every 10 months.\n2. Maria spends $80.00 every 10 months. So Maria spends $80.00 * 12 = \n$960.00 every year.\nThe answer is $960.00. \nPROGRAM \nSoap_price = 8.00\nSoap_per_year = 2 * 12\nSoap_total = Soap_price * Soap_per_year\nprint(Soap_total) \nOutput: 192.0\nFigure 3.Example problem and corresponding outputs by different\nprompting techniques (best viewed in color). The COT answer to\nthe original problem is highlighted in green. The added irrelevant\nsentence is in italic and highlighted in red, which causes different\nerrors (highlighted in yellow) for all prompting techniques. More\nexamples of model predictions can be found in Appendix B.\nindicating that such a prompt design can be beneficial in\nachieving better accuracy and robustness simultaneously.\nSelf-consistency significantly reduces the distractibility.\nTaking the majority vote from 20 samples,6 SC improves\nthe overall micro accuracy by more than 11 percentage\npoints. This means that in addition to improving model per-\nformance on clean arithmetic reasoning tasks (Wang et al.,\n2022c), SC also substantially reduces the distractibility of\nlarge language models to irrelevant context. The gain on\nmicro accuracy is notably large on 0-COT (35.5 percentage\npoints). Furthermore, the correct answer for 99.7% of the\nproblems is in the 20 sampled answers for both COT and\nLTM. Even for 0-COT, the recall of correct solutions within\n20 samples is 96.5%. Despite these improvements, the best\nmacro accuracy among all prompting techniques is only\n45%, suggesting that for more than half of the base prob-\nlems, SC fails to prevent the model from being distracted\nby different variants of irrelevant information. These results\nimply that a better algorithm may be developed to further\nreduce the distractibility based on a few sampled solutions.\n6If there is a tie, we take a random top-tier result for evaluation,\nfollowing Wang et al. (2022c) and Shi et al. (2022a).\n2 3 4 ≥ 5\n# Steps\n0\n20\n40\n60\n80\n100Accuracy (× 100)\n73.5\n78.8\n66.4 66.3\n74.9\n86.5\n77.3 80.3\n59.1 57.2\n46.3\n36.7\nCOT\nLTM\nPROGRAM\nFigure 4.Micro accuracies on GSM-IC -4K with respect to the\nnumber of required reasoning steps.\n5.2. Break-Down Analysis\n5.2.1. F ACTORS OF THE IRRELEVANT CONTEXT\nWe analyze the performance of COT, LTM and PROGRAM\nwith respect to the considered factors ( §3.1) of the irrele-\nvant sentences (Table 4). For both models, we find that\n(1) in-topic sentences with (2) role name overlap and (3)\nin-range numbers are generally more challenging, which\nis examplified by Figure 3. For LTM, the latter two fac-\ntors do not have a large effect on the micro accuracy. The\ndifference is more significant for the macro accuracy and,\nas an anomaly, using distractors with in-range numbers\nturns out to be less challenging than out-of-range numbers\nwhen using irrelevant context in the exemplar. Again, with\ncode-davinci-002, LTM outperforms COT and PRO-\nGRAM on all investigated sub-categories. On the other hand,\nusing text-davinci-003, LTM outperforms COT in\nterms of the micro accuracy, but the macro accuracy is much\nlower on all sub-categories.\n5.2.2. B REAK -DOWN ACCURACIES W .R.T. # S TEPS\nWe analyze the break-down accuracies for problems with\nrespect to the reasoning steps (Figure 4). While we see a\nsignificant drop for COT and PROGRAM on problems that\nrequire four or more steps in the reasoning process, the\nperformance of LTM is fairly consistent across difficulty.\nIn addition to the advantage of LTM on clean problems\nfor complicated reasoning (Zhou et al., 2022), our results\nshow that LTM is also less sensitive to irrelevant context for\ncomplicated problems that require more steps to solve.\n5.3. Instructed Prompting Improves Robustness to\nIrrelevant Context\nWe have shown that using exemplars with distractors im-\nproves robustness to irrelevant context. We also compare\nthe performance of instructed prompting and that of the\nprompts without instructions in Table 3. Adding instruc-\ntions to COT, LTM, and PROGRAM consistently improves\ntheir performance. Surprisingly, instructed prompting with\n7Large Language Models Can Be Easily Distracted by Irrelevant Context\nMethod\nMicro Accuracy Macro Accuracy\nTopic Role Overlap Num. Range Topic Role Overlap Num. Range\nIn Off Yes No In Out In Off Yes No In Out\nPrompting Exemplar w/o Irrelevant Context (code-davinci-002)\nCOT 63.1 80.7 68.3 76.6 70.2 74.6 10.2 33.0 10.3 22.2 11.0 19.0\nLTM 70.8 83.4 77.0 78.2 77.2 77.8 23.5 45.0 25.8 35.4 27.0 29.0\nPROGRAM 44.1 63.5 50.7 58.4 54.3 54.5 4.1 24.0 9.3 16.2 7.0 11.0\nPrompting Exemplar w/o Irrelevant Context (text-davinci-003)\nCOT 63.3 72.9 68.7 68.1 67.2 69.6 16.3 36.0 17.5 20.2 19.0 22.0\nLTM 75.4 76.9 75.6 76.8 75.3 77.2 6.1 7.0 6.2 9.1 6.0 6.0\nPrompting Exemplar w/ Irrelevant Context (code-davinci-002)\nCOT 70.2 82.7 73.6 80.2 76.1 77.7 18.4 43.0 21.6 32.3 22.0 26.0\nLTM 73.0 87.5 81.4 80.2 80.0 81.4 28.6 58.0 37.1 42.4 41.0 35.0\nPROGRAM 52.9 70.5 60.2 64.5 61.5 62.8 10.2 37.0 14.4 23.2 15.0 17.0\nTable 4.Breakdown accuracies (×100) w.r.t. the factors of the added irrelevant sentence. Lower accuracy indicates the model is more\nfragile to the corresponding type of irrelevant contexts. Note that the macro average accuracies are higher than the corresponding ones\nreported in Table 3, as we only include a subset of created problems (i.e., those corresponding to the appropriate factor) here to compute\nthe metric. The best result in each column is in boldface.\noriginal exemplars reaches comparable or even better per-\nformance than uninstructed prompting that uses exemplars\nwith distractors for both COT and LTM. Note that adding\nthe instruction “Solve grade school math problems. ”alone\ndoes not significantly improve the performance, and it is the\ninstruction “Feel free to ignore irrelevant information given\nin the questions. ”that makes the difference. Similar to the\ninstruction “Let’s think step by step. ”employed by 0-CoT,\nthis shows that language models are—to some extent—able\nto follow natural language instructions in a way that dramat-\nically changes their problem solving behavior, suggesting\nthat such instructions may be useful for guiding the behavior\nof language models on more tasks.\nOn the original GSM8K development set (Cobbe et al.,\n2021; Zhou et al., 2022), we do not observe a drop in ac-\ncuracy when using exemplars with irrelevant information,\nadding natural language instructions, or both (Table 5). The\nsame holds for SV AMP (Patel et al., 2021), an arithmetic rea-\nsoning benchmark constructed by applying different types\nof variations to math problems from existing clean datasets,\ne.g., changing sentence structures, asking different ques-\ntions with the same information, etc. This is impressive\nbecause the results on GSM-IC show that prompt exem-\nplars with irrelevant information and instructed prompting\nboth improve robustness. For the PROGRAM prompt, we\nfind that using exemplars with distractors even increases\nperformance on SV AMP.\n5.4. Complicated Prompts May Hurt the Robustness to\nIrrelevant Context\nWe compare our 1-exemplar COT prompt (Figure 2) to\na 4-exemplar prompt (Appendix D of Zhou et al., 2022),\nMethod Exemplar Accuracy\nW/ IRRCTX? GSM8K Dev. SV AMP Test\nCOT ✓ 59.3 79.1\n✗ 60.3 77.6\nCOT ✓ 59.3 79.1\n+ INST. ✗ 58.8 78.7\nLTM ✓ 61.9 76.9\n✗ 59.8 76.6\nLTM ✓ 60.9 76.2\n+ INST. ✗ 60.3 76.3\nPROGRAM ✓ 58.6 80.0\n✗ 59.8 77.3\nPROGRAM ✓ 59.2 77.9\n+ INST. ✗ 61.1 77.8\nTable 5.Accuracies (×100) on the GSM8K development set and\nthe SV AMP test set. IRRCTX denotes irrelevant contexts, and\n+INST. denotes instructed prompting. The baseline results (i.e.,\nthose using the simplest exemplars without irrelevant context and\nwithout instructions) are underlined.\nwhich is reported as the best-performing COT prompt on\nGSM8K, on GSM-IC (Table 6). Note that the 1-exemplar\nCOT prompt only includes a problem with a 2-step solution,\nwhile the 4-exemplar prompt includes problems that require\nmore reasoning steps. While the 4-exemplar prompt leads\nto better performance on the original GSM8K development\nset, the 4-exemplar prompt is surprisingly more susceptible\nto the distraction provided by the irrelevant context. In par-\nticular, the 4-exemplar prompt is consistently worse than\nthe 1-exemplar prompt on problems with more than 2 in-\ntermediate steps. Even for 2-step prompts, the accuracy\n8Large Language Models Can Be Easily Distracted by Irrelevant Context\nMethod #Prompting GSM8K GSM-IC-4K\nExemplars Dev. 2 Steps > 2 Steps\nCOT 1 60.3 73.6 70.8\n4 66.3 78.0 69.4\nCOT 1 58.8 79.0 76.0\n+ INST. 4 66.5 79.2 70.6\nTable 6.Micro accuracies (×100) on the GSM8K development set\nand GSM-IC -4K. # Prompting exemplars denotes the number of\nexemplars used in the prompt. The best number in each column is\nin boldface.\nMethod code-davinci-002 text-davinci-003\nCOT 67.4 68.2\nCOT + INST. 68.9 69.9\nLTM 73.4 70.2\nLTM + INST. 74.4 72.8\nTable 7.Accuracies (×100) on the football split of DROP (Dua\net al., 2019) benchmark.\nimprovement from adding more exemplars is almost negli-\ngible when using instructions (79.0 vs 79.2). Overall, this\nfinding indicates that adding more exemplars can make the\nprompt less robust as it leads to some overfitting.\n5.5. Extension to DROP\nIn addition to GSM-IC, we extend our evaluation to the\nDROP dataset (Dua et al., 2019), where the task is to answer\na question according to a long passage that naturally con-\ntains irrelevant context. We show an example about football\ngames in Table 8.\nWe use the COT and LTM prompts in (Zhou et al., 2022) as\nthe baselines, and we evaluate the prompt variants with the\ninstruction “Solve following questions. Feel free to ignore\nirrelevant information given in the questions. ”added before\nthe exemplars. Note that by adding a problem reduction step\nin the exemplar solution, the least-to-most prompt implicitly\nleads the model to come up with relevant subproblems to\nsolve the given problem. Again, we observe that the instruc-\ntion consistently improves the performance of both COT\nand LTM prompting (Table 7).\n6. Conclusion and Discussion\nIn this work, we introduce GSM-IC , a dataset that supports\ncomprehensive study of the distractibility of large language\nmodels when performing arithmetic reasoning in presence\nof irrelevant contexts. We examine a variety of prompt-\ning techniques on GSM-IC , and demonstrate that they are\nall sensitive to the irrelevant information in the problems.\nAmong the studied techniques, self-consistency (Wang et al.,\n2022c) leads to a substantial improvement in robustness to\nirrelevant context across the board, and presenting example\nParagraph\nThe Seahawks played the San Francisco 49ers. In the\nfirst quarter, the Hawks RB Julius Jones got a 27-yard\nTD run, along with DT Craig Terrill returning a fumble\n9 yards for a touchdown. In the third quarter, the 49ers\nalmost rallied as RB H. J. Torres made a 12-yard TD\npass to Lucas Nelly, along with Mare kicking a 32-yard\nfield goal. In the final quarter, Julius Jones got another\n11-yard TD.\nQuestion\nHow many yards do the shortest touchdown run and the\nlongest touchdown pass combine for?\nLTM solution\nTo answer the question ”How many yards do the shortest\ntouchdown run and the longest touchdown pass combine\nfor?”, we need to know: ”How many yards was the\nshortest touchdown run?”, ”How many yards was the\nlongest touchdown pass?”.\nQ: How many yards was the shortest touchdown run?\nA: All the touchdown runs are: a 27-yard touchdown\nrun, a 9-yard touchdown run, a 11-yard touchdown run.\nThe smallest number between 27, 9, 11 is 9. The shortest\ntouchdown run was 9 yards. The answer is 9.\nQ: How many yards was the longest touchdown pass?\nA: All the touchdown passes are: a 12-yard touchdown\npass. The longest touchdown pass was 12 yards. The\nanswer is 12.\nQ: How many yards do the shortest touchdown run and\nthe longest touchdown pass combine for?\nA: We know that the shortest touchdown run was 9-yard.\nWe also know that the longest touchdown pass was 12-\nyard. 9 + 12 = 21. The answer is 21.\nTable 8.A DROP example about football games.\nproblems with irrelevant context in the prompt also consis-\ntently improves the performance. Similarly, we find that\nsimply adding an instruction to ignore irrelevant information\nbrings notable performance gains on our benchmark.\nDespite the improvement achieved by these methods, the\nfundamental issue remains: a single piece of irrelevant in-\nformation can distract the models and substantially degrade\ntheir performance, even on problems whose clean versions\nthey correctly solve. We encourage researchers to also pri-\noritize improving on this fundamental limitation when de-\nveloping new training and prompting techniques. We leave\nfurther investigation on the distractibility for other tasks and\ndifferent language models for future work.\nAcknowledgement\nWe would like to thank Dale Schuurmans, Olivier Bousquet\nand Jack Nikodem for helpful discussion and feedback.\n9Large Language Models Can Be Easily Distracted by Irrelevant Context\nReferences\nAustin, J., Odena, A., Nye, M., Bosma, M., Michalewski,\nH., Dohan, D., Jiang, E., Cai, C., Terry, M., Le, Q., and\nSutton, C. Program synthesis with large language models.\narXiv preprint arXiv:2108.07732, 2021.\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D.,\nDhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,\nAskell, A., et al. Language models are few-shot learners.\nNeurIPS, 2020.\nChaves, R. P. and Richter, S. N. Look at that! BERT can\nbe easily distracted from paying attention to morphosyn-\ntax. In Proceedings of the Society for Computation in\nLinguistics 2021, pp. 28–38, Online, February 2021. As-\nsociation for Computational Linguistics. URL https:\n//aclanthology.org/2021.scil-1.3.\nChen, W., Ma, X., Wang, X., and Cohen, W. W. Program\nof thoughts prompting: Disentangling computation from\nreasoning for numerical reasoning tasks. arXiv preprint\narXiv:2211.12588, 2022.\nChowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra,\nG., Roberts, A., Barham, P., Chung, H. W., Sutton, C.,\nGehrmann, S., et al. Palm: Scaling language modeling\nwith pathways. arXiv preprint arXiv:2204.02311, 2022.\nChung, H. W., Hou, L., Longpre, S., Zoph, B., Tay, Y .,\nFedus, W., Li, E., Wang, X., Dehghani, M., Brahma,\nS., et al. Scaling instruction-finetuned language models.\narXiv preprint arXiv:2210.11416, 2022.\nClark, P., Tafjord, O., and Richardson, K. Transformers\nas soft reasoners over language. In Proceedings of the\nTwenty-Ninth International Conference on International\nJoint Conferences on Artificial Intelligence , pp. 3882–\n3890, 2021.\nCobbe, K., Kosaraju, V ., Bavarian, M., Hilton, J., Nakano,\nR., Hesse, C., and Schulman, J. Training veri-\nfiers to solve math word problems. arXiv preprint\narXiv:2110.14168, 2021. URL https://arxiv.\norg/pdf/2110.14168.\nCreswell, A., Shanahan, M., and Higgins, I. Selection-\ninference: Exploiting large language models for\ninterpretable logical reasoning. arXiv preprint\narXiv:2205.09712, 2022.\nDohan, D., Xu, W., Lewkowycz, A., Austin, J., Bieber, D.,\nLopes, R. G., Wu, Y ., Michalewski, H., Saurous, R. A.,\nSohl-Dickstein, J., Murphy, K., and Sutton, C. Language\nmodel cascades. arXiv preprint arXiv:2207.10342, 2022.\nDrozdov, A., Sch ¨arli, N., Aky ¨urek, E., Scales, N., Song,\nX., Chen, X., Bousquet, O., and Zhou, D. Composi-\ntional semantic parsing with large language models.arXiv\npreprint arXiv:2209.15003, 2022.\nDua, D., Wang, Y ., Dasigi, P., Stanovsky, G., Singh, S., and\nGardner, M. Drop: A reading comprehension benchmark\nrequiring discrete reasoning over paragraphs. In Pro-\nceedings of the 2019 Conference of the North American\nChapter of the Association for Computational Linguis-\ntics: Human Language Technologies, Volume 1 (Long\nand Short Papers), 2019.\nGao, L., Madaan, A., Zhou, S., Alon, U., Liu, P., Yang, Y .,\nCallan, J., and Neubig, G. Pal: Program-aided language\nmodels. arXiv preprint arXiv:2211.10435, 2022.\nHan, S., Schoelkopf, H., Zhao, Y ., Qi, Z., Riddell, M.,\nBenson, L., Sun, L., Zubova, E., Qiao, Y ., Burtell, M.,\net al. Folio: Natural language reasoning with first-order\nlogic. arXiv preprint arXiv:2209.00840, 2022.\nHoyer, W. J., Rebok, G. W., and Sved, S. M. Effects of\nvarying irrelevant information on adult age differences in\nproblem solving. Journal of gerontology, 34(4):553–560,\n1979.\nJia, R. and Liang, P. Adversarial examples for evalu-\nating reading comprehension systems. arXiv preprint\narXiv:1707.07328, 2017.\nJones, E. and Steinhardt, J. Capturing failures of large lan-\nguage models via human cognitive biases. arXiv preprint\narXiv:2202.12299, 2022.\nKassner, N. and Sch¨utze, H. Negated and misprimed probes\nfor pretrained language models: Birds can talk, but can-\nnot fly. In Proceedings of the 58th Annual Meeting\nof the Association for Computational Linguistics , pp.\n7811–7818, Online, July 2020. Association for Compu-\ntational Linguistics. doi: 10.18653/v1/2020.acl-main.\n698. URL https://aclanthology.org/2020.\nacl-main.698.\nKhashabi, D., Khot, T., Sabharwal, A., and Roth, D. Learn-\ning what is essential in questions. In Proceedings of\nthe 21st Conference on Computational Natural Lan-\nguage Learning (CoNLL 2017) , pp. 80–89, Vancou-\nver, Canada, August 2017. Association for Computa-\ntional Linguistics. doi: 10.18653/v1/K17-1010. URL\nhttps://aclanthology.org/K17-1010.\nKhot, T., Trivedi, H., Finlayson, M., Fu, Y ., Richardson, K.,\nClark, P., and Sabharwal, A. Decomposed prompting:\nA modular approach for solving complex tasks. arXiv\npreprint arXiv:2210.02406, 2022.\n10Large Language Models Can Be Easily Distracted by Irrelevant Context\nKim, J., Kim, H. J., Cho, H., Jo, H., Lee, S.-W., Lee, S.-\ng., Yoo, K. M., and Kim, T. Ground-truth labels matter:\nA deeper look into input-label demonstrations. arXiv\npreprint arXiv:2205.12685, 2022.\nKojima, T., Gu, S. S., Reid, M., Matsuo, Y ., and Iwasawa,\nY . Large language models are zero-shot reasoners.arXiv\npreprint arXiv:2205.11916, 2022.\nKumar, V ., Maheshwary, R., and Pudi, V . Adversarial ex-\namples for evaluating math word problem solvers. In\nFindings of the Association for Computational Linguis-\ntics: EMNLP 2021, pp. 2705–2712, 2021.\nLi, D., Rawat, A. S., Zaheer, M., Wang, X., Lukasik, M.,\nVeit, A., Yu, F., and Kumar, S. Large language mod-\nels with controllable working memory. arXiv preprint\narXiv:2211.05110, 2022.\nLiang, P., Bommasani, R., Lee, T., Tsipras, D., Soylu, D.,\nYasunaga, M., Zhang, Y ., Narayanan, D., Wu, Y ., Kumar,\nA., et al. Holistic evaluation of language models. arXiv\npreprint arXiv:2211.09110, 2022.\nLing, W., Yogatama, D., Dyer, C., and Blunsom, P. Program\ninduction by rationale generation: Learning to solve and\nexplain algebraic word problems. In Proceedings of the\n55th Annual Meeting of the Association for Computa-\ntional Linguistics (Volume 1: Long Papers), pp. 158–167,\nVancouver, Canada, July 2017. Association for Compu-\ntational Linguistics. doi: 10.18653/v1/P17-1015. URL\nhttps://aclanthology.org/P17-1015.\nMadaan, A. and Yazdanbakhsh, A. Text and patterns: For\neffective chain of thought, it takes two to tango. arXiv\npreprint arXiv:2209.07686, 2022.\nMarzocchi, G. M., Lucangeli, D., De Meo, T., Fini, F., and\nCornoldi, C. The disturbing effect of irrelevant informa-\ntion on arithmetic problem solving in inattentive children.\nDevelopmental neuropsychology, 21(1):73–92, 2002.\nMin, S., Lyu, X., Holtzman, A., Artetxe, M., Lewis, M.,\nHajishirzi, H., and Zettlemoyer, L. Rethinking the role of\ndemonstrations: What makes in-context learning work?\narXiv preprint arXiv:2202.12837, 2022.\nMisra, K., Rayz, J., and Ettinger, A. COMPS: Conceptual\nminimal pair sentences for testing robust property knowl-\nedge and its inheritance in pre-trained language models.\nIn Proceedings of the 17th Conference of the European\nChapter of the Association for Computational Linguistics,\n2023.\nMorris, J. X., Lifland, E., Yoo, J. Y ., Grigsby, J., Jin, D., and\nQi, Y . Textattack: A framework for adversarial attacks,\ndata augmentation, and adversarial training in nlp. arXiv\npreprint arXiv:2005.05909, 2020.\nNi, J., Zhu, C., Chen, W., and McAuley, J. Learning to at-\ntend on essential terms: An enhanced retriever-reader\nmodel for open-domain question answering. In Pro-\nceedings of the 2019 Conference of the North American\nChapter of the Association for Computational Linguis-\ntics: Human Language Technologies, Volume 1 (Long\nand Short Papers) , pp. 335–344, Minneapolis, Min-\nnesota, June 2019. Association for Computational Lin-\nguistics. doi: 10.18653/v1/N19-1030. URL https:\n//aclanthology.org/N19-1030.\nNye, M., Andreassen, A. J., Gur-Ari, G., Michalewski, H.,\nAustin, J., Bieber, D., Dohan, D., Lewkowycz, A., Bosma,\nM., Luan, D., Sutton, C., and Odena, A. Show your work:\nScratchpads for intermediate computation with language\nmodels. arXiv preprint arXiv:2112.00114, 2021.\nOuyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright,\nC. L., Mishkin, P., Zhang, C., Agarwal, S., Slama,\nK., Ray, A., et al. Training language models to fol-\nlow instructions with human feedback. arXiv preprint\narXiv:2203.02155, 2022. URL https://arxiv.\norg/abs/2203.02155.\nPandia, L. and Ettinger, A. Sorting through the noise:\nTesting robustness of information processing in pre-\ntrained language models. In Proceedings of the 2021\nConference on Empirical Methods in Natural Language\nProcessing, pp. 1583–1596, Online and Punta Cana,\nDominican Republic, 2021. Association for Computa-\ntional Linguistics. doi: 10.18653/v1/2021.emnlp-main.\n119. URL https://aclanthology.org/2021.\nemnlp-main.119.\nPasolunghi, M. C., Cornoldi, C., and De Liberto, S. Work-\ning memory and intrusions of irrelevant information in\na group of specific poor problem solvers. Memory &\nCognition, 27:779–790, 1999.\nPatel, A., Bhattamishra, S., and Goyal, N. Are nlp models\nreally able to solve simple math word problems? In\nNAACL-HLT, 2021. URL https://aclanthology.\norg/2021.naacl-main.168.pdf.\nPress, O., Zhang, M., Min, S., Schmidt, L., Smith, N. A.,\nand Lewis, M. Measuring and narrowing the com-\npositionality gap in language models. arXiv preprint\narXiv:2210.03350, 2022.\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S.,\nMatena, M., Zhou, Y ., Li, W., and Liu, P. J. Explor-\ning the limits of transfer learning with a unified text-\nto-text transformer. Journal of Machine Learning Re-\nsearch, 2020. URL https://jmlr.org/papers/\nv21/20-074.html.\n11Large Language Models Can Be Easily Distracted by Irrelevant Context\nRavichander, A., Gardner, M., and Marasovi´c, A. Condaqa:\nA contrastive reading comprehension dataset for reason-\ning about negation. arXiv preprint arXiv:2211.00295 ,\n2022.\nSanh, V ., Webson, A., Raffel, C., Bach, S. H., Sutawika, L.,\nAlyafeai, Z., Chaffin, A., Stiegler, A., Scao, T. L., Raja,\nA., et al. Multitask prompted training enables zero-shot\ntask generalization. arXiv preprint arXiv:2110.08207 ,\n2021.\nShi, F., Fried, D., Ghazvininejad, M., Zettlemoyer, L., and\nWang, S. I. Natural language to code translation with\nexecution. In EMNLP, 2022a.\nShi, F., Suzgun, M., Freitag, M., Wang, X., Srivats, S.,\nV osoughi, S., Chung, H. W., Tay, Y ., Ruder, S., Zhou,\nD., Das, D., and Wei, J. Language models are mul-\ntilingual chain-of-thought reasoners. arXiv preprint\narXiv:2210.03057, 2022b. URL https://arxiv.\norg/pdf/2210.03057.\nShi, H., Mao, J., Xiao, T., Jiang, Y ., and Sun, J. Learning\nvisually-grounded semantics from contrastive adversarial\nsamples. In Proceedings of the 27th International Con-\nference on Computational Linguistics , pp. 3715–3727,\n2018.\nSinha, K., Sodhani, S., Dong, J., Pineau, J., and Hamilton,\nW. L. CLUTRR: A diagnostic benchmark for induc-\ntive reasoning from text. In Proceedings of the 2019\nConference on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Conference\non Natural Language Processing (EMNLP-IJCNLP), pp.\n4506–4515, Hong Kong, China, 2019. Association for\nComputational Linguistics. doi: 10.18653/v1/D19-1458.\nURL https://aclanthology.org/D19-1458.\nSuzgun, M., Scales, N., Sch ¨arli, N., Gehrmann, S., Tay,\nY ., Chung, H. W., Chowdhery, A., Le, Q. V ., Chi, E. H.,\nZhou, D., and Wei, J. Challenging big-bench tasks and\nwhether chain-of-thought can solve them. arXiv preprint\narXiv:2210.09261, 2022.\nTafjord, O., Mishra, B. D., and Clark, P. Proofwriter: Gen-\nerating implications, proofs, and abductive statements\nover natural language. arXiv preprint arXiv:2012.13048,\n2020.\nWang, B., Xu, C., Wang, S., Gan, Z., Cheng, Y ., Gao, J.,\nAwadallah, A. H., and Li, B. Adversarial glue: A multi-\ntask benchmark for robustness evaluation of language\nmodels. arXiv preprint arXiv:2111.02840, 2021.\nWang, B., Min, S., Deng, X., Shen, J., Wu, Y ., Zettlemoyer,\nL., and Sun, H. Towards understanding chain-of-thought\nprompting: An empirical study of what matters. arXiv\npreprint arXiv:2212.10001, 2022a.\nWang, X., Wei, J., Schuurmans, D., Le, Q., Chi, E., and\nZhou, D. Rationale-augmented ensembles in language\nmodels. arXiv preprint arXiv:2207.00747, 2022b.\nWang, X., Wei, J., Schuurmans, D., Le, Q., Chi, E.,\nand Zhou, D. Self-consistency improves chain of\nthought reasoning in language models. arXiv preprint\narXiv:2203.11171, 2022c.\nWebson, A. and Pavlick, E. Do prompt-based models really\nunderstand the meaning of their prompts? arXiv preprint\narXiv:2109.01247, 2021.\nWei, J., Bosma, M., Zhao, V . Y ., Guu, K., Yu, A. W., Lester,\nB., Du, N., Dai, A. M., and Le, Q. V . Finetuned lan-\nguage models are zero-shot learners. arXiv preprint\narXiv:2109.01652, 2021.\nWei, J., Wang, X., Schuurmans, D., Bosma, M., Chi,\nE., Le, Q., and Zhou, D. Chain of thought prompt-\ning elicits reasoning in large language models. In\nNeurIPS, 2022. URL https://openreview.net/\npdf?id=_VjQlMeSB_J.\nWeston, J., Bordes, A., Chopra, S., Rush, A. M.,\nVan Merri¨enboer, B., Joulin, A., and Mikolov, T. Towards\nai-complete question answering: A set of prerequisite toy\ntasks. arXiv preprint arXiv:1502.05698, 2015.\nZhou, D., Sch¨arli, N., Hou, L., Wei, J., Scales, N., Wang, X.,\nSchuurmans, D., Bousquet, O., Le, Q., and Chi, E. Least-\nto-most prompting enables complex reasoning in large\nlanguage models. arXiv preprint arXiv:2205.10625, 2022.\nURL https://arxiv.org/pdf/2205.10625.\n12Large Language Models Can Be Easily Distracted by Irrelevant Context\nA. GSM-IC Details\nEach of the 100 base problem require two to seven steps to solve (Figure 5).\n2 3 4 5 6 7\n# Steps\n0\n20\n40\n60# Problems\n60\n15 12 9\n3 1\nFigure 5.Base problem distribution of GSM-IC with respect to the number of reasoning steps in the ground truth problem solution.\nStarting from the base problems, we follow the protocols below to create GSM-IC (§3.1).\n1. Irrelevant sentence template.\n(a) For in-topic sentences, we manually write templates within the topic that is close to the original problem\ndescription. We are particularly careful about the shareable stuff, for example, money is sometimes considered\nshareable between family members. In such cases, we make sure that the added do not change the amount of\nshareable stuff to ensure that the final standard answer is not affected.\n(b) For off-topic sentences, we use general templates (Table 9) for all problems unless some of them can be considered\nas in-topic sentences for some problems—for example, the sentence “The height of {role} is {number} feet.” is\nconsidered as an in-topic sentence for problems about heights of people.\nThe shoe size of [ROLE] is [NUMBER].\n[ROLE] is [NUMBER] years old.\nThe height of [ROLE] is [NUMBER] feet.\n[ROLE] bought [NUMBER] tomatoes from the grocery store.\n[ROLE] has read [NUMBER] books in the past year.\nTable 9.Off-topic sentence templates for GSM-IC.\n(c) We make sure that all sentences derived by each template are grammatical English sentences.\n(d) We write four in-topic and choose four off-topic distractor sentence templates for each problem.\n2. Blank fillers: role names.\n(a) We randomly choose a role name X, and use X’s father, X’s mother, X’s brother, X’s sister\nand X’s neighbor as the overlapped role names.\n(b) We choose from the name set {Ada, David, Emma, Jack, John, Mary, Max, Tom} for non-\noverlapped role names.\n(c) We write five names that have overlap with the original character, and five names that do not have overlap for each\nproblem.\n3. Blank fillers: numbers.\n(a) For in-range numbers, we randomly sample positive integers in the range of [ ℓ\n10 , 10r], where ℓ and r denote the\nsmallest and the largest number that appear in the problem description and standard solution, respectively.\n13Large Language Models Can Be Easily Distracted by Irrelevant Context\nOriginal Problem Kim plants 80 cherry pits. 25% of them sprout and Kim sells 6 of the saplings.\nHow many cherry saplings does she have left?\nAdded Sentence Kim’s mother plants 20 more potatoes.\nAmbiguous Problem Kim plants 80 cherry pits. 25% of them sprout and Kim sells 6 of the saplings.\nKim’s mother plants 20 more potatoes. How many cherry saplings does she have\nleft?\nFixed Problem Kim plants 80 cherry pits. 25% of them sprout and Kim sells 6 of the saplings.\nKim’s mother plants 20 more potatoes. How many cherry saplings does Kim\nhave left?\nTable 10.An example that adding irrelevant contexts causes ambiguity: after adding the sentence, it is unclear whether she refers to Kim\nor Kim’s mother. To ensure that the standard answer is unchanged, we modify the final question to make it clear and faithful to the original\nproblem.\n(b) For out-of-range numbers, we choose from the range of [2, +∞)\\[ ℓ\n10 , 10r]. For very few problems that ℓ is\nrelatively large (i.e., ℓ >105) where we choose out-of-range numbers from the range of[2, ℓ\n10 ]; for other problems\nwe choose out-of-range numbers n = a × 10b from the range [10r,∞), where a and b are both non-negative\nintegers.\n(c) We write four in-range numbers and four out-of-range numbers for each problem.\n4. Finally, if adding the irrelevant sentence causes ambiguity (e.g., Table 10), we fix the question to ensure that the\nstandard solution to the generated problem remain the same as the base problem.\nB. Sample Predictions on GSM-IC\nIn addition to the example outputs shown in Figure 3, we include more example problems and the predictions by different\ntechniques (Tables 11 and 12).\nC. Full prompts in experiments\nWe list the prompts for all experiments in Tables 13 and 14.\n14Large Language Models Can Be Easily Distracted by Irrelevant Context\nMethod Original GSM8K Problem Modified Problem (w/ Irrelevant Sentence)\nJessica is six years older than Claire. In two years, Claire will\nbe 20 years old. How old is Jessica now?\nJessica is six years older than Claire. In two years,\nClaire will be 20 years old. Twenty years ago, the age\nof Claire’s father is 3 times of Jessica’s age. How old\nis Jessica now?\nCOT In two years, Claire will be 20 years old. So, Claire is 18 years\nold now. Jessica is six years older than Claire, so Jessica is 18\n+ 6 = 24 years old now. The answer is 24.\nJessica is six years older than Claire, so Jessica is 20 -\n6 = 14 years old. Twenty years ago, the age of Claire’s\nfather is 3 times of Jessica’s age, so 20 years ago, the\nage of Claire’s father is 3 * 14 = 42 years old. The\nanswer is 14.\nLTM Let’s break down this problem: 1. How old is Claire now? 2.\nHow old is Jessica now?\nA: Let’s break down this problem: 1. How old is Claire\nnow? 2. How old is Jessica now?\n1. In two years, Claire will be 20 years old. So Claire is 20 - 2\n= 18 years old now.\n1. In two years, Claire will be 20 years old. So Claire\nis 18 years old now.\n2. Jessica is six years older than Claire. So Jessica is 18 + 6 =\n24 years old now.\n2. Twenty years ago, the age of Claire’s father is 3 times\nof Jessica’s age. So 20 years ago, Claire’s father is 3\n* Jessica’s age. So Claire’s father is 3 * Jessica’s age\nnow. So Claire’s father is 3 * Jessica’s age - 20 years\nold now. So Claire’s father is 3 * Jessica’s age - 20 +\n18 = 3 * Jessica’s age - 2 years old now. So Claire’s\nfather is 3 * Jessica’s age - 2 + 6 = 3 * Jessica’s age + 4\nyears old now. So Claire’s father is 3 * Jessica’s age + 4\n- 18 = 3 * Jessica’s age - 14 years old now. So Claire’s\nfather is 3 * Jessica’s age - 14 + 20 = 3 * Jessica’s age +\n6 years old now. So 3 * Jessica’s age + 6 = 3 * Jessica’s\nage - 2 + 6 = 3 * Jessica’s age + 4. So 3 * Jessica’s age\n+ 6 = 3 * Jessica’s age + 4. So 6 = 4. So Jessica’s age =\n2. So Jessica is 2 years old now.\nThe answer is 24. The answer is 2.\nPROGRAM Claire age = 20 - 2 Claire age = 20 - 2\nJessica age = Claire age + 6 Jessica age = Claire age - 6\nprint(Jessica age) Claire father age = 3 * Jessica age\nprint(Jessica age)\nTable 11.Output example #1 for different prompting techniques, on a GSM8K problem and a corresponding GSM-IC problem. While all\ntechniques give correct answers to the original problem, they output different different results for the modified GSM-IC problem.\n15Large Language Models Can Be Easily Distracted by Irrelevant Context\nMethod Original GSM8K Problem Modified Problem (w/ Irrelevant Sentence)\nLucy has $65 in the bank. She made a $15 deposit and\nthen followed by a $4 withdrawal. What is Lucy’s bank\nbalance?\nLucy has $65 in the bank. She made a $15 deposit and\nthen followed by a $4 withdrawal. Lucy’s mother’s\nmonthly rent is $10. What is Lucy’s bank balance?\nCOT Lucy has $65 in the bank. She made a $15 deposit,\nso she has $65 + $15 = $80. She then made a $4\nwithdrawal, so she has $80 - $4 = $76. The answer is\n$76.\nLucy has $65 in the bank. She made a $15 deposit and\nthen followed by a $4 withdrawal. Lucy’s mother’s\nmonthly rent is $10. Lucy’s bank balance is $65 + $15\n- $4 - $10 = $66. The answer is $66.\nLTM Let’s break down this problem: 1. How much money\ndid Lucy deposit? 2. How much money did Lucy\nwithdraw? 3. How much money does Lucy have in the\nbank?\nLet’s break down this problem: 1. How much money\ndid Lucy deposit? 2. How much money did Lucy\nwithdraw? 3. How much money does Lucy have in the\nbank?\n1. Lucy deposited $15. 1. Lucy deposited $15.\n2. Lucy withdrew $4. 2. Lucy withdrew $4.\n3. Lucy has $65 + $15 - $4 = $76 in the bank. 3. Lucy has $65 + $15 - $4 - $10 = $66 in the bank.\nThe answer is $76. The answer is $66.\nPROGRAM Lucy balance = 65 Lucy bank = 65\nLucy balance = Lucy balance + 15 Lucy bank = Lucy bank + 15\nLucy balance = Lucy balance - 4 Lucy bank = Lucy bank - 4\nprint(Lucy balance) Lucy bank = Lucy bank - 10\nprint(Lucy bank)\nTable 12.Output example #2 for different prompting techniques, on a GSM8K problem and a corresponding GSM-IC problem. While all\ntechniques give correct answers to the original problem, they output the same incorrect result for the modified GSM-IC problem.\n16Large Language Models Can Be Easily Distracted by Irrelevant Context\nMethod Example Prompt\nw/ IrrCtx?\nCOT ✗ Q: Elsa has 5 apples. Anna has 2 more apples than Elsa. How many apples do they\nhave together?\nA: Anna has 2 more apples than Elsa, so Anna has 2 + 5 = 7 apples. Elsa and Anna\nhave 5 + 7 = 12 apples together. The answer is 12.\nQ: [Problem of Interest]\nA:\nCOT ✓ Q: Elsa has 5 apples. Anna has 2 more apples than Elsa. Liz has 4 peaches. How\nmany apples do they have together?\nA: Anna has 2 more apples than Elsa, so Anna has 2 + 5 = 7 apples. Elsa and Anna\nhave 5 + 7 = 12 apples together. The answer is 12.\nQ: [Problem of Interest]\nA:\nLTM ✗ Q: Elsa has 5 apples. Anna has 2 more apples than Elsa. How many apples do they\nhave together?\nA: Let’s break down this problem: 1. How many apples does Anna have? 2. How\nmany apples do Elsa and Anna have together?\n1. Anna has 2 more apples than Elsa. So Anna has 2 + 5 = 7 apples.\n2. Elsa and Anna have 5 + 7 = 12 apples together.\nQ: [Problem of Interest]\nA: Let’s break down this problem:\nLTM ✓ Q: Elsa has 5 apples. Anna has 2 more apples than Elsa. Liz has 4 peaches. How\nmany apples do they have together?\nA: Let’s break down this problem: 1. How many apples does Anna have? 2. How\nmany apples do Elsa and Anna have together?\n1. Anna has 2 more apples than Elsa. So Anna has 2 + 5 = 7 apples.\n2. Elsa and Anna have 5 + 7 = 12 apples together.\nQ: [Problem of Interest]\nA: Let’s break down this problem:\n0-COT N/A Q: [Problem of Interest]\nA: Let’s think step by step:\nPROGRAM ✗ Q: Elsa has 5 apples. Anna has 2 more apples than Elsa. How many apples do they\nhave together?\nA: Let’s solve the problem by a Python program:\nElsa apples = 5\nAnna apples = 2 + Elsa apples\nElsa Anna apples = Elsa apples + Anna apples\nprint(Elsa Anna apples)\nQ: [Problem of Interest]\nA: Let’s solve the problem by a Python program:\nPROGRAM ✓ Q: Elsa has 5 apples. Anna has 2 more apples than Elsa. Liz has 4 peaches. How\nmany apples do they have together?\nA: Let’s solve the problem by a Python program:\nElsa apples = 5\nAnna apples = 2 + Elsa apples\nElsa Anna apples = Elsa apples + Anna apples\nprint(Elsa Anna apples)\nQ: [Problem of Interest]\nA: Let’s solve the problem by a Python program:\nTable 13.Prompts used for all investigated techniques, without instruction. The placeholder [Problem of Interest] is substituted for each\nproblem at the test time.\n17Large Language Models Can Be Easily Distracted by Irrelevant Context\nMethod Example Prompt\nw/ IrrCtx?\nCOT ✗ Solve grade school math problems. Feel free to ignore irrelevant information given in the questions.\nQ: Elsa has 5 apples. Anna has 2 more apples than Elsa. How many apples do they have together?\nA: Anna has 2 more apples than Elsa, so Anna has 2 + 5 = 7 apples. Elsa and Anna have 5 + 7 = 12\napples together. The answer is 12.\nQ: [Problem of Interest]\nA:\nCOT ✓ Solve grade school math problems. Feel free to ignore irrelevant information given in the questions.\nQ: Elsa has 5 apples. Anna has 2 more apples than Elsa. Liz has 4 peaches. How many apples do they\nhave together?\nA: Anna has 2 more apples than Elsa, so Anna has 2 + 5 = 7 apples. Elsa and Anna have 5 + 7 = 12\napples together. The answer is 12.\nQ: [Problem of Interest]\nA:\nLTM ✗ Solve grade school math problems. Feel free to ignore irrelevant information given in the questions.\nQ: Elsa has 5 apples. Anna has 2 more apples than Elsa. How many apples do they have together?\nA: Let’s break down this problem: 1. How many apples does Anna have? 2. How many apples do Elsa\nand Anna have together?\n1. Anna has 2 more apples than Elsa. So Anna has 2 + 5 = 7 apples.\n2. Elsa and Anna have 5 + 7 = 12 apples together.\nQ: [Problem of Interest]\nA: Let’s break down this problem:\nLTM ✓ Solve grade school math problems. Feel free to ignore irrelevant information given in the questions.\nQ: Elsa has 5 apples. Anna has 2 more apples than Elsa. Liz has 4 peaches. How many apples do they\nhave together?\nA: Let’s break down this problem: 1. How many apples does Anna have? 2. How many apples do Elsa\nand Anna have together?\n1. Anna has 2 more apples than Elsa. So Anna has 2 + 5 = 7 apples.\n2. Elsa and Anna have 5 + 7 = 12 appl